{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips from prof\n",
    "\n",
    "- Narrow scope of work (e.g. court level)\n",
    "\n",
    "- Could try both binary/multi-class model outcomes and compare the performance \n",
    "\n",
    "- Change user from layperson to legal professional (and mention that this project is a stepping stone towards having layperson use the model)\n",
    "\n",
    "- Link features to predicted outcome (if time permits can try using XGBoost with LIME for model interpretability)\n",
    "\n",
    "- Can also try to see accuracy of models with different areas of law, lowest accuracy may be hardest area of law to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joelleng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joelleng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import json\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "# Ignore the DeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename                                        area_of_law  \\\n",
      "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
      "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
      "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
      "3  2000_SGCA_12.pdf  {'courts and jurisdiction': ['court of appeal'...   \n",
      "4  2000_SGCA_13.pdf                     {'criminal law': ['offences']}   \n",
      "\n",
      "  court_level                                             issues  \\\n",
      "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
      "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
      "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
      "3        SGCA  the appeals from the assistant registrar. In h...   \n",
      "4        SGCA  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...            12   \n",
      "1  facts and surrounding circumstances including ...             8   \n",
      "2  Background \\nThe first appellants, a French co...             0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...            27   \n",
      "4  facts. Mere assertion would not suffice. In ex...            28   \n",
      "\n",
      "   facts_topic        target  \n",
      "0            7    Favourable  \n",
      "1            3    Favourable  \n",
      "2           12    No outcome  \n",
      "3           10  Unfavourable  \n",
      "4           13  Unfavourable  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV files into DataFrames\n",
    "areas_of_law_df = pd.read_csv(\"data/prediction_data/areas_of_law.csv\")\n",
    "coram_df = pd.read_csv(\"data/prediction_data/coram.csv\")\n",
    "sg_legal_cases_df = pd.read_csv(\"data/prediction_data/sg_legal_cases_dataset.csv\")\n",
    "target_rulings_df = pd.read_csv(\"data/prediction_data/target_rulings.csv\")\n",
    "issues_facts_df = pd.read_csv(\"data/prediction_data/issues_facts_topic.csv\")\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/issues.json') as f:\n",
    "    issues_data = [json.loads(line) for line in f]\n",
    "issues_df = pd.DataFrame(issues_data)\n",
    "\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/updated_facts.json') as f:\n",
    "    facts_data = [json.loads(line) for line in f]\n",
    "raw_facts_df = pd.DataFrame(facts_data)\n",
    "raw_facts_df[\"casename\"] = raw_facts_df[\"casename\"].apply(lambda case: case + \".pdf\" if case[-4:] != \".pdf\" else case)\n",
    "raw_facts_df[\"facts\"] = raw_facts_df[\"facts\"].fillna(\"\") + raw_facts_df[\"fact\"].fillna(\"\")\n",
    "raw_facts_df = raw_facts_df.drop(columns=[\"fact\"])\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(areas_of_law_df, sg_legal_cases_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, raw_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, target_rulings_df, on='casename', how='inner')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "casename        0\n",
       "area_of_law     0\n",
       "court_level     0\n",
       "issues          0\n",
       "facts           0\n",
       "issues_topic    0\n",
       "facts_topic     0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate coram names and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coram_names(coram_list):\n",
    "    all_names = set()\n",
    "    for item in coram_list:\n",
    "        split_names = re.split(r';\\s(?![a-zA-Z]+\\s)', item)\n",
    "        for name in split_names:\n",
    "            if ';' in name and not re.search(r';\\s[a-zA-Z]+$', name):\n",
    "                sub_names = name.split(';')\n",
    "                all_names.update([n.strip() for n in sub_names if n.strip()])\n",
    "            else:\n",
    "                all_names.add(name.strip())\n",
    "    return list(all_names)\n",
    "\n",
    "def remove_coram_roles(coram_list):\n",
    "    roles = [' CJ', ' AG', ' J', ' DCJ', ' JA', ' AR', ' JC', 'SAR']\n",
    "    for role in roles:\n",
    "        coram_list = [re.sub(rf'{role}$', '', name) for name in coram_list]\n",
    "    return coram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "coram_df = coram_df.dropna()\n",
    "for i, coram_str in enumerate(coram_df['Coram']):\n",
    "    coram = ast.literal_eval(coram_str)\n",
    "    \n",
    "    coram_modified = clean_coram_names(coram)\n",
    "    coram_modified = remove_coram_roles(coram_modified)\n",
    "    coram_df.at[i, 'Coram'] = str(coram_modified)\n",
    "merged_df = pd.merge(merged_df, coram_df, on='casename', how='outer')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casename         7\n",
      "area_of_law     54\n",
      "court_level     54\n",
      "issues          54\n",
      "facts           54\n",
      "issues_topic    54\n",
      "facts_topic     54\n",
      "target          54\n",
      "Coram            7\n",
      "dtype: int64\n",
      "               casename area_of_law court_level issues facts  issues_topic  \\\n",
      "241   2000_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "274   2000_SGHC_290.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "412    2001_SGCA_66.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "432   2001_SGHC_101.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "438   2001_SGHC_108.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "442   2001_SGHC_111.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "448   2001_SGHC_118.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "457   2001_SGHC_128.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "460   2001_SGHC_130.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "462   2001_SGHC_132.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "475   2001_SGHC_148.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "478   2001_SGHC_150.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "479   2001_SGHC_151.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "489   2001_SGHC_163.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "498   2001_SGHC_174.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "536   2001_SGHC_214.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "537   2001_SGHC_215.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "544   2001_SGHC_222.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "546   2001_SGHC_224.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "550   2001_SGHC_228.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "551   2001_SGHC_229.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "555   2001_SGHC_232.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "564   2001_SGHC_240.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "568   2001_SGHC_244.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "574   2001_SGHC_250.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "578   2001_SGHC_254.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "581   2001_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "591   2001_SGHC_266.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "592   2001_SGHC_267.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "596   2001_SGHC_270.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "603   2001_SGHC_277.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "615   2001_SGHC_289.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "619   2001_SGHC_292.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "622   2001_SGHC_295.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "630   2001_SGHC_301.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "633   2001_SGHC_304.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "647   2001_SGHC_319.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "651   2001_SGHC_322.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "658   2001_SGHC_329.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "667   2001_SGHC_337.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "675   2001_SGHC_344.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "678   2001_SGHC_347.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "700   2001_SGHC_367.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "701   2001_SGHC_368.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "705   2001_SGHC_371.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "707   2001_SGHC_373.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "713   2001_SGHC_379.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "8567                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8568                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8569                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8570                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8571                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8572                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8573                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "\n",
      "      facts_topic target                                              Coram  \n",
      "241           NaN    NaN                                ['Sundaresh Menon']  \n",
      "274           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Tan Lee Me...  \n",
      "412           NaN    NaN                                 ['Ang Cheng Hock']  \n",
      "432           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Choo Han T...  \n",
      "438           NaN    NaN                                ['Pang Khang Chau']  \n",
      "442           NaN    NaN                                    ['Edmund Leow']  \n",
      "448           NaN    NaN                                 ['Amarjeet Singh']  \n",
      "457           NaN    NaN                                    ['Quentin Loh']  \n",
      "460           NaN    NaN                     ['Chao Hick Tin', 'V K Rajah']  \n",
      "462           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "475           NaN    NaN                            ['Vinodh Coomaraswamy']  \n",
      "478           NaN    NaN                                  ['Kannan Ramesh']  \n",
      "479           NaN    NaN  ['Judith Prakash', 'Steven Chong', 'Sundaresh ...  \n",
      "489           NaN    NaN                                     ['Woo Bih Li']  \n",
      "498           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "536           NaN    NaN                                    ['S Rajendran']  \n",
      "537           NaN    NaN                                      ['MPH Rubin']  \n",
      "544           NaN    NaN  ['Judith Prakash', 'Sundaresh Menon', 'Tay Yon...  \n",
      "546           NaN    NaN                                 ['Tan Siong Thye']  \n",
      "550           NaN    NaN                                 ['Hoo Sheau Peng']  \n",
      "551           NaN    NaN                                   ['Lai Kew Chai']  \n",
      "555           NaN    NaN                                  ['Choo Han Teck']  \n",
      "564           NaN    NaN                                     ['George Wei']  \n",
      "568           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "574           NaN    NaN                                 ['Tay Yong Kwang']  \n",
      "578           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "581           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Tan Lee Me...  \n",
      "591           NaN    NaN                                 ['Judith Prakash']  \n",
      "592           NaN    NaN                                   ['Lee Seiu Kin']  \n",
      "596           NaN    NaN                                   ['Lee Kim Shin']  \n",
      "603           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "615           NaN    NaN                                      ['MPH Rubin']  \n",
      "619           NaN    NaN                                 ['Kwek Mean Luck']  \n",
      "622           NaN    NaN                                     ['Woo Bih Li']  \n",
      "630           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "633           NaN    NaN                                  ['Choo Han Teck']  \n",
      "647           NaN    NaN                                  ['Valerie Thean']  \n",
      "651           NaN    NaN                                 ['Judith Prakash']  \n",
      "658           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "667           NaN    NaN                                     ['Woo Bih Li']  \n",
      "675           NaN    NaN                                  ['Yong Pung How']  \n",
      "678           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "700           NaN    NaN                                     ['Woo Bih Li']  \n",
      "701           NaN    NaN                                  ['Valerie Thean']  \n",
      "705           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "707           NaN    NaN                                  ['Yong Pung How']  \n",
      "713           NaN    NaN                               ['Dedar Singh Gill']  \n",
      "8567          NaN    NaN  ['Andrew Phang Boon Leong', 'Steven Chong', 'S...  \n",
      "8568          NaN    NaN                                    ['Lim Jian Yi']  \n",
      "8569          NaN    NaN  ['Judith Prakash', 'Andrew Phang Boon Leong', ...  \n",
      "8570          NaN    NaN                     ['Chao Hick Tin', 'L P Thean']  \n",
      "8571          NaN    NaN                                 ['Judith Prakash']  \n",
      "8572          NaN    NaN     ['V K Rajah', 'Steven Chong', 'Kan Ting Chiu']  \n",
      "8573          NaN    NaN                                  ['Valerie Thean']  \n",
      "casename        0\n",
      "area_of_law     0\n",
      "court_level     0\n",
      "issues          0\n",
      "facts           0\n",
      "issues_topic    0\n",
      "facts_topic     0\n",
      "target          0\n",
      "Coram           0\n",
      "dtype: int64\n",
      "target\n",
      "Favourable      3941\n",
      "Unfavourable    2056\n",
      "No outcome       794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = merged_df.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "#nas are probably those reassigned cases, coram has 7, i just drop them for now\n",
    "na_target_rows = merged_df[merged_df['target'].isna()]\n",
    "print(na_target_rows)\n",
    "\n",
    "merged_df.dropna(axis=0, inplace=True)\n",
    "print(merged_df.isna().sum())\n",
    "\n",
    "#remove empty lists\n",
    "merged_df = merged_df.query(\"area_of_law != '[]'\")\n",
    "\n",
    "#target is unbalanced\n",
    "target_counts = merged_df['target'].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True) # prevent nan values from appearing after one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casename</th>\n",
       "      <th>area_of_law</th>\n",
       "      <th>court_level</th>\n",
       "      <th>issues</th>\n",
       "      <th>facts</th>\n",
       "      <th>issues_topic</th>\n",
       "      <th>facts_topic</th>\n",
       "      <th>target</th>\n",
       "      <th>Coram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_SGCA_1.pdf</td>\n",
       "      <td>{'civil procedure': ['pleadings'], 'res judica...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The claim was dismissed with costs by the\\nHig...</td>\n",
       "      <td>The facts\\nThe appellant is the widow of one T...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[V K Rajah, Andrew Phang Boon Leong, Chan Sek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_SGCA_10.pdf</td>\n",
       "      <td>{'contract': ['formation'], 'equity': ['defenc...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the claim and\\nagainst that decision this appe...</td>\n",
       "      <td>facts and surrounding circumstances including ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[Chao Hick Tin, V K Rajah, Andrew Phang Boon L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000_SGCA_11.pdf</td>\n",
       "      <td>{'contract': ['discharge'], 'damages': ['asses...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The appeal \\nThe questions which arise in this...</td>\n",
       "      <td>Background \\nThe first appellants, a French co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>[Andrew Phang Boon Leong, Tan Lee Meng, Chan S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           casename                                        area_of_law  \\\n",
       "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
       "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
       "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
       "\n",
       "  court_level                                             issues  \\\n",
       "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
       "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
       "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
       "\n",
       "                                               facts  issues_topic  \\\n",
       "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
       "1  facts and surrounding circumstances including ...           8.0   \n",
       "2  Background \\nThe first appellants, a French co...           0.0   \n",
       "\n",
       "   facts_topic      target                                              Coram  \n",
       "0          7.0  Favourable  [V K Rajah, Andrew Phang Boon Leong, Chan Sek ...  \n",
       "1          3.0  Favourable  [Chao Hick Tin, V K Rajah, Andrew Phang Boon L...  \n",
       "2         12.0  No outcome  [Andrew Phang Boon Leong, Tan Lee Meng, Chan S...  "
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(ast.literal_eval)\n",
    "merged_df['Coram'] = merged_df['Coram'].apply(ast.literal_eval)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten areas_of_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_areas = []\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "\n",
    "    areas = row['area_of_law']\n",
    "    flat_areas = []\n",
    "    for main_area, sub_areas in areas.items():\n",
    "        flat_areas.append(main_area)\n",
    "        for sarea in sub_areas.copy():\n",
    "            if len(sarea) > 33:\n",
    "                sub_areas.remove(sarea)\n",
    "        flat_areas.extend(sub_areas)\n",
    "    all_areas.append(flat_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in all_areas:\n",
    "    if area == []:\n",
    "        print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "\n",
      "   facts_topic      target                                              Coram  \\\n",
      "0          7.0  Favourable  [V K Rajah, Andrew Phang Boon Leong, Chan Sek ...   \n",
      "1          3.0  Favourable  [Chao Hick Tin, V K Rajah, Andrew Phang Boon L...   \n",
      "2         12.0  No outcome  [Andrew Phang Boon Leong, Tan Lee Meng, Chan S...   \n",
      "\n",
      "   \"a larger sum being repaid\"  \"abet\"  ...  work injury compensation act  \\\n",
      "0                            0       0  ...                             0   \n",
      "1                            0       0  ...                             0   \n",
      "2                            0       0  ...                             0   \n",
      "\n",
      "   workmen’s compensation act  writ of  seizure and sale  writ of summons  \\\n",
      "0                           0                          0                0   \n",
      "1                           0                          0                0   \n",
      "2                           0                          0                0   \n",
      "\n",
      "   wrongful dismissal  young offenders]  “any claim  hereunder”  \\\n",
      "0                   0                 0                       0   \n",
      "1                   0                 0                       0   \n",
      "2                   0                 0                       0   \n",
      "\n",
      "   “any fire accidentally begin”  “charity proceedings”  \\\n",
      "0                              0                      0   \n",
      "1                              0                      0   \n",
      "2                              0                      0   \n",
      "\n",
      "   “rash” and “negligent”  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "\n",
      "[3 rows x 1377 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode aol\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(all_areas)\n",
    "\n",
    "binary_aol_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_aol_df = binary_aol_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([merged_df.drop('area_of_law', axis=1), binary_aol_df], axis=1)\n",
    "processed_df = processed_df[processed_df['Coram'].apply(lambda x: isinstance(x, list))]\n",
    "print(processed_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "3  2000_SGCA_12.pdf        SGCA   \n",
      "4  2000_SGCA_13.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "3  the appeals from the assistant registrar. In h...   \n",
      "4  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
      "4  facts. Mere assertion would not suffice. In ex...          28.0   \n",
      "\n",
      "   facts_topic        target  \"a larger sum being repaid\"  \"abet\"  \\\n",
      "0          7.0    Favourable                            0       0   \n",
      "1          3.0    Favourable                            0       0   \n",
      "2         12.0    No outcome                            0       0   \n",
      "3         10.0  Unfavourable                            0       0   \n",
      "4         13.0  Unfavourable                            0       0   \n",
      "\n",
      "   \"an interest in any matter\"  ...  Vincent Hoong  Vincent Leow  \\\n",
      "0                            0  ...              0             0   \n",
      "1                            0  ...              0             0   \n",
      "2                            0  ...              0             0   \n",
      "3                            0  ...              0             0   \n",
      "4                            0  ...              0             0   \n",
      "\n",
      "   Vinodh Coomaraswamy  Wong Li Kok, Alex  Woo Bih Li  Yap Yew Choh Kenneth  \\\n",
      "0                    0                  0           0                     0   \n",
      "1                    0                  0           0                     0   \n",
      "2                    0                  0           0                     0   \n",
      "3                    0                  0           0                     0   \n",
      "4                    0                  0           0                     0   \n",
      "\n",
      "   Yeong Zee Kin  Yong Pung How  Yong Pung How,  Zhuo Wenzhao  \n",
      "0              0              0               0             0  \n",
      "1              0              0               0             0  \n",
      "2              0              0               0             0  \n",
      "3              0              0               0             0  \n",
      "4              0              0               0             0  \n",
      "\n",
      "[5 rows x 1487 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode coram\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(processed_df['Coram'])\n",
    "\n",
    "binary_coram_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_coram_df = binary_coram_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([processed_df.drop('Coram', axis=1), binary_coram_df], axis=1)\n",
    "\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['SGCA'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGCA' else 0)\n",
    "processed_df['SGHC'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGHC' else 0)\n",
    "processed_df = processed_df.drop('court_level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'\\W*\\b(?!no)\\w{1,2}\\b', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    legal_stopwords = ('appellant', 'respondent', 'plaintiff', 'defendant', 'mr', 'mrs', 'dr', 'mdm', 'court','version', 'hr', 'would', 'case', 'sghc', 'court', 'sgca', 'slr', 'sgdc', 'also', 'first', 'person', 'statement', 'line', 'para', 'fact', 'one', 'may', 'time', 'could', 'next', 'legal', 'issues', 'issue')\n",
    "    stop_words.update(legal_stopwords)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "processed_df['processed_facts'] = processed_df['facts'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['facts'], inplace=True)\n",
    "\n",
    "processed_df['processed_issues'] = processed_df['issues'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['issues'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(columns=['target','casename'])\n",
    "y = processed_df['target']\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test_val = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test_val = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "#balanced dataset (target variable was imbalanced Favourable 5006 Unfavourable 2523 No outcome 984)\n",
    "#randomly found one online, can be changed -> need to check am i doing this right \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smt = SMOTE(random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "# X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "#split further from X_test_val into X_val and X_test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train_resampled\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_val\n",
    "X_val_text = tfidf_vectorizer.transform(X_val['processed_facts'].astype('U') + ' ' + X_val['processed_issues'].astype('U'))\n",
    "X_val_text = pd.DataFrame(X_val_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_val = X_val.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_val = pd.concat([X_val.reset_index(drop=True), X_val_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target variable to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_resampled = X_train_resampled.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_test = X_test.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_val = X_val.drop(columns=['processed_facts', 'processed_issues'])\n",
    "\n",
    "mapping = {'Favourable': 1, 'Unfavourable': 0, 'No outcome':0.5}\n",
    "\n",
    "y_train_resampled, y_test, y_val = y_train_resampled.copy().map(mapping), y_test.copy().map(mapping), y_val.copy().map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform modelling\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  epochs = 20\n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: torch.Size([8274, 2484])\n",
      "Shape of X_test: torch.Size([1019, 2484])\n",
      "Shape of X_val: torch.Size([1019, 2484])\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled = X_train_resampled.iloc[:, :].copy()\n",
    "X_train_resampled = torch.tensor(X_train_resampled.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_train_resampled: {X_train_resampled.shape}')\n",
    "\n",
    "X_test = X_test.iloc[:, :].copy()\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "\n",
    "X_val = X_val.iloc[:,:].copy()\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8274, 1, 2484])\n"
     ]
    }
   ],
   "source": [
    "y_train_resampled, y_test, y_val = torch.tensor(y_train_resampled.values).to(device), torch.tensor(y_test.values).to(device), torch.tensor(y_val.values).to(device)\n",
    "\n",
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0],1,X_train_resampled.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])\n",
    "print(X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, 1,1, bias=True)\n",
    "        # Define the first 1D convolution layer. Takes 1 input channel, outputs 32 channels, kernel size is 3, stride is 1, padding is 1.\n",
    "        self.Bn1 = nn.BatchNorm1d(64)\n",
    "        # Apply Batch Normalization to the output of the first convolutional layer.\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # Apply 1D Average Pooling after the first Batch Normalization. The kernel size and stride are 2.\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3, 1,1, bias=True)\n",
    "        self.Bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(39744, 512, bias=True)\n",
    "        # Define the first fully connected layer. It takes 25472 39744 79488 inputs and outputs 100 nodes.\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 128, bias=True)\n",
    "        # Define the second fully connected layer. It takes 100 inputs and outputs 50 nodes.\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 3, bias=True)\n",
    "        # Define the third fully connected layer (output layer). It takes 50 inputs and outputs 3 nodes.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.Bn1(self.conv1(x)))\n",
    "        # Pass the input through the first convolutional layer, then Batch Normalization, and then apply ReLU activation.\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        # Apply Average Pooling to the output of the previous step.\n",
    "        x = F.relu(self.Bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        # Flatten the output from the previous step. This is necessary because fully connected layers expect a 1D input.\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output through the first fully connected layer.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Pass the output through the second fully connected layer with tanh activation.\n",
    "        x = self.fc3(x)\n",
    "        # Pass the output through the third fully connected layer. This is the output of the network.\n",
    "        return x\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n",
    "        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous training step\n",
    "        output = model(data)  # Run forward pass (model predictions)\n",
    "        #print(output.shape)\n",
    "        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n",
    "        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n",
    "        for data, target in test_loader:  # Loop over each batch from the testing set\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "            target = target.long()  # Convert target to long after adjusting value\n",
    "            output = model(data)  # Run forward pass (model predictions)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # Sum up the batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    return correct  # Return the number of correctly classified samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([64, 1, 3])\n",
      "conv1.bias \t torch.Size([64])\n",
      "Bn1.weight \t torch.Size([64])\n",
      "Bn1.bias \t torch.Size([64])\n",
      "Bn1.running_mean \t torch.Size([64])\n",
      "Bn1.running_var \t torch.Size([64])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 64, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "Bn2.weight \t torch.Size([64])\n",
      "Bn2.bias \t torch.Size([64])\n",
      "Bn2.running_mean \t torch.Size([64])\n",
      "Bn2.running_var \t torch.Size([64])\n",
      "Bn2.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([512, 39744])\n",
      "fc1.bias \t torch.Size([512])\n",
      "fc2.weight \t torch.Size([128, 512])\n",
      "fc2.bias \t torch.Size([128])\n",
      "fc3.weight \t torch.Size([3, 128])\n",
      "fc3.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_resampled, y_train_resampled)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.134500\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.717784\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.663773\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.639536\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.551335\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.554366\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.664776\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.665908\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.586407\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.624767\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.515241\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.670607\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.624561\n",
      "\n",
      "Test set: Average loss: 0.7224, Accuracy: 507/1019 (50%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.660024\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.624892\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.656390\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.599287\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.691674\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.539923\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.577140\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.555897\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.577146\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.590387\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.559351\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.508887\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.518235\n",
      "\n",
      "Test set: Average loss: 0.7727, Accuracy: 521/1019 (51%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.514470\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.664947\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.515095\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.619180\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.514198\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.601301\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.587675\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.760063\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.543024\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.614975\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.633406\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.556010\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.500372\n",
      "\n",
      "Test set: Average loss: 0.7858, Accuracy: 528/1019 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.527488\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.606864\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.580867\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.497102\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.485350\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.579664\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.501398\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.519305\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.479799\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.563091\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.460401\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.550116\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.449815\n",
      "\n",
      "Test set: Average loss: 0.8222, Accuracy: 520/1019 (51%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.451314\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.378011\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.412982\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.310076\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.433385\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.349266\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.481950\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.394111\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.489222\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.368007\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.507388\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.462337\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.496912\n",
      "\n",
      "Test set: Average loss: 0.8476, Accuracy: 540/1019 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.386176\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.391672\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.370332\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.446657\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.316980\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.389328\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.306142\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.310388\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.353488\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.327262\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.444706\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.387968\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.442957\n",
      "\n",
      "Test set: Average loss: 0.9004, Accuracy: 547/1019 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.325312\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.318418\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.266251\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.293001\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.244431\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.436565\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.296611\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.220516\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.340107\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.305380\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.321132\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.370354\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.281233\n",
      "\n",
      "Test set: Average loss: 0.9771, Accuracy: 548/1019 (54%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.349037\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.270368\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.247460\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.295943\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.255235\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.195970\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.282795\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.239012\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.264683\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.326284\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.241801\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.245067\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.296324\n",
      "\n",
      "Test set: Average loss: 1.0116, Accuracy: 550/1019 (54%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.252684\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.172230\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.234690\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.279252\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.382044\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.287353\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.217415\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.320289\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.277532\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.133770\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.240793\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.217725\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.223283\n",
      "\n",
      "Test set: Average loss: 1.0925, Accuracy: 538/1019 (53%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.173330\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.189907\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.146680\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.249771\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.247533\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.334026\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.184487\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.217966\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.152891\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.193847\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.244522\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.291693\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.221246\n",
      "\n",
      "Test set: Average loss: 1.0927, Accuracy: 561/1019 (55%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.264512\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.195075\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.196318\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.239104\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.274160\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.281793\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.269067\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.153565\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.159435\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.243845\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.257084\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.148188\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.301926\n",
      "\n",
      "Test set: Average loss: 1.1049, Accuracy: 553/1019 (54%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.168548\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.251439\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.296794\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.208932\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.231988\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.209359\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.246739\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.198940\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.223002\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.228199\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.149263\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.230241\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.205701\n",
      "\n",
      "Test set: Average loss: 1.1378, Accuracy: 552/1019 (54%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.227189\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.149021\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.187139\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.213559\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.218412\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.266714\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.185863\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.123594\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.272299\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.201215\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.321989\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.152346\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.282378\n",
      "\n",
      "Test set: Average loss: 1.1495, Accuracy: 550/1019 (54%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.142398\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.150217\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.182615\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.159751\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.236217\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.187027\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.177104\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.193972\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.109549\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.231320\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.226256\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.231180\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.178436\n",
      "\n",
      "Test set: Average loss: 1.1583, Accuracy: 549/1019 (54%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.154032\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.214163\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.147651\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.153299\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.188070\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.145520\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.234487\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.196478\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.241241\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.195194\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.147292\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.210397\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.141856\n",
      "\n",
      "Test set: Average loss: 1.1597, Accuracy: 551/1019 (54%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.304230\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.171673\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.185213\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.224853\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.200898\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.207688\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.157163\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.159036\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.172314\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.212082\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.151085\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.219374\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.110497\n",
      "\n",
      "Test set: Average loss: 1.1572, Accuracy: 552/1019 (54%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.121897\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.184083\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.148304\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.190794\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.199162\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.129596\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.245722\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.269187\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.163360\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.208228\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.149519\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.106500\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.162587\n",
      "\n",
      "Test set: Average loss: 1.1723, Accuracy: 555/1019 (54%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.158973\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.215753\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.170869\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.175425\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.215055\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.177901\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.090672\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.201144\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.214139\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.250509\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.174277\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.161216\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.151317\n",
      "\n",
      "Test set: Average loss: 1.1710, Accuracy: 553/1019 (54%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.162659\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.210764\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.210200\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.202219\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.171202\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.205171\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.122415\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.184947\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.118895\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.123424\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.121960\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.229221\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.130405\n",
      "\n",
      "Test set: Average loss: 1.1579, Accuracy: 553/1019 (54%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.173159\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.178485\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.258402\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.244232\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.170384\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.195082\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.266185\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.199318\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.183680\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.135245\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.188068\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.261195\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.293756\n",
      "\n",
      "Test set: Average loss: 1.1643, Accuracy: 551/1019 (54%)\n",
      "\n",
      "561\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    ACC_ = test(model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(model.state_dict(), \"Baseline_CNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 57.7036310107949\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = model(data)\n",
    "        #pred = torch.argmax(output_test, 1)\n",
    "        pred = output_test.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "\n",
    "        \n",
    "        val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        #correct_val += (pred == target).sum().item()\n",
    "        correct_val += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "        \n",
    "        total_val += target.size(0)\n",
    "    \n",
    "    test_accuracy = (correct_val / total_val) * 100\n",
    "            \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(2484, 128, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(128)\n",
    "        # self.fc2 = nn.Linear(128, 128, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(128)\n",
    "        # self.fc3 = nn.Linear(128, 5, bias=True)\n",
    "\n",
    "        # self.fc1 = nn.Linear(2484, 1024, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(1024)\n",
    "        # self.fc2 = nn.Linear(1024, 512, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(512)\n",
    "        # self.fc3 = nn.Linear(512, 256, bias=True)\n",
    "        # self.Bn3 = nn.BatchNorm1d(256)\n",
    "        # self.fc4 = nn.Linear(256, 128, bias=True)\n",
    "        # self.Bn4 = nn.BatchNorm1d(128)\n",
    "        # self.fc5 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(2484, 512, bias=True)\n",
    "        self.Bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(512, 128, bias=True)\n",
    "        self.fc3 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.Bn1(self.fc1(x)))\n",
    "        # x = F.tanh(self.Bn2(self.fc2(x)))\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.fc1(x))\n",
    "        # x = F.leaky_relu(self.fc2(x))\n",
    "        # x = F.leaky_relu(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc4(x))\n",
    "        # x = self.fc5(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.tanh(self.fc1(x)) # [leaky_relu, tanh, relu,]\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight \t torch.Size([512, 2484])\n",
      "fc1.bias \t torch.Size([512])\n",
      "Bn1.weight \t torch.Size([256])\n",
      "Bn1.bias \t torch.Size([256])\n",
      "Bn1.running_mean \t torch.Size([256])\n",
      "Bn1.running_var \t torch.Size([256])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "fc2.weight \t torch.Size([128, 512])\n",
      "fc2.bias \t torch.Size([128])\n",
      "fc3.weight \t torch.Size([3, 128])\n",
      "fc3.bias \t torch.Size([3])\n",
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.144096\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.591128\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.659717\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.637089\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.598890\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.621578\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.602656\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.615508\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.686932\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.652801\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.601091\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.513449\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.574737\n",
      "\n",
      "Test set: Average loss: 0.9043, Accuracy: 466/1019 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.625196\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.540608\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.514788\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.464694\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.527348\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.571075\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.608517\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.416023\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.598106\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.533073\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.480460\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.562345\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.515361\n",
      "\n",
      "Test set: Average loss: 0.7409, Accuracy: 537/1019 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.510362\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.429560\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.413808\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.464106\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.467952\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.524088\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.457678\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.678763\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.428776\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.568854\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.586704\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.454760\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.559691\n",
      "\n",
      "Test set: Average loss: 0.8130, Accuracy: 525/1019 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.446254\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.341359\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.535521\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.413134\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.563724\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.351350\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.332520\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.490715\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.442139\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.368996\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.354835\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.497785\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.478773\n",
      "\n",
      "Test set: Average loss: 0.9427, Accuracy: 536/1019 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.300998\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.391380\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.389028\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.573730\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.325227\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.414783\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.413460\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.498807\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.369913\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.272909\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.421780\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.553532\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.407320\n",
      "\n",
      "Test set: Average loss: 0.9253, Accuracy: 545/1019 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.337476\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.386359\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.434817\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.340883\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.286864\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.352078\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.324878\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.413646\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.322184\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.344062\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.403441\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.348992\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.360235\n",
      "\n",
      "Test set: Average loss: 1.0666, Accuracy: 554/1019 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.269191\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.318814\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.246343\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.193961\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.253149\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.272330\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.302685\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.408747\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.300993\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.355405\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.172651\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.264682\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.349531\n",
      "\n",
      "Test set: Average loss: 1.1016, Accuracy: 550/1019 (54%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.257087\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.270247\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.278059\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.208732\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.230168\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.189265\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.291188\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.327778\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.465877\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.482833\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.252025\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.309847\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.337604\n",
      "\n",
      "Test set: Average loss: 1.2469, Accuracy: 573/1019 (56%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.278554\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.201720\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.290454\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.152320\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.298668\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.252314\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.181425\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.332476\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.430491\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.262352\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.272417\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.284263\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.220035\n",
      "\n",
      "Test set: Average loss: 1.4497, Accuracy: 555/1019 (54%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.166448\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.132067\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.213159\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.222318\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.317187\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.264981\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.264913\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.190150\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.289548\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.224466\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.282692\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.189489\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.329829\n",
      "\n",
      "Test set: Average loss: 1.3761, Accuracy: 563/1019 (55%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.216788\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.220935\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.199135\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.079475\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.118267\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.134767\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.209565\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.175697\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.149763\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.351301\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.328691\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.388703\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.148398\n",
      "\n",
      "Test set: Average loss: 1.6501, Accuracy: 558/1019 (55%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.088030\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.168931\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.170107\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.159161\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.213372\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.151190\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.227589\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.197462\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.317766\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.228947\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.095872\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.155563\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.245609\n",
      "\n",
      "Test set: Average loss: 1.6142, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.164204\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.122563\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.112346\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.152036\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.077023\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.146233\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.135326\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.098915\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.282919\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.147741\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.141697\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.081373\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.128392\n",
      "\n",
      "Test set: Average loss: 1.7065, Accuracy: 574/1019 (56%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.061978\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.093049\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.073246\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.211850\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.120688\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.217451\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.103348\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.075388\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.114320\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.077610\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.091969\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.205663\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.180729\n",
      "\n",
      "Test set: Average loss: 1.8795, Accuracy: 569/1019 (56%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.165696\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.113459\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.103383\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.181091\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.052874\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.043493\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.104581\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.053355\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.102458\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.189194\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.153335\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.157744\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.065375\n",
      "\n",
      "Test set: Average loss: 1.7935, Accuracy: 567/1019 (56%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.165693\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.157382\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.205832\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.034632\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.111201\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.078135\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.090278\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.159436\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.070372\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.072910\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.108162\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.105169\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.103306\n",
      "\n",
      "Test set: Average loss: 1.9715, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.073738\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.088898\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.074882\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.114307\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.149007\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.066079\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.053714\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.197973\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.045736\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.082869\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.111240\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.178065\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.091870\n",
      "\n",
      "Test set: Average loss: 2.0132, Accuracy: 580/1019 (57%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.080005\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.077100\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.022260\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.058934\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.031629\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.021345\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.111898\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.027325\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.134367\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.052373\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.090575\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.053379\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.224937\n",
      "\n",
      "Test set: Average loss: 2.2072, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.034369\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.297416\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.118109\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.164532\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.043562\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.067502\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.071764\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.075907\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.087371\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.068724\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.179662\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.056441\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.046467\n",
      "\n",
      "Test set: Average loss: 2.2023, Accuracy: 589/1019 (58%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.060618\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.042289\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.031317\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.036602\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.043819\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.018168\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.102273\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.060927\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.034336\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.012616\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.063110\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.101758\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.029475\n",
      "\n",
      "Test set: Average loss: 2.2883, Accuracy: 605/1019 (59%)\n",
      "\n",
      "Train Epoch: 21 [0/8274 (0%)]\tLoss: 0.081450\n",
      "Train Epoch: 21 [640/8274 (8%)]\tLoss: 0.038360\n",
      "Train Epoch: 21 [1280/8274 (15%)]\tLoss: 0.101007\n",
      "Train Epoch: 21 [1920/8274 (23%)]\tLoss: 0.020144\n",
      "Train Epoch: 21 [2560/8274 (31%)]\tLoss: 0.059641\n",
      "Train Epoch: 21 [3200/8274 (38%)]\tLoss: 0.053674\n",
      "Train Epoch: 21 [3840/8274 (46%)]\tLoss: 0.055626\n",
      "Train Epoch: 21 [4480/8274 (54%)]\tLoss: 0.042002\n",
      "Train Epoch: 21 [5120/8274 (62%)]\tLoss: 0.039061\n",
      "Train Epoch: 21 [5760/8274 (69%)]\tLoss: 0.043903\n",
      "Train Epoch: 21 [6400/8274 (77%)]\tLoss: 0.054774\n",
      "Train Epoch: 21 [7040/8274 (85%)]\tLoss: 0.023870\n",
      "Train Epoch: 21 [7680/8274 (92%)]\tLoss: 0.065674\n",
      "\n",
      "Test set: Average loss: 2.4667, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 22 [0/8274 (0%)]\tLoss: 0.022672\n",
      "Train Epoch: 22 [640/8274 (8%)]\tLoss: 0.067600\n",
      "Train Epoch: 22 [1280/8274 (15%)]\tLoss: 0.031368\n",
      "Train Epoch: 22 [1920/8274 (23%)]\tLoss: 0.032587\n",
      "Train Epoch: 22 [2560/8274 (31%)]\tLoss: 0.020036\n",
      "Train Epoch: 22 [3200/8274 (38%)]\tLoss: 0.033595\n",
      "Train Epoch: 22 [3840/8274 (46%)]\tLoss: 0.031401\n",
      "Train Epoch: 22 [4480/8274 (54%)]\tLoss: 0.044433\n",
      "Train Epoch: 22 [5120/8274 (62%)]\tLoss: 0.012348\n",
      "Train Epoch: 22 [5760/8274 (69%)]\tLoss: 0.034236\n",
      "Train Epoch: 22 [6400/8274 (77%)]\tLoss: 0.017131\n",
      "Train Epoch: 22 [7040/8274 (85%)]\tLoss: 0.059666\n",
      "Train Epoch: 22 [7680/8274 (92%)]\tLoss: 0.058031\n",
      "\n",
      "Test set: Average loss: 2.3352, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 23 [0/8274 (0%)]\tLoss: 0.034518\n",
      "Train Epoch: 23 [640/8274 (8%)]\tLoss: 0.056811\n",
      "Train Epoch: 23 [1280/8274 (15%)]\tLoss: 0.073154\n",
      "Train Epoch: 23 [1920/8274 (23%)]\tLoss: 0.020761\n",
      "Train Epoch: 23 [2560/8274 (31%)]\tLoss: 0.102095\n",
      "Train Epoch: 23 [3200/8274 (38%)]\tLoss: 0.093329\n",
      "Train Epoch: 23 [3840/8274 (46%)]\tLoss: 0.075587\n",
      "Train Epoch: 23 [4480/8274 (54%)]\tLoss: 0.052083\n",
      "Train Epoch: 23 [5120/8274 (62%)]\tLoss: 0.032461\n",
      "Train Epoch: 23 [5760/8274 (69%)]\tLoss: 0.045350\n",
      "Train Epoch: 23 [6400/8274 (77%)]\tLoss: 0.015498\n",
      "Train Epoch: 23 [7040/8274 (85%)]\tLoss: 0.039392\n",
      "Train Epoch: 23 [7680/8274 (92%)]\tLoss: 0.039548\n",
      "\n",
      "Test set: Average loss: 2.4957, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 24 [0/8274 (0%)]\tLoss: 0.013583\n",
      "Train Epoch: 24 [640/8274 (8%)]\tLoss: 0.015115\n",
      "Train Epoch: 24 [1280/8274 (15%)]\tLoss: 0.022435\n",
      "Train Epoch: 24 [1920/8274 (23%)]\tLoss: 0.023230\n",
      "Train Epoch: 24 [2560/8274 (31%)]\tLoss: 0.009442\n",
      "Train Epoch: 24 [3200/8274 (38%)]\tLoss: 0.020905\n",
      "Train Epoch: 24 [3840/8274 (46%)]\tLoss: 0.019933\n",
      "Train Epoch: 24 [4480/8274 (54%)]\tLoss: 0.126925\n",
      "Train Epoch: 24 [5120/8274 (62%)]\tLoss: 0.012094\n",
      "Train Epoch: 24 [5760/8274 (69%)]\tLoss: 0.177631\n",
      "Train Epoch: 24 [6400/8274 (77%)]\tLoss: 0.008758\n",
      "Train Epoch: 24 [7040/8274 (85%)]\tLoss: 0.068008\n",
      "Train Epoch: 24 [7680/8274 (92%)]\tLoss: 0.011037\n",
      "\n",
      "Test set: Average loss: 2.6644, Accuracy: 590/1019 (58%)\n",
      "\n",
      "Train Epoch: 25 [0/8274 (0%)]\tLoss: 0.007161\n",
      "Train Epoch: 25 [640/8274 (8%)]\tLoss: 0.012277\n",
      "Train Epoch: 25 [1280/8274 (15%)]\tLoss: 0.007818\n",
      "Train Epoch: 25 [1920/8274 (23%)]\tLoss: 0.020836\n",
      "Train Epoch: 25 [2560/8274 (31%)]\tLoss: 0.010443\n",
      "Train Epoch: 25 [3200/8274 (38%)]\tLoss: 0.008561\n",
      "Train Epoch: 25 [3840/8274 (46%)]\tLoss: 0.030838\n",
      "Train Epoch: 25 [4480/8274 (54%)]\tLoss: 0.052377\n",
      "Train Epoch: 25 [5120/8274 (62%)]\tLoss: 0.026540\n",
      "Train Epoch: 25 [5760/8274 (69%)]\tLoss: 0.006049\n",
      "Train Epoch: 25 [6400/8274 (77%)]\tLoss: 0.041571\n",
      "Train Epoch: 25 [7040/8274 (85%)]\tLoss: 0.012301\n",
      "Train Epoch: 25 [7680/8274 (92%)]\tLoss: 0.024669\n",
      "\n",
      "Test set: Average loss: 2.7589, Accuracy: 591/1019 (58%)\n",
      "\n",
      "Train Epoch: 26 [0/8274 (0%)]\tLoss: 0.007807\n",
      "Train Epoch: 26 [640/8274 (8%)]\tLoss: 0.008154\n",
      "Train Epoch: 26 [1280/8274 (15%)]\tLoss: 0.005541\n",
      "Train Epoch: 26 [1920/8274 (23%)]\tLoss: 0.027546\n",
      "Train Epoch: 26 [2560/8274 (31%)]\tLoss: 0.022375\n",
      "Train Epoch: 26 [3200/8274 (38%)]\tLoss: 0.007919\n",
      "Train Epoch: 26 [3840/8274 (46%)]\tLoss: 0.004708\n",
      "Train Epoch: 26 [4480/8274 (54%)]\tLoss: 0.007779\n",
      "Train Epoch: 26 [5120/8274 (62%)]\tLoss: 0.026501\n",
      "Train Epoch: 26 [5760/8274 (69%)]\tLoss: 0.039110\n",
      "Train Epoch: 26 [6400/8274 (77%)]\tLoss: 0.031134\n",
      "Train Epoch: 26 [7040/8274 (85%)]\tLoss: 0.023837\n",
      "Train Epoch: 26 [7680/8274 (92%)]\tLoss: 0.011239\n",
      "\n",
      "Test set: Average loss: 2.8549, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 27 [0/8274 (0%)]\tLoss: 0.014370\n",
      "Train Epoch: 27 [640/8274 (8%)]\tLoss: 0.013153\n",
      "Train Epoch: 27 [1280/8274 (15%)]\tLoss: 0.028805\n",
      "Train Epoch: 27 [1920/8274 (23%)]\tLoss: 0.022800\n",
      "Train Epoch: 27 [2560/8274 (31%)]\tLoss: 0.008444\n",
      "Train Epoch: 27 [3200/8274 (38%)]\tLoss: 0.019067\n",
      "Train Epoch: 27 [3840/8274 (46%)]\tLoss: 0.009789\n",
      "Train Epoch: 27 [4480/8274 (54%)]\tLoss: 0.008590\n",
      "Train Epoch: 27 [5120/8274 (62%)]\tLoss: 0.078325\n",
      "Train Epoch: 27 [5760/8274 (69%)]\tLoss: 0.133475\n",
      "Train Epoch: 27 [6400/8274 (77%)]\tLoss: 0.006276\n",
      "Train Epoch: 27 [7040/8274 (85%)]\tLoss: 0.134562\n",
      "Train Epoch: 27 [7680/8274 (92%)]\tLoss: 0.066697\n",
      "\n",
      "Test set: Average loss: 2.9342, Accuracy: 589/1019 (58%)\n",
      "\n",
      "Train Epoch: 28 [0/8274 (0%)]\tLoss: 0.086127\n",
      "Train Epoch: 28 [640/8274 (8%)]\tLoss: 0.036908\n",
      "Train Epoch: 28 [1280/8274 (15%)]\tLoss: 0.022360\n",
      "Train Epoch: 28 [1920/8274 (23%)]\tLoss: 0.079906\n",
      "Train Epoch: 28 [2560/8274 (31%)]\tLoss: 0.046611\n",
      "Train Epoch: 28 [3200/8274 (38%)]\tLoss: 0.117060\n",
      "Train Epoch: 28 [3840/8274 (46%)]\tLoss: 0.017903\n",
      "Train Epoch: 28 [4480/8274 (54%)]\tLoss: 0.056925\n",
      "Train Epoch: 28 [5120/8274 (62%)]\tLoss: 0.081049\n",
      "Train Epoch: 28 [5760/8274 (69%)]\tLoss: 0.138689\n",
      "Train Epoch: 28 [6400/8274 (77%)]\tLoss: 0.018585\n",
      "Train Epoch: 28 [7040/8274 (85%)]\tLoss: 0.189612\n",
      "Train Epoch: 28 [7680/8274 (92%)]\tLoss: 0.070425\n",
      "\n",
      "Test set: Average loss: 2.7016, Accuracy: 601/1019 (59%)\n",
      "\n",
      "Train Epoch: 29 [0/8274 (0%)]\tLoss: 0.058079\n",
      "Train Epoch: 29 [640/8274 (8%)]\tLoss: 0.026835\n",
      "Train Epoch: 29 [1280/8274 (15%)]\tLoss: 0.148137\n",
      "Train Epoch: 29 [1920/8274 (23%)]\tLoss: 0.050415\n",
      "Train Epoch: 29 [2560/8274 (31%)]\tLoss: 0.137175\n",
      "Train Epoch: 29 [3200/8274 (38%)]\tLoss: 0.019211\n",
      "Train Epoch: 29 [3840/8274 (46%)]\tLoss: 0.016074\n",
      "Train Epoch: 29 [4480/8274 (54%)]\tLoss: 0.020260\n",
      "Train Epoch: 29 [5120/8274 (62%)]\tLoss: 0.005042\n",
      "Train Epoch: 29 [5760/8274 (69%)]\tLoss: 0.054763\n",
      "Train Epoch: 29 [6400/8274 (77%)]\tLoss: 0.151654\n",
      "Train Epoch: 29 [7040/8274 (85%)]\tLoss: 0.012993\n",
      "Train Epoch: 29 [7680/8274 (92%)]\tLoss: 0.011769\n",
      "\n",
      "Test set: Average loss: 2.7261, Accuracy: 577/1019 (57%)\n",
      "\n",
      "Train Epoch: 30 [0/8274 (0%)]\tLoss: 0.014381\n",
      "Train Epoch: 30 [640/8274 (8%)]\tLoss: 0.037837\n",
      "Train Epoch: 30 [1280/8274 (15%)]\tLoss: 0.022595\n",
      "Train Epoch: 30 [1920/8274 (23%)]\tLoss: 0.008967\n",
      "Train Epoch: 30 [2560/8274 (31%)]\tLoss: 0.012117\n",
      "Train Epoch: 30 [3200/8274 (38%)]\tLoss: 0.007264\n",
      "Train Epoch: 30 [3840/8274 (46%)]\tLoss: 0.010802\n",
      "Train Epoch: 30 [4480/8274 (54%)]\tLoss: 0.036873\n",
      "Train Epoch: 30 [5120/8274 (62%)]\tLoss: 0.015675\n",
      "Train Epoch: 30 [5760/8274 (69%)]\tLoss: 0.006613\n",
      "Train Epoch: 30 [6400/8274 (77%)]\tLoss: 0.041388\n",
      "Train Epoch: 30 [7040/8274 (85%)]\tLoss: 0.007792\n",
      "Train Epoch: 30 [7680/8274 (92%)]\tLoss: 0.013853\n",
      "\n",
      "Test set: Average loss: 2.7536, Accuracy: 594/1019 (58%)\n",
      "\n",
      "Train Epoch: 31 [0/8274 (0%)]\tLoss: 0.036755\n",
      "Train Epoch: 31 [640/8274 (8%)]\tLoss: 0.011611\n",
      "Train Epoch: 31 [1280/8274 (15%)]\tLoss: 0.012398\n",
      "Train Epoch: 31 [1920/8274 (23%)]\tLoss: 0.014843\n",
      "Train Epoch: 31 [2560/8274 (31%)]\tLoss: 0.018913\n",
      "Train Epoch: 31 [3200/8274 (38%)]\tLoss: 0.035337\n",
      "Train Epoch: 31 [3840/8274 (46%)]\tLoss: 0.009089\n",
      "Train Epoch: 31 [4480/8274 (54%)]\tLoss: 0.005416\n",
      "Train Epoch: 31 [5120/8274 (62%)]\tLoss: 0.006482\n",
      "Train Epoch: 31 [5760/8274 (69%)]\tLoss: 0.008757\n",
      "Train Epoch: 31 [6400/8274 (77%)]\tLoss: 0.002251\n",
      "Train Epoch: 31 [7040/8274 (85%)]\tLoss: 0.022499\n",
      "Train Epoch: 31 [7680/8274 (92%)]\tLoss: 0.007302\n",
      "\n",
      "Test set: Average loss: 2.9249, Accuracy: 596/1019 (58%)\n",
      "\n",
      "Train Epoch: 32 [0/8274 (0%)]\tLoss: 0.004376\n",
      "Train Epoch: 32 [640/8274 (8%)]\tLoss: 0.053395\n",
      "Train Epoch: 32 [1280/8274 (15%)]\tLoss: 0.007430\n",
      "Train Epoch: 32 [1920/8274 (23%)]\tLoss: 0.010962\n",
      "Train Epoch: 32 [2560/8274 (31%)]\tLoss: 0.006542\n",
      "Train Epoch: 32 [3200/8274 (38%)]\tLoss: 0.003529\n",
      "Train Epoch: 32 [3840/8274 (46%)]\tLoss: 0.075894\n",
      "Train Epoch: 32 [4480/8274 (54%)]\tLoss: 0.012384\n",
      "Train Epoch: 32 [5120/8274 (62%)]\tLoss: 0.003031\n",
      "Train Epoch: 32 [5760/8274 (69%)]\tLoss: 0.006107\n",
      "Train Epoch: 32 [6400/8274 (77%)]\tLoss: 0.007841\n",
      "Train Epoch: 32 [7040/8274 (85%)]\tLoss: 0.004080\n",
      "Train Epoch: 32 [7680/8274 (92%)]\tLoss: 0.005105\n",
      "\n",
      "Test set: Average loss: 3.0370, Accuracy: 581/1019 (57%)\n",
      "\n",
      "Train Epoch: 33 [0/8274 (0%)]\tLoss: 0.002988\n",
      "Train Epoch: 33 [640/8274 (8%)]\tLoss: 0.002767\n",
      "Train Epoch: 33 [1280/8274 (15%)]\tLoss: 0.010811\n",
      "Train Epoch: 33 [1920/8274 (23%)]\tLoss: 0.007541\n",
      "Train Epoch: 33 [2560/8274 (31%)]\tLoss: 0.017338\n",
      "Train Epoch: 33 [3200/8274 (38%)]\tLoss: 0.001549\n",
      "Train Epoch: 33 [3840/8274 (46%)]\tLoss: 0.005696\n",
      "Train Epoch: 33 [4480/8274 (54%)]\tLoss: 0.001752\n",
      "Train Epoch: 33 [5120/8274 (62%)]\tLoss: 0.012058\n",
      "Train Epoch: 33 [5760/8274 (69%)]\tLoss: 0.002786\n",
      "Train Epoch: 33 [6400/8274 (77%)]\tLoss: 0.010941\n",
      "Train Epoch: 33 [7040/8274 (85%)]\tLoss: 0.001580\n",
      "Train Epoch: 33 [7680/8274 (92%)]\tLoss: 0.003238\n",
      "\n",
      "Test set: Average loss: 3.1142, Accuracy: 590/1019 (58%)\n",
      "\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "class DNNArgs:\n",
    "  epochs = 33 # Tuning the epoch to 33 using validation set \n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "DNNargs = DNNArgs()\n",
    "\n",
    "torch.manual_seed(DNNargs.seed)\n",
    "\n",
    "dnn_model = DNN().to(device)\n",
    "\n",
    "for param_tensor in dnn_model.state_dict():\n",
    "        print(param_tensor, \"\\t\", dnn_model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "dnn_optimizer = optim.Adam(dnn_model.parameters(), lr=DNNargs.lr)\n",
    "\n",
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, DNNargs.epochs + 1):\n",
    "    train(DNNargs, dnn_model, device, train_loader, dnn_optimizer, epoch)\n",
    "    ACC_ = test(dnn_model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(dnn_model.state_dict(), \"Baseline_DNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 60.15701668302257\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "dnn_model.eval()\n",
    "dnn_correct_val = 0\n",
    "dnn_total_val = 0\n",
    "dnn_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = dnn_model(data)\n",
    "        pred = torch.argmax(output_test, 1)\n",
    "        \n",
    "        dnn_val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        dnn_correct_val += (pred == target).sum().item()\n",
    "        \n",
    "        dnn_total_val += target.size(0)\n",
    "    \n",
    "    test_accuracy = (dnn_correct_val / dnn_total_val) * 100\n",
    "            \n",
    "    dnn_val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(['target', 'casename'], axis=1)\n",
    "y = processed_df['target'] \n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "# Handle imbalanced classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Param Tuning (Grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.83\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Optionally, use the best estimator to make predictions\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(best_rf, 'model/rf_model.joblib')\n",
    "best_rf = load('model/rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>division</td>\n",
       "      <td>0.009125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>anything</td>\n",
       "      <td>0.004404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>july</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>criminal law</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>argued</td>\n",
       "      <td>0.002695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>adverse possession</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>international taxation</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>hdb flat</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>courts and jurisdiction</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>misrepresentation act</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  Importance\n",
       "1788                 division    0.009125\n",
       "1542                 anything    0.004404\n",
       "1985                     july    0.003800\n",
       "487              criminal law    0.003110\n",
       "1567                   argued    0.002695\n",
       "...                       ...         ...\n",
       "226        adverse possession    0.000053\n",
       "825    international taxation    0.000052\n",
       "732                  hdb flat    0.000050\n",
       "473   courts and jurisdiction    0.000050\n",
       "940     misrepresentation act    0.000050\n",
       "\n",
       "[1166 rows x 2 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_train_resampled.columns\n",
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df = feature_importance_df[(feature_importance_df['Importance']) > 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>division</th>\n",
       "      <th>anything</th>\n",
       "      <th>july</th>\n",
       "      <th>criminal law</th>\n",
       "      <th>argued</th>\n",
       "      <th>majority</th>\n",
       "      <th>allegedly</th>\n",
       "      <th>failed</th>\n",
       "      <th>ordinary</th>\n",
       "      <th>iii</th>\n",
       "      <th>...</th>\n",
       "      <th>res judicata</th>\n",
       "      <th>family violence</th>\n",
       "      <th>admiralty and shipping</th>\n",
       "      <th>advice</th>\n",
       "      <th>advice</th>\n",
       "      <th>adverse possession</th>\n",
       "      <th>international taxation</th>\n",
       "      <th>hdb flat</th>\n",
       "      <th>courts and jurisdiction</th>\n",
       "      <th>misrepresentation act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029518</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.174046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8269</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.019155</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8273</th>\n",
       "      <td>0.007187</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8274 rows × 1257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      division  anything      july  criminal law    argued  majority  \\\n",
       "0     0.009229  0.000000  0.000000             0  0.005410  0.000000   \n",
       "1     0.001328  0.001974  0.000802             0  0.003113  0.002480   \n",
       "2     0.000000  0.017659  0.038272             0  0.000000  0.000000   \n",
       "3     0.000000  0.001575  0.174046             0  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000             0  0.008033  0.000000   \n",
       "...        ...       ...       ...           ...       ...       ...   \n",
       "8269  0.000000  0.001823  0.017783             0  0.030196  0.002291   \n",
       "8270  0.000000  0.000000  0.000000             0  0.000000  0.000000   \n",
       "8271  0.000000  0.000000  0.005170             0  0.020067  0.000000   \n",
       "8272  0.000000  0.000000  0.005254             0  0.010197  0.000000   \n",
       "8273  0.007187  0.005342  0.008684             0  0.000000  0.000000   \n",
       "\n",
       "      allegedly    failed  ordinary       iii  ...  res judicata  \\\n",
       "0      0.000000  0.005148  0.000000  0.029518  ...             0   \n",
       "1      0.000000  0.002962  0.012157  0.005308  ...             0   \n",
       "2      0.006375  0.022086  0.006592  0.000000  ...             0   \n",
       "3      0.017053  0.009453  0.001763  0.000000  ...             0   \n",
       "4      0.000000  0.007644  0.000000  0.010957  ...             0   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "8269   0.001975  0.019155  0.004084  0.005884  ...             0   \n",
       "8270   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8271   0.006889  0.038189  0.007124  0.000000  ...             0   \n",
       "8272   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8273   0.000000  0.016036  0.000000  0.000000  ...             0   \n",
       "\n",
       "      family violence  admiralty and shipping  advice  advice  \\\n",
       "0                   0                       0       0     0.0   \n",
       "1                   0                       0       0     0.0   \n",
       "2                   0                       0       0     0.0   \n",
       "3                   0                       0       0     0.0   \n",
       "4                   0                       0       0     0.0   \n",
       "...               ...                     ...     ...     ...   \n",
       "8269                0                       0       0     0.0   \n",
       "8270                0                       0       0     0.0   \n",
       "8271                0                       0       0     0.0   \n",
       "8272                0                       0       0     0.0   \n",
       "8273                0                       0       0     0.0   \n",
       "\n",
       "      adverse possession  international taxation  hdb flat  \\\n",
       "0                      0                       0         0   \n",
       "1                      0                       0         0   \n",
       "2                      0                       0         0   \n",
       "3                      0                       0         0   \n",
       "4                      0                       0         0   \n",
       "...                  ...                     ...       ...   \n",
       "8269                   0                       0         0   \n",
       "8270                   0                       0         0   \n",
       "8271                   0                       0         0   \n",
       "8272                   0                       0         0   \n",
       "8273                   0                       0         0   \n",
       "\n",
       "      courts and jurisdiction  misrepresentation act  \n",
       "0                           0                      0  \n",
       "1                           0                      0  \n",
       "2                           0                      0  \n",
       "3                           0                      0  \n",
       "4                           0                      0  \n",
       "...                       ...                    ...  \n",
       "8269                        0                      0  \n",
       "8270                        0                      0  \n",
       "8271                        0                      0  \n",
       "8272                        0                      0  \n",
       "8273                        0                      0  \n",
       "\n",
       "[8274 rows x 1257 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = feature_importance_df['Feature'].tolist()\n",
    "\n",
    "X_train_filtered = X_train_resampled[important_features]\n",
    "X_test_filtered = X_test[important_features]\n",
    "X_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector = RFE(best_rf, n_features_to_select=1000, step=1)\n",
    "# selector = selector.fit(X_train_filtered, y_train_resampled)\n",
    "# dump(selector, 'model/rfe_selector.joblib')\n",
    "selector = load('model/rfe_selector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = selector.transform(X_train_filtered)\n",
    "X_test_reduced = selector.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-15 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-15 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-15 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-15 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-15 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-15 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-15 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-15 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-15 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.fit(X_train_reduced, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5912659470068695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Favourable       0.60      0.95      0.73      1183\n",
      "  No outcome       0.40      0.03      0.06       238\n",
      "Unfavourable       0.54      0.11      0.19       617\n",
      "\n",
      "    accuracy                           0.59      2038\n",
      "   macro avg       0.51      0.37      0.33      2038\n",
      "weighted avg       0.56      0.59      0.49      2038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_rf.predict(X_test_reduced)\n",
    "\n",
    "# Evaluating the Model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Multiclass Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from numpy import mean\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "X = processed_df.drop(['target', 'casename'], axis=1)\n",
    "y = processed_df['target'] \n",
    "\n",
    "#For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n",
    "#For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n",
    "#‘liblinear’ is limited to one-versus-rest schemes.\n",
    "all_models = [ \n",
    "    ['multinomial','lbfgs','l2'],\n",
    "    ['multinomial','saga','l1'],\n",
    "    # ['ovr','liblinear','l2'],\n",
    "    # ['ovr','liblinear','l1']\n",
    "] \n",
    "\n",
    "results, max_score, best = [], 0, []\n",
    "\n",
    "def log_reg_finetune(solvers, results, best, max_score):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        #for each model, find the best C parameter value\n",
    "        for model in solvers:\n",
    "            #tested only with 3 values for C parameter. For future improvements, can fine tune more if time allows.\n",
    "            for p in [0.0001, 0.01, 1.0]:\n",
    "                    scores = []\n",
    "                    print(f'\\nProcessing for C = {p} and model = {model[0] + \" \" + model[1] + \" \" + model[2]}')\n",
    "                    lm = LogisticRegression(multi_class=model[0], solver=model[1], penalty=model[2], C=p, max_iter=1000, n_jobs = -1, random_state=42)\n",
    "\n",
    "                    #Stratified ensure each fold of dataset has the same proportion of observations with a given label.\n",
    "                    #StratifiedKFold might have noisy estimate so used RepeatedStratifiedKFold to help lessen this problem\n",
    "                    rskf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "                    for train_index, test_index in rskf.split(X, y):\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                        # Handle imbalanced classes\n",
    "                        ros = RandomOverSampler(random_state=42)\n",
    "                        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "                        # Vectorizing textual features using TF-IDF for X_train\n",
    "                        tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "                        X_train_text = tfidf_vectorizer.fit_transform(X_train['processed_facts'].astype('U') + ' ' + X_train['processed_issues'].astype('U'))\n",
    "                        X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "                        # Drop original text columns and concatenate TF-IDF features\n",
    "                        X_train = X_train.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "                        X_train = pd.concat([X_train.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "                        # Vectorizing textual features using TF-IDF for X_test\n",
    "                        X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "                        X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "                        # Drop original text columns and concatenate TF-IDF features\n",
    "                        X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "                        X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)\n",
    "                        \n",
    "                        lm.fit(X_train, y_train)\n",
    "                        y_pred = lm.predict(X_test)\n",
    "                        scores.append(accuracy_score(y_test, y_pred))\n",
    "                        # scores = cross_val_score(lm, X_train, y_train, scoring='accuracy', cv=rskf)\n",
    "                        \n",
    "                    #get the average score of the folds and pick the best scores\n",
    "                    result = [model[0] + \" \" + model[1] + \" \" + model[2], p, mean(scores)]\n",
    "                    results.append(result)\n",
    "                    print('model = %s C = %s acc = %.3f' % (result[0], result[1], result[2]))\n",
    "                    if mean(scores) > max_score:\n",
    "                        best = [model, p, mean(scores)]\n",
    "                        max_score = mean(scores)\n",
    "                    print(f'Done processing for C = {p} and model = {model[0] + \" \" + model[1] + \" \" + model[2]}')\n",
    "        return results, best, max_score\n",
    "\n",
    "result, best, max_score = log_reg_finetune(all_models, results, best, max_score)\n",
    "all_models = [ \n",
    "    # ['multinomial','lbfgs','l2'],\n",
    "    # ['multinomial','saga','l1'],\n",
    "    ['ovr','liblinear','l2'],\n",
    "    ['ovr','liblinear','l1']\n",
    "] \n",
    "result, best, max_score = log_reg_finetune(all_models, results, best, max_score)\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "    print('model = %s C = %s acc = %.3f' % (result[0], result[1], result[2]))\n",
    "\t\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
