{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips from prof\n",
    "\n",
    "- Narrow scope of work (e.g. court level)\n",
    "\n",
    "- Could try both binary/multi-class model outcomes and compare the performance \n",
    "\n",
    "- Change user from layperson to legal professional (and mention that this project is a stepping stone towards having layperson use the model)\n",
    "\n",
    "- Link features to predicted outcome (if time permits can try using XGBoost with LIME for model interpretability)\n",
    "\n",
    "- Can also try to see accuracy of models with different areas of law, lowest accuracy may be hardest area of law to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import json\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "# Ignore the DeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename                                        area_of_law  \\\n",
      "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
      "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
      "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
      "3  2000_SGCA_12.pdf  {'courts and jurisdiction': ['court of appeal'...   \n",
      "4  2000_SGCA_13.pdf                     {'criminal law': ['offences']}   \n",
      "\n",
      "  court_level                                             issues  \\\n",
      "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
      "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
      "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
      "3        SGCA  the appeals from the assistant registrar. In h...   \n",
      "4        SGCA  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...            12   \n",
      "1  facts and surrounding circumstances including ...             8   \n",
      "2  Background \\nThe first appellants, a French co...             0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...            27   \n",
      "4  facts. Mere assertion would not suffice. In ex...            28   \n",
      "\n",
      "   facts_topic        target  \n",
      "0            7    Favourable  \n",
      "1            3    Favourable  \n",
      "2           12    No outcome  \n",
      "3           10  Unfavourable  \n",
      "4           13  Unfavourable  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV files into DataFrames\n",
    "areas_of_law_df = pd.read_csv(\"data/prediction_data/areas_of_law.csv\")\n",
    "coram_df = pd.read_csv(\"data/prediction_data/coram.csv\")\n",
    "sg_legal_cases_df = pd.read_csv(\"data/prediction_data/sg_legal_cases_dataset.csv\")\n",
    "target_rulings_df = pd.read_csv(\"data/prediction_data/target_rulings.csv\")\n",
    "issues_facts_df = pd.read_csv(\"data/prediction_data/issues_facts_topic.csv\")\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/issues.json') as f:\n",
    "    issues_data = [json.loads(line) for line in f]\n",
    "issues_df = pd.DataFrame(issues_data)\n",
    "\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/updated_facts.json') as f:\n",
    "    facts_data = [json.loads(line) for line in f]\n",
    "raw_facts_df = pd.DataFrame(facts_data)\n",
    "raw_facts_df[\"casename\"] = raw_facts_df[\"casename\"].apply(lambda case: case + \".pdf\" if case[-4:] != \".pdf\" else case)\n",
    "raw_facts_df[\"facts\"] = raw_facts_df[\"facts\"].fillna(\"\") + raw_facts_df[\"fact\"].fillna(\"\")\n",
    "raw_facts_df = raw_facts_df.drop(columns=[\"fact\"])\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(areas_of_law_df, sg_legal_cases_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, raw_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, target_rulings_df, on='casename', how='inner')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "casename        0\n",
       "area_of_law     0\n",
       "court_level     0\n",
       "issues          0\n",
       "facts           0\n",
       "issues_topic    0\n",
       "facts_topic     0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate coram names and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coram_names(coram_list):\n",
    "    all_names = set()\n",
    "    for item in coram_list:\n",
    "        split_names = re.split(r';\\s(?![a-zA-Z]+\\s)', item)\n",
    "        for name in split_names:\n",
    "            if ';' in name and not re.search(r';\\s[a-zA-Z]+$', name):\n",
    "                sub_names = name.split(';')\n",
    "                all_names.update([n.strip() for n in sub_names if n.strip()])\n",
    "            else:\n",
    "                all_names.add(name.strip())\n",
    "    return list(all_names)\n",
    "\n",
    "def remove_coram_roles(coram_list):\n",
    "    roles = [' CJ', ' AG', ' J', ' DCJ', ' JA', ' AR', ' JC', 'SAR']\n",
    "    for role in roles:\n",
    "        coram_list = [re.sub(rf'{role}$', '', name) for name in coram_list]\n",
    "    return coram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "coram_df = coram_df.dropna()\n",
    "for i, coram_str in enumerate(coram_df['Coram']):\n",
    "    coram = ast.literal_eval(coram_str)\n",
    "    \n",
    "    coram_modified = clean_coram_names(coram)\n",
    "    coram_modified = remove_coram_roles(coram_modified)\n",
    "    coram_df.at[i, 'Coram'] = str(coram_modified)\n",
    "merged_df = pd.merge(merged_df, coram_df, on='casename', how='outer')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casename         7\n",
      "area_of_law     54\n",
      "court_level     54\n",
      "issues          54\n",
      "facts           54\n",
      "issues_topic    54\n",
      "facts_topic     54\n",
      "target          54\n",
      "Coram            7\n",
      "dtype: int64\n",
      "               casename area_of_law court_level issues facts  issues_topic  \\\n",
      "241   2000_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "274   2000_SGHC_290.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "412    2001_SGCA_66.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "432   2001_SGHC_101.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "438   2001_SGHC_108.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "442   2001_SGHC_111.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "448   2001_SGHC_118.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "457   2001_SGHC_128.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "460   2001_SGHC_130.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "462   2001_SGHC_132.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "475   2001_SGHC_148.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "478   2001_SGHC_150.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "479   2001_SGHC_151.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "489   2001_SGHC_163.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "498   2001_SGHC_174.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "536   2001_SGHC_214.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "537   2001_SGHC_215.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "544   2001_SGHC_222.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "546   2001_SGHC_224.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "550   2001_SGHC_228.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "551   2001_SGHC_229.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "555   2001_SGHC_232.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "564   2001_SGHC_240.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "568   2001_SGHC_244.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "574   2001_SGHC_250.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "578   2001_SGHC_254.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "581   2001_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "591   2001_SGHC_266.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "592   2001_SGHC_267.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "596   2001_SGHC_270.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "603   2001_SGHC_277.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "615   2001_SGHC_289.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "619   2001_SGHC_292.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "622   2001_SGHC_295.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "630   2001_SGHC_301.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "633   2001_SGHC_304.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "647   2001_SGHC_319.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "651   2001_SGHC_322.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "658   2001_SGHC_329.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "667   2001_SGHC_337.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "675   2001_SGHC_344.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "678   2001_SGHC_347.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "700   2001_SGHC_367.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "701   2001_SGHC_368.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "705   2001_SGHC_371.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "707   2001_SGHC_373.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "713   2001_SGHC_379.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "8567                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8568                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8569                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8570                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8571                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8572                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8573                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "\n",
      "      facts_topic target                                              Coram  \n",
      "241           NaN    NaN                                ['Sundaresh Menon']  \n",
      "274           NaN    NaN  ['Yong Pung How', 'Chao Hick Tin', 'Tan Lee Me...  \n",
      "412           NaN    NaN                                 ['Ang Cheng Hock']  \n",
      "432           NaN    NaN  ['Yong Pung How', 'Chao Hick Tin', 'Choo Han T...  \n",
      "438           NaN    NaN                                ['Pang Khang Chau']  \n",
      "442           NaN    NaN                                    ['Edmund Leow']  \n",
      "448           NaN    NaN                                 ['Amarjeet Singh']  \n",
      "457           NaN    NaN                                    ['Quentin Loh']  \n",
      "460           NaN    NaN                     ['Chao Hick Tin', 'V K Rajah']  \n",
      "462           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "475           NaN    NaN                            ['Vinodh Coomaraswamy']  \n",
      "478           NaN    NaN                                  ['Kannan Ramesh']  \n",
      "479           NaN    NaN  ['Steven Chong', 'Sundaresh Menon', 'Judith Pr...  \n",
      "489           NaN    NaN                                     ['Woo Bih Li']  \n",
      "498           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "536           NaN    NaN                                    ['S Rajendran']  \n",
      "537           NaN    NaN                                      ['MPH Rubin']  \n",
      "544           NaN    NaN  ['Tay Yong Kwang', 'Sundaresh Menon', 'Judith ...  \n",
      "546           NaN    NaN                                 ['Tan Siong Thye']  \n",
      "550           NaN    NaN                                 ['Hoo Sheau Peng']  \n",
      "551           NaN    NaN                                   ['Lai Kew Chai']  \n",
      "555           NaN    NaN                                  ['Choo Han Teck']  \n",
      "564           NaN    NaN                                     ['George Wei']  \n",
      "568           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "574           NaN    NaN                                 ['Tay Yong Kwang']  \n",
      "578           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "581           NaN    NaN  ['Yong Pung How', 'Chao Hick Tin', 'Tan Lee Me...  \n",
      "591           NaN    NaN                                 ['Judith Prakash']  \n",
      "592           NaN    NaN                                   ['Lee Seiu Kin']  \n",
      "596           NaN    NaN                                   ['Lee Kim Shin']  \n",
      "603           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "615           NaN    NaN                                      ['MPH Rubin']  \n",
      "619           NaN    NaN                                 ['Kwek Mean Luck']  \n",
      "622           NaN    NaN                                     ['Woo Bih Li']  \n",
      "630           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "633           NaN    NaN                                  ['Choo Han Teck']  \n",
      "647           NaN    NaN                                  ['Valerie Thean']  \n",
      "651           NaN    NaN                                 ['Judith Prakash']  \n",
      "658           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "667           NaN    NaN                                     ['Woo Bih Li']  \n",
      "675           NaN    NaN                                  ['Yong Pung How']  \n",
      "678           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "700           NaN    NaN                                     ['Woo Bih Li']  \n",
      "701           NaN    NaN                                  ['Valerie Thean']  \n",
      "705           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "707           NaN    NaN                                  ['Yong Pung How']  \n",
      "713           NaN    NaN                               ['Dedar Singh Gill']  \n",
      "8567          NaN    NaN  ['Steven Chong', 'Andrew Phang Boon Leong', 'S...  \n",
      "8568          NaN    NaN                                    ['Lim Jian Yi']  \n",
      "8569          NaN    NaN  ['Tay Yong Kwang', 'Andrew Phang Boon Leong', ...  \n",
      "8570          NaN    NaN                     ['Chao Hick Tin', 'L P Thean']  \n",
      "8571          NaN    NaN                                 ['Judith Prakash']  \n",
      "8572          NaN    NaN     ['Kan Ting Chiu', 'V K Rajah', 'Steven Chong']  \n",
      "8573          NaN    NaN                                  ['Valerie Thean']  \n",
      "casename        0\n",
      "area_of_law     0\n",
      "court_level     0\n",
      "issues          0\n",
      "facts           0\n",
      "issues_topic    0\n",
      "facts_topic     0\n",
      "target          0\n",
      "Coram           0\n",
      "dtype: int64\n",
      "target\n",
      "Favourable      3941\n",
      "Unfavourable    2056\n",
      "No outcome       794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = merged_df.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "#nas are probably those reassigned cases, coram has 7, i just drop them for now\n",
    "na_target_rows = merged_df[merged_df['target'].isna()]\n",
    "print(na_target_rows)\n",
    "\n",
    "merged_df.dropna(axis=0, inplace=True)\n",
    "print(merged_df.isna().sum())\n",
    "\n",
    "#remove empty lists\n",
    "merged_df = merged_df.query(\"area_of_law != '[]'\")\n",
    "\n",
    "#target is unbalanced\n",
    "target_counts = merged_df['target'].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True) # prevent nan values from appearing after one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casename</th>\n",
       "      <th>area_of_law</th>\n",
       "      <th>court_level</th>\n",
       "      <th>issues</th>\n",
       "      <th>facts</th>\n",
       "      <th>issues_topic</th>\n",
       "      <th>facts_topic</th>\n",
       "      <th>target</th>\n",
       "      <th>Coram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_SGCA_1.pdf</td>\n",
       "      <td>{'civil procedure': ['pleadings'], 'res judica...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The claim was dismissed with costs by the\\nHig...</td>\n",
       "      <td>The facts\\nThe appellant is the widow of one T...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[Andrew Phang Boon Leong, V K Rajah, Chan Sek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_SGCA_10.pdf</td>\n",
       "      <td>{'contract': ['formation'], 'equity': ['defenc...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the claim and\\nagainst that decision this appe...</td>\n",
       "      <td>facts and surrounding circumstances including ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[Andrew Phang Boon Leong, Chao Hick Tin, V K R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000_SGCA_11.pdf</td>\n",
       "      <td>{'contract': ['discharge'], 'damages': ['asses...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The appeal \\nThe questions which arise in this...</td>\n",
       "      <td>Background \\nThe first appellants, a French co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>[Andrew Phang Boon Leong, Chan Sek Keong, Tan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           casename                                        area_of_law  \\\n",
       "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
       "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
       "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
       "\n",
       "  court_level                                             issues  \\\n",
       "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
       "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
       "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
       "\n",
       "                                               facts  issues_topic  \\\n",
       "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
       "1  facts and surrounding circumstances including ...           8.0   \n",
       "2  Background \\nThe first appellants, a French co...           0.0   \n",
       "\n",
       "   facts_topic      target                                              Coram  \n",
       "0          7.0  Favourable  [Andrew Phang Boon Leong, V K Rajah, Chan Sek ...  \n",
       "1          3.0  Favourable  [Andrew Phang Boon Leong, Chao Hick Tin, V K R...  \n",
       "2         12.0  No outcome  [Andrew Phang Boon Leong, Chan Sek Keong, Tan ...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(ast.literal_eval)\n",
    "merged_df['Coram'] = merged_df['Coram'].apply(ast.literal_eval)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten areas_of_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_areas = []\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "\n",
    "    areas = row['area_of_law']\n",
    "    flat_areas = []\n",
    "    for main_area, sub_areas in areas.items():\n",
    "        flat_areas.append(main_area)\n",
    "        for sarea in sub_areas.copy():\n",
    "            if len(sarea) > 33:\n",
    "                sub_areas.remove(sarea)\n",
    "        flat_areas.extend(sub_areas)\n",
    "    all_areas.append(flat_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in all_areas:\n",
    "    if area == []:\n",
    "        print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "\n",
      "   facts_topic      target                                              Coram  \\\n",
      "0          7.0  Favourable  [Andrew Phang Boon Leong, V K Rajah, Chan Sek ...   \n",
      "1          3.0  Favourable  [Andrew Phang Boon Leong, Chao Hick Tin, V K R...   \n",
      "2         12.0  No outcome  [Andrew Phang Boon Leong, Chan Sek Keong, Tan ...   \n",
      "\n",
      "   \"a larger sum being repaid\"  \"abet\"  ...  work injury compensation act  \\\n",
      "0                            0       0  ...                             0   \n",
      "1                            0       0  ...                             0   \n",
      "2                            0       0  ...                             0   \n",
      "\n",
      "   workmen’s compensation act  writ of  seizure and sale  writ of summons  \\\n",
      "0                           0                          0                0   \n",
      "1                           0                          0                0   \n",
      "2                           0                          0                0   \n",
      "\n",
      "   wrongful dismissal  young offenders]  “any claim  hereunder”  \\\n",
      "0                   0                 0                       0   \n",
      "1                   0                 0                       0   \n",
      "2                   0                 0                       0   \n",
      "\n",
      "   “any fire accidentally begin”  “charity proceedings”  \\\n",
      "0                              0                      0   \n",
      "1                              0                      0   \n",
      "2                              0                      0   \n",
      "\n",
      "   “rash” and “negligent”  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "\n",
      "[3 rows x 1377 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode aol\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(all_areas)\n",
    "\n",
    "binary_aol_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_aol_df = binary_aol_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([merged_df.drop('area_of_law', axis=1), binary_aol_df], axis=1)\n",
    "processed_df = processed_df[processed_df['Coram'].apply(lambda x: isinstance(x, list))]\n",
    "print(processed_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "3  2000_SGCA_12.pdf        SGCA   \n",
      "4  2000_SGCA_13.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "3  the appeals from the assistant registrar. In h...   \n",
      "4  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
      "4  facts. Mere assertion would not suffice. In ex...          28.0   \n",
      "\n",
      "   facts_topic        target  \"a larger sum being repaid\"  \"abet\"  \\\n",
      "0          7.0    Favourable                            0       0   \n",
      "1          3.0    Favourable                            0       0   \n",
      "2         12.0    No outcome                            0       0   \n",
      "3         10.0  Unfavourable                            0       0   \n",
      "4         13.0  Unfavourable                            0       0   \n",
      "\n",
      "   \"an interest in any matter\"  ...  Vincent Hoong  Vincent Leow  \\\n",
      "0                            0  ...              0             0   \n",
      "1                            0  ...              0             0   \n",
      "2                            0  ...              0             0   \n",
      "3                            0  ...              0             0   \n",
      "4                            0  ...              0             0   \n",
      "\n",
      "   Vinodh Coomaraswamy  Wong Li Kok, Alex  Woo Bih Li  Yap Yew Choh Kenneth  \\\n",
      "0                    0                  0           0                     0   \n",
      "1                    0                  0           0                     0   \n",
      "2                    0                  0           0                     0   \n",
      "3                    0                  0           0                     0   \n",
      "4                    0                  0           0                     0   \n",
      "\n",
      "   Yeong Zee Kin  Yong Pung How  Yong Pung How,  Zhuo Wenzhao  \n",
      "0              0              0               0             0  \n",
      "1              0              0               0             0  \n",
      "2              0              0               0             0  \n",
      "3              0              0               0             0  \n",
      "4              0              0               0             0  \n",
      "\n",
      "[5 rows x 1487 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode coram\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(processed_df['Coram'])\n",
    "\n",
    "binary_coram_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_coram_df = binary_coram_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([processed_df.drop('Coram', axis=1), binary_coram_df], axis=1)\n",
    "\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['SGCA'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGCA' else 0)\n",
    "processed_df['SGHC'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGHC' else 0)\n",
    "processed_df = processed_df.drop('court_level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'\\W*\\b(?!no)\\w{1,2}\\b', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    legal_stopwords = ('appellant', 'respondent', 'plaintiff', 'defendant', 'mr', 'mrs', 'dr', 'mdm', 'court','version', 'hr', 'would', 'case', 'sghc', 'court', 'sgca', 'slr', 'sgdc', 'also', 'first', 'person', 'statement', 'line', 'para', 'fact', 'one', 'may', 'time', 'could', 'next', 'legal', 'issues', 'issue')\n",
    "    stop_words.update(legal_stopwords)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "processed_df['processed_facts'] = processed_df['facts'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['facts'], inplace=True)\n",
    "\n",
    "processed_df['processed_issues'] = processed_df['issues'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['issues'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(columns=['target','casename'])\n",
    "y = processed_df['target']\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test_val = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test_val = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "#balanced dataset (target variable was imbalanced Favourable 5006 Unfavourable 2523 No outcome 984)\n",
    "#randomly found one online, can be changed -> need to check am i doing this right \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smt = SMOTE(random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "# X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "#split further from X_test_val into X_val and X_test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train_resampled\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_val\n",
    "X_val_text = tfidf_vectorizer.transform(X_val['processed_facts'].astype('U') + ' ' + X_val['processed_issues'].astype('U'))\n",
    "X_val_text = pd.DataFrame(X_val_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_val = X_val.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_val = pd.concat([X_val.reset_index(drop=True), X_val_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target variable to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_resampled = X_train_resampled.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_test = X_test.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_val = X_val.drop(columns=['processed_facts', 'processed_issues'])\n",
    "\n",
    "mapping = {'Favourable': 1, 'Unfavourable': 0, 'No outcome':0.5}\n",
    "\n",
    "y_train_resampled, y_test, y_val = y_train_resampled.copy().map(mapping), y_test.copy().map(mapping), y_val.copy().map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform modelling\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  epochs = 20\n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: torch.Size([8274, 2484])\n",
      "Shape of X_test: torch.Size([1019, 2484])\n",
      "Shape of X_val: torch.Size([1019, 2484])\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled = X_train_resampled.iloc[:, :].copy()\n",
    "X_train_resampled = torch.tensor(X_train_resampled.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_train_resampled: {X_train_resampled.shape}')\n",
    "\n",
    "X_test = X_test.iloc[:, :].copy()\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "\n",
    "X_val = X_val.iloc[:,:].copy()\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8274, 1, 2484])\n"
     ]
    }
   ],
   "source": [
    "y_train_resampled, y_test, y_val = torch.tensor(y_train_resampled.values).to(device), torch.tensor(y_test.values).to(device), torch.tensor(y_val.values).to(device)\n",
    "\n",
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0],1,X_train_resampled.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])\n",
    "print(X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, 1,1, bias=True)\n",
    "        # Define the first 1D convolution layer. Takes 1 input channel, outputs 32 channels, kernel size is 3, stride is 1, padding is 1.\n",
    "        self.Bn1 = nn.BatchNorm1d(64)\n",
    "        # Apply Batch Normalization to the output of the first convolutional layer.\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # Apply 1D Average Pooling after the first Batch Normalization. The kernel size and stride are 2.\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3, 1,1, bias=True)\n",
    "        self.Bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(39744, 100, bias=True)\n",
    "        # Define the first fully connected layer. It takes 25472 inputs and outputs 100 nodes.\n",
    "\n",
    "        self.fc2 = nn.Linear(100, 30, bias=True)\n",
    "        # Define the second fully connected layer. It takes 100 inputs and outputs 50 nodes.\n",
    "\n",
    "        self.fc3 = nn.Linear(30, 3, bias=True)\n",
    "        # Define the third fully connected layer (output layer). It takes 50 inputs and outputs 3 nodes.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.Bn1(self.conv1(x)))\n",
    "        # Pass the input through the first convolutional layer, then Batch Normalization, and then apply ReLU activation.\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        # Apply Average Pooling to the output of the previous step.\n",
    "        x = F.tanh(self.Bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Flatten the output from the previous step. This is necessary because fully connected layers expect a 1D input.\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output through the first fully connected layer.\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        # Pass the output through the second fully connected layer with tanh activation.\n",
    "        x = self.fc3(x)\n",
    "        # Pass the output through the third fully connected layer. This is the output of the network.\n",
    "        return x\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n",
    "        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous training step\n",
    "        output = model(data)  # Run forward pass (model predictions)\n",
    "        #print(output.shape)\n",
    "        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n",
    "        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n",
    "        for data, target in test_loader:  # Loop over each batch from the testing set\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "            target = target.long()  # Convert target to long after adjusting value\n",
    "            output = model(data)  # Run forward pass (model predictions)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # Sum up the batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    return correct  # Return the number of correctly classified samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([64, 1, 3])\n",
      "conv1.bias \t torch.Size([64])\n",
      "Bn1.weight \t torch.Size([64])\n",
      "Bn1.bias \t torch.Size([64])\n",
      "Bn1.running_mean \t torch.Size([64])\n",
      "Bn1.running_var \t torch.Size([64])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 64, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "Bn2.weight \t torch.Size([64])\n",
      "Bn2.bias \t torch.Size([64])\n",
      "Bn2.running_mean \t torch.Size([64])\n",
      "Bn2.running_var \t torch.Size([64])\n",
      "Bn2.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([100, 39744])\n",
      "fc1.bias \t torch.Size([100])\n",
      "fc2.weight \t torch.Size([30, 100])\n",
      "fc2.bias \t torch.Size([30])\n",
      "fc3.weight \t torch.Size([3, 30])\n",
      "fc3.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_resampled, y_train_resampled)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.083987\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.643475\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.590362\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.667143\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.663067\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.589450\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.582944\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.642578\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.608457\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.693919\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.601276\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.621974\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.619719\n",
      "\n",
      "Test set: Average loss: 0.7847, Accuracy: 429/1019 (42%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.606153\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.597948\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.613925\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.573005\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.592010\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.600560\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.608680\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.515746\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.642304\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.666376\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.638715\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.653095\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.698892\n",
      "\n",
      "Test set: Average loss: 0.8390, Accuracy: 463/1019 (45%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.596579\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.602479\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.707325\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.673635\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.573903\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.610198\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.598841\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.543935\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.664753\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.713382\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.653668\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.615888\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.564220\n",
      "\n",
      "Test set: Average loss: 0.7968, Accuracy: 499/1019 (49%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.505755\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.542043\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.574055\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.639755\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.637398\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.667490\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.564643\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.556378\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.686359\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.566588\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.577020\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.512974\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.596380\n",
      "\n",
      "Test set: Average loss: 0.7899, Accuracy: 501/1019 (49%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.552044\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.507901\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.600568\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.635056\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.521043\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.543507\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.590698\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.522296\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.481553\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.557235\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.557695\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.497073\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.438976\n",
      "\n",
      "Test set: Average loss: 0.7650, Accuracy: 548/1019 (54%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.465268\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.425838\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.475219\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.491540\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.493948\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.518557\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.543403\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.616622\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.455374\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.448253\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.441636\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.495071\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.565555\n",
      "\n",
      "Test set: Average loss: 0.7617, Accuracy: 549/1019 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.401399\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.527906\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.518949\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.526117\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.525791\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.532947\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.559501\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.368355\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.551861\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.510116\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.502716\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.490126\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.496872\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 553/1019 (54%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.440099\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.453910\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.386624\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.492244\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.514443\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.522369\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.428121\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.445445\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.334341\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.357508\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.497261\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.492831\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.383672\n",
      "\n",
      "Test set: Average loss: 0.7597, Accuracy: 584/1019 (57%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.424525\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.475455\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.342830\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.418147\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.346184\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.453012\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.458544\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.449902\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.389601\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.563425\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.535756\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.471573\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.393794\n",
      "\n",
      "Test set: Average loss: 0.7695, Accuracy: 561/1019 (55%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.451836\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.427260\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.417723\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.444247\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.434126\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.389626\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.442672\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.393254\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.423104\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.335781\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.408620\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.448947\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.477396\n",
      "\n",
      "Test set: Average loss: 0.7629, Accuracy: 576/1019 (57%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.359182\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.468880\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.419423\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.401103\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.383584\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.405307\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.452902\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.504680\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.386737\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.404145\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.374082\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.390142\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.455749\n",
      "\n",
      "Test set: Average loss: 0.7688, Accuracy: 570/1019 (56%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.479001\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.331230\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.372042\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.483213\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.562507\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.357554\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.345779\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.435146\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.473515\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.347141\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.424355\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.319043\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.431599\n",
      "\n",
      "Test set: Average loss: 0.7787, Accuracy: 569/1019 (56%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.403241\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.352626\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.472169\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.412448\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.437304\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.436152\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.431681\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.437513\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.392880\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.368463\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.428602\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.374350\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.487675\n",
      "\n",
      "Test set: Average loss: 0.7716, Accuracy: 570/1019 (56%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.422066\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.487725\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.439031\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.346063\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.462322\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.408749\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.499780\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.450211\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.532892\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.285838\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.385790\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.399239\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.442836\n",
      "\n",
      "Test set: Average loss: 0.7737, Accuracy: 572/1019 (56%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.359228\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.437837\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.402570\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.396217\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.419410\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.482156\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.379140\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.420897\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.478255\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.392197\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.488092\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.381737\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.358315\n",
      "\n",
      "Test set: Average loss: 0.7749, Accuracy: 575/1019 (56%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.378089\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.366047\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.427958\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.406124\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.431352\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.299428\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.368301\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.386439\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.334199\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.448860\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.383289\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.433315\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.413540\n",
      "\n",
      "Test set: Average loss: 0.7731, Accuracy: 580/1019 (57%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.383472\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.392284\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.296725\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.352856\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.381785\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.396192\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.435825\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.372466\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.482055\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.323439\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.397065\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.341871\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.365188\n",
      "\n",
      "Test set: Average loss: 0.7743, Accuracy: 577/1019 (57%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.355350\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.402891\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.377741\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.378713\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.437672\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.346861\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.439950\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.394999\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.431649\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.492084\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.296376\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.502665\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.386464\n",
      "\n",
      "Test set: Average loss: 0.7750, Accuracy: 578/1019 (57%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.369699\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.516222\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.455168\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.343976\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.428776\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.398855\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.372699\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.343764\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.373981\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.335348\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.363811\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.351833\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.427765\n",
      "\n",
      "Test set: Average loss: 0.7754, Accuracy: 577/1019 (57%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.345629\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.437578\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.383553\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.392979\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.317789\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.343925\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.509401\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.426115\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.327395\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.379291\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.383281\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.431075\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.521181\n",
      "\n",
      "Test set: Average loss: 0.7759, Accuracy: 578/1019 (57%)\n",
      "\n",
      "584\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[465], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m output_test \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output_test, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     28\u001b[0m correct_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m total_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Double"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    ACC_ = test(model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(model.state_dict(), \"Baseline_CNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 57.016683022571144\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = model(data)\n",
    "        #pred = torch.argmax(output_test, 1)\n",
    "        pred = output_test.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "\n",
    "        \n",
    "        val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        #correct_val += (pred == target).sum().item()\n",
    "        correct_val += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "        \n",
    "        total_val += target.size(0)\n",
    "    \n",
    "    test_accuracy = (correct_val / total_val) * 100\n",
    "            \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(2484, 128, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(128)\n",
    "        # self.fc2 = nn.Linear(128, 128, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(128)\n",
    "        # self.fc3 = nn.Linear(128, 5, bias=True)\n",
    "\n",
    "        # self.fc1 = nn.Linear(2484, 1024, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(1024)\n",
    "        # self.fc2 = nn.Linear(1024, 512, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(512)\n",
    "        # self.fc3 = nn.Linear(512, 256, bias=True)\n",
    "        # self.Bn3 = nn.BatchNorm1d(256)\n",
    "        # self.fc4 = nn.Linear(256, 128, bias=True)\n",
    "        # self.Bn4 = nn.BatchNorm1d(128)\n",
    "        # self.fc5 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(2484, 512, bias=True)\n",
    "        self.Bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(512, 128, bias=True)\n",
    "        self.fc3 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.Bn1(self.fc1(x)))\n",
    "        # x = F.tanh(self.Bn2(self.fc2(x)))\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.fc1(x))\n",
    "        # x = F.leaky_relu(self.fc2(x))\n",
    "        # x = F.leaky_relu(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc4(x))\n",
    "        # x = self.fc5(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.tanh(self.fc1(x)) # [leaky_relu, tanh, relu,]\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight \t torch.Size([512, 2484])\n",
      "fc1.bias \t torch.Size([512])\n",
      "Bn1.weight \t torch.Size([256])\n",
      "Bn1.bias \t torch.Size([256])\n",
      "Bn1.running_mean \t torch.Size([256])\n",
      "Bn1.running_var \t torch.Size([256])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "fc2.weight \t torch.Size([128, 512])\n",
      "fc2.bias \t torch.Size([128])\n",
      "fc3.weight \t torch.Size([3, 128])\n",
      "fc3.bias \t torch.Size([3])\n",
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.144096\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.591128\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.659717\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.637089\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.598890\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.621578\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.602656\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.615508\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.686932\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.652801\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.601091\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.513449\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.574737\n",
      "\n",
      "Test set: Average loss: 0.9043, Accuracy: 466/1019 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.625196\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.540608\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.514788\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.464694\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.527348\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.571075\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.608517\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.416023\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.598106\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.533073\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.480460\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.562345\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.515361\n",
      "\n",
      "Test set: Average loss: 0.7409, Accuracy: 537/1019 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.510362\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.429560\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.413808\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.464106\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.467952\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.524088\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.457678\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.678763\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.428776\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.568854\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.586704\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.454760\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.559691\n",
      "\n",
      "Test set: Average loss: 0.8130, Accuracy: 525/1019 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.446254\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.341359\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.535521\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.413134\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.563724\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.351350\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.332520\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.490715\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.442139\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.368996\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.354835\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.497785\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.478773\n",
      "\n",
      "Test set: Average loss: 0.9427, Accuracy: 536/1019 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.300998\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.391380\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.389028\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.573730\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.325227\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.414783\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.413460\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.498807\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.369913\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.272909\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.421780\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.553532\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.407320\n",
      "\n",
      "Test set: Average loss: 0.9253, Accuracy: 545/1019 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.337476\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.386359\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.434817\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.340883\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.286864\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.352078\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.324878\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.413646\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.322184\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.344062\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.403441\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.348992\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.360235\n",
      "\n",
      "Test set: Average loss: 1.0666, Accuracy: 554/1019 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.269191\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.318814\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.246343\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.193961\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.253149\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.272330\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.302685\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.408747\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.300993\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.355405\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.172651\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.264682\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.349531\n",
      "\n",
      "Test set: Average loss: 1.1016, Accuracy: 550/1019 (54%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.257087\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.270247\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.278059\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.208732\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.230168\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.189265\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.291188\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.327778\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.465877\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.482833\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.252025\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.309847\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.337604\n",
      "\n",
      "Test set: Average loss: 1.2469, Accuracy: 573/1019 (56%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.278554\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.201720\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.290454\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.152320\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.298668\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.252314\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.181425\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.332476\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.430491\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.262352\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.272417\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.284263\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.220035\n",
      "\n",
      "Test set: Average loss: 1.4497, Accuracy: 555/1019 (54%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.166448\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.132067\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.213159\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.222318\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.317187\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.264981\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.264913\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.190150\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.289548\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.224466\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.282692\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.189489\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.329829\n",
      "\n",
      "Test set: Average loss: 1.3761, Accuracy: 563/1019 (55%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.216788\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.220935\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.199135\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.079475\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.118267\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.134767\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.209565\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.175697\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.149763\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.351301\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.328691\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.388703\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.148398\n",
      "\n",
      "Test set: Average loss: 1.6501, Accuracy: 558/1019 (55%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.088030\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.168931\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.170107\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.159161\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.213372\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.151190\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.227589\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.197462\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.317766\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.228947\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.095872\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.155563\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.245609\n",
      "\n",
      "Test set: Average loss: 1.6142, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.164204\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.122563\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.112346\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.152036\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.077023\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.146233\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.135326\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.098915\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.282919\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.147741\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.141697\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.081373\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.128392\n",
      "\n",
      "Test set: Average loss: 1.7065, Accuracy: 574/1019 (56%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.061978\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.093049\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.073246\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.211850\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.120688\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.217451\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.103348\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.075388\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.114320\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.077610\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.091969\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.205663\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.180729\n",
      "\n",
      "Test set: Average loss: 1.8795, Accuracy: 569/1019 (56%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.165696\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.113459\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.103383\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.181091\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.052874\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.043493\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.104581\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.053355\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.102458\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.189194\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.153335\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.157744\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.065375\n",
      "\n",
      "Test set: Average loss: 1.7935, Accuracy: 567/1019 (56%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.165693\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.157382\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.205832\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.034632\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.111201\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.078135\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.090278\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.159436\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.070372\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.072910\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.108162\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.105169\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.103306\n",
      "\n",
      "Test set: Average loss: 1.9715, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.073738\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.088898\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.074882\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.114307\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.149007\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.066079\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.053714\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.197973\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.045736\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.082869\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.111240\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.178065\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.091870\n",
      "\n",
      "Test set: Average loss: 2.0132, Accuracy: 580/1019 (57%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.080005\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.077100\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.022260\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.058934\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.031629\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.021345\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.111898\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.027325\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.134367\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.052373\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.090575\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.053379\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.224937\n",
      "\n",
      "Test set: Average loss: 2.2072, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.034369\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.297416\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.118109\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.164532\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.043562\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.067502\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.071764\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.075907\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.087371\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.068724\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.179662\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.056441\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.046467\n",
      "\n",
      "Test set: Average loss: 2.2023, Accuracy: 589/1019 (58%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.060618\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.042289\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.031317\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.036602\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.043819\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.018168\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.102273\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.060927\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.034336\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.012616\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.063110\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.101758\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.029475\n",
      "\n",
      "Test set: Average loss: 2.2883, Accuracy: 605/1019 (59%)\n",
      "\n",
      "Train Epoch: 21 [0/8274 (0%)]\tLoss: 0.081450\n",
      "Train Epoch: 21 [640/8274 (8%)]\tLoss: 0.038360\n",
      "Train Epoch: 21 [1280/8274 (15%)]\tLoss: 0.101007\n",
      "Train Epoch: 21 [1920/8274 (23%)]\tLoss: 0.020144\n",
      "Train Epoch: 21 [2560/8274 (31%)]\tLoss: 0.059641\n",
      "Train Epoch: 21 [3200/8274 (38%)]\tLoss: 0.053674\n",
      "Train Epoch: 21 [3840/8274 (46%)]\tLoss: 0.055626\n",
      "Train Epoch: 21 [4480/8274 (54%)]\tLoss: 0.042002\n",
      "Train Epoch: 21 [5120/8274 (62%)]\tLoss: 0.039061\n",
      "Train Epoch: 21 [5760/8274 (69%)]\tLoss: 0.043903\n",
      "Train Epoch: 21 [6400/8274 (77%)]\tLoss: 0.054774\n",
      "Train Epoch: 21 [7040/8274 (85%)]\tLoss: 0.023870\n",
      "Train Epoch: 21 [7680/8274 (92%)]\tLoss: 0.065674\n",
      "\n",
      "Test set: Average loss: 2.4667, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 22 [0/8274 (0%)]\tLoss: 0.022672\n",
      "Train Epoch: 22 [640/8274 (8%)]\tLoss: 0.067600\n",
      "Train Epoch: 22 [1280/8274 (15%)]\tLoss: 0.031368\n",
      "Train Epoch: 22 [1920/8274 (23%)]\tLoss: 0.032587\n",
      "Train Epoch: 22 [2560/8274 (31%)]\tLoss: 0.020036\n",
      "Train Epoch: 22 [3200/8274 (38%)]\tLoss: 0.033595\n",
      "Train Epoch: 22 [3840/8274 (46%)]\tLoss: 0.031401\n",
      "Train Epoch: 22 [4480/8274 (54%)]\tLoss: 0.044433\n",
      "Train Epoch: 22 [5120/8274 (62%)]\tLoss: 0.012348\n",
      "Train Epoch: 22 [5760/8274 (69%)]\tLoss: 0.034236\n",
      "Train Epoch: 22 [6400/8274 (77%)]\tLoss: 0.017131\n",
      "Train Epoch: 22 [7040/8274 (85%)]\tLoss: 0.059666\n",
      "Train Epoch: 22 [7680/8274 (92%)]\tLoss: 0.058031\n",
      "\n",
      "Test set: Average loss: 2.3352, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 23 [0/8274 (0%)]\tLoss: 0.034518\n",
      "Train Epoch: 23 [640/8274 (8%)]\tLoss: 0.056811\n",
      "Train Epoch: 23 [1280/8274 (15%)]\tLoss: 0.073154\n",
      "Train Epoch: 23 [1920/8274 (23%)]\tLoss: 0.020761\n",
      "Train Epoch: 23 [2560/8274 (31%)]\tLoss: 0.102095\n",
      "Train Epoch: 23 [3200/8274 (38%)]\tLoss: 0.093329\n",
      "Train Epoch: 23 [3840/8274 (46%)]\tLoss: 0.075587\n",
      "Train Epoch: 23 [4480/8274 (54%)]\tLoss: 0.052083\n",
      "Train Epoch: 23 [5120/8274 (62%)]\tLoss: 0.032461\n",
      "Train Epoch: 23 [5760/8274 (69%)]\tLoss: 0.045350\n",
      "Train Epoch: 23 [6400/8274 (77%)]\tLoss: 0.015498\n",
      "Train Epoch: 23 [7040/8274 (85%)]\tLoss: 0.039392\n",
      "Train Epoch: 23 [7680/8274 (92%)]\tLoss: 0.039548\n",
      "\n",
      "Test set: Average loss: 2.4957, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 24 [0/8274 (0%)]\tLoss: 0.013583\n",
      "Train Epoch: 24 [640/8274 (8%)]\tLoss: 0.015115\n",
      "Train Epoch: 24 [1280/8274 (15%)]\tLoss: 0.022435\n",
      "Train Epoch: 24 [1920/8274 (23%)]\tLoss: 0.023230\n",
      "Train Epoch: 24 [2560/8274 (31%)]\tLoss: 0.009442\n",
      "Train Epoch: 24 [3200/8274 (38%)]\tLoss: 0.020905\n",
      "Train Epoch: 24 [3840/8274 (46%)]\tLoss: 0.019933\n",
      "Train Epoch: 24 [4480/8274 (54%)]\tLoss: 0.126925\n",
      "Train Epoch: 24 [5120/8274 (62%)]\tLoss: 0.012094\n",
      "Train Epoch: 24 [5760/8274 (69%)]\tLoss: 0.177631\n",
      "Train Epoch: 24 [6400/8274 (77%)]\tLoss: 0.008758\n",
      "Train Epoch: 24 [7040/8274 (85%)]\tLoss: 0.068008\n",
      "Train Epoch: 24 [7680/8274 (92%)]\tLoss: 0.011037\n",
      "\n",
      "Test set: Average loss: 2.6644, Accuracy: 590/1019 (58%)\n",
      "\n",
      "Train Epoch: 25 [0/8274 (0%)]\tLoss: 0.007161\n",
      "Train Epoch: 25 [640/8274 (8%)]\tLoss: 0.012277\n",
      "Train Epoch: 25 [1280/8274 (15%)]\tLoss: 0.007818\n",
      "Train Epoch: 25 [1920/8274 (23%)]\tLoss: 0.020836\n",
      "Train Epoch: 25 [2560/8274 (31%)]\tLoss: 0.010443\n",
      "Train Epoch: 25 [3200/8274 (38%)]\tLoss: 0.008561\n",
      "Train Epoch: 25 [3840/8274 (46%)]\tLoss: 0.030838\n",
      "Train Epoch: 25 [4480/8274 (54%)]\tLoss: 0.052377\n",
      "Train Epoch: 25 [5120/8274 (62%)]\tLoss: 0.026540\n",
      "Train Epoch: 25 [5760/8274 (69%)]\tLoss: 0.006049\n",
      "Train Epoch: 25 [6400/8274 (77%)]\tLoss: 0.041571\n",
      "Train Epoch: 25 [7040/8274 (85%)]\tLoss: 0.012301\n",
      "Train Epoch: 25 [7680/8274 (92%)]\tLoss: 0.024669\n",
      "\n",
      "Test set: Average loss: 2.7589, Accuracy: 591/1019 (58%)\n",
      "\n",
      "Train Epoch: 26 [0/8274 (0%)]\tLoss: 0.007807\n",
      "Train Epoch: 26 [640/8274 (8%)]\tLoss: 0.008154\n",
      "Train Epoch: 26 [1280/8274 (15%)]\tLoss: 0.005541\n",
      "Train Epoch: 26 [1920/8274 (23%)]\tLoss: 0.027546\n",
      "Train Epoch: 26 [2560/8274 (31%)]\tLoss: 0.022375\n",
      "Train Epoch: 26 [3200/8274 (38%)]\tLoss: 0.007919\n",
      "Train Epoch: 26 [3840/8274 (46%)]\tLoss: 0.004708\n",
      "Train Epoch: 26 [4480/8274 (54%)]\tLoss: 0.007779\n",
      "Train Epoch: 26 [5120/8274 (62%)]\tLoss: 0.026501\n",
      "Train Epoch: 26 [5760/8274 (69%)]\tLoss: 0.039110\n",
      "Train Epoch: 26 [6400/8274 (77%)]\tLoss: 0.031134\n",
      "Train Epoch: 26 [7040/8274 (85%)]\tLoss: 0.023837\n",
      "Train Epoch: 26 [7680/8274 (92%)]\tLoss: 0.011239\n",
      "\n",
      "Test set: Average loss: 2.8549, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 27 [0/8274 (0%)]\tLoss: 0.014370\n",
      "Train Epoch: 27 [640/8274 (8%)]\tLoss: 0.013153\n",
      "Train Epoch: 27 [1280/8274 (15%)]\tLoss: 0.028805\n",
      "Train Epoch: 27 [1920/8274 (23%)]\tLoss: 0.022800\n",
      "Train Epoch: 27 [2560/8274 (31%)]\tLoss: 0.008444\n",
      "Train Epoch: 27 [3200/8274 (38%)]\tLoss: 0.019067\n",
      "Train Epoch: 27 [3840/8274 (46%)]\tLoss: 0.009789\n",
      "Train Epoch: 27 [4480/8274 (54%)]\tLoss: 0.008590\n",
      "Train Epoch: 27 [5120/8274 (62%)]\tLoss: 0.078325\n",
      "Train Epoch: 27 [5760/8274 (69%)]\tLoss: 0.133475\n",
      "Train Epoch: 27 [6400/8274 (77%)]\tLoss: 0.006276\n",
      "Train Epoch: 27 [7040/8274 (85%)]\tLoss: 0.134562\n",
      "Train Epoch: 27 [7680/8274 (92%)]\tLoss: 0.066697\n",
      "\n",
      "Test set: Average loss: 2.9342, Accuracy: 589/1019 (58%)\n",
      "\n",
      "Train Epoch: 28 [0/8274 (0%)]\tLoss: 0.086127\n",
      "Train Epoch: 28 [640/8274 (8%)]\tLoss: 0.036908\n",
      "Train Epoch: 28 [1280/8274 (15%)]\tLoss: 0.022360\n",
      "Train Epoch: 28 [1920/8274 (23%)]\tLoss: 0.079906\n",
      "Train Epoch: 28 [2560/8274 (31%)]\tLoss: 0.046611\n",
      "Train Epoch: 28 [3200/8274 (38%)]\tLoss: 0.117060\n",
      "Train Epoch: 28 [3840/8274 (46%)]\tLoss: 0.017903\n",
      "Train Epoch: 28 [4480/8274 (54%)]\tLoss: 0.056925\n",
      "Train Epoch: 28 [5120/8274 (62%)]\tLoss: 0.081049\n",
      "Train Epoch: 28 [5760/8274 (69%)]\tLoss: 0.138689\n",
      "Train Epoch: 28 [6400/8274 (77%)]\tLoss: 0.018585\n",
      "Train Epoch: 28 [7040/8274 (85%)]\tLoss: 0.189612\n",
      "Train Epoch: 28 [7680/8274 (92%)]\tLoss: 0.070425\n",
      "\n",
      "Test set: Average loss: 2.7016, Accuracy: 601/1019 (59%)\n",
      "\n",
      "Train Epoch: 29 [0/8274 (0%)]\tLoss: 0.058079\n",
      "Train Epoch: 29 [640/8274 (8%)]\tLoss: 0.026835\n",
      "Train Epoch: 29 [1280/8274 (15%)]\tLoss: 0.148137\n",
      "Train Epoch: 29 [1920/8274 (23%)]\tLoss: 0.050415\n",
      "Train Epoch: 29 [2560/8274 (31%)]\tLoss: 0.137175\n",
      "Train Epoch: 29 [3200/8274 (38%)]\tLoss: 0.019211\n",
      "Train Epoch: 29 [3840/8274 (46%)]\tLoss: 0.016074\n",
      "Train Epoch: 29 [4480/8274 (54%)]\tLoss: 0.020260\n",
      "Train Epoch: 29 [5120/8274 (62%)]\tLoss: 0.005042\n",
      "Train Epoch: 29 [5760/8274 (69%)]\tLoss: 0.054763\n",
      "Train Epoch: 29 [6400/8274 (77%)]\tLoss: 0.151654\n",
      "Train Epoch: 29 [7040/8274 (85%)]\tLoss: 0.012993\n",
      "Train Epoch: 29 [7680/8274 (92%)]\tLoss: 0.011769\n",
      "\n",
      "Test set: Average loss: 2.7261, Accuracy: 577/1019 (57%)\n",
      "\n",
      "Train Epoch: 30 [0/8274 (0%)]\tLoss: 0.014381\n",
      "Train Epoch: 30 [640/8274 (8%)]\tLoss: 0.037837\n",
      "Train Epoch: 30 [1280/8274 (15%)]\tLoss: 0.022595\n",
      "Train Epoch: 30 [1920/8274 (23%)]\tLoss: 0.008967\n",
      "Train Epoch: 30 [2560/8274 (31%)]\tLoss: 0.012117\n",
      "Train Epoch: 30 [3200/8274 (38%)]\tLoss: 0.007264\n",
      "Train Epoch: 30 [3840/8274 (46%)]\tLoss: 0.010802\n",
      "Train Epoch: 30 [4480/8274 (54%)]\tLoss: 0.036873\n",
      "Train Epoch: 30 [5120/8274 (62%)]\tLoss: 0.015675\n",
      "Train Epoch: 30 [5760/8274 (69%)]\tLoss: 0.006613\n",
      "Train Epoch: 30 [6400/8274 (77%)]\tLoss: 0.041388\n",
      "Train Epoch: 30 [7040/8274 (85%)]\tLoss: 0.007792\n",
      "Train Epoch: 30 [7680/8274 (92%)]\tLoss: 0.013853\n",
      "\n",
      "Test set: Average loss: 2.7536, Accuracy: 594/1019 (58%)\n",
      "\n",
      "Train Epoch: 31 [0/8274 (0%)]\tLoss: 0.036755\n",
      "Train Epoch: 31 [640/8274 (8%)]\tLoss: 0.011611\n",
      "Train Epoch: 31 [1280/8274 (15%)]\tLoss: 0.012398\n",
      "Train Epoch: 31 [1920/8274 (23%)]\tLoss: 0.014843\n",
      "Train Epoch: 31 [2560/8274 (31%)]\tLoss: 0.018913\n",
      "Train Epoch: 31 [3200/8274 (38%)]\tLoss: 0.035337\n",
      "Train Epoch: 31 [3840/8274 (46%)]\tLoss: 0.009089\n",
      "Train Epoch: 31 [4480/8274 (54%)]\tLoss: 0.005416\n",
      "Train Epoch: 31 [5120/8274 (62%)]\tLoss: 0.006482\n",
      "Train Epoch: 31 [5760/8274 (69%)]\tLoss: 0.008757\n",
      "Train Epoch: 31 [6400/8274 (77%)]\tLoss: 0.002251\n",
      "Train Epoch: 31 [7040/8274 (85%)]\tLoss: 0.022499\n",
      "Train Epoch: 31 [7680/8274 (92%)]\tLoss: 0.007302\n",
      "\n",
      "Test set: Average loss: 2.9249, Accuracy: 596/1019 (58%)\n",
      "\n",
      "Train Epoch: 32 [0/8274 (0%)]\tLoss: 0.004376\n",
      "Train Epoch: 32 [640/8274 (8%)]\tLoss: 0.053395\n",
      "Train Epoch: 32 [1280/8274 (15%)]\tLoss: 0.007430\n",
      "Train Epoch: 32 [1920/8274 (23%)]\tLoss: 0.010962\n",
      "Train Epoch: 32 [2560/8274 (31%)]\tLoss: 0.006542\n",
      "Train Epoch: 32 [3200/8274 (38%)]\tLoss: 0.003529\n",
      "Train Epoch: 32 [3840/8274 (46%)]\tLoss: 0.075894\n",
      "Train Epoch: 32 [4480/8274 (54%)]\tLoss: 0.012384\n",
      "Train Epoch: 32 [5120/8274 (62%)]\tLoss: 0.003031\n",
      "Train Epoch: 32 [5760/8274 (69%)]\tLoss: 0.006107\n",
      "Train Epoch: 32 [6400/8274 (77%)]\tLoss: 0.007841\n",
      "Train Epoch: 32 [7040/8274 (85%)]\tLoss: 0.004080\n",
      "Train Epoch: 32 [7680/8274 (92%)]\tLoss: 0.005105\n",
      "\n",
      "Test set: Average loss: 3.0370, Accuracy: 581/1019 (57%)\n",
      "\n",
      "Train Epoch: 33 [0/8274 (0%)]\tLoss: 0.002988\n",
      "Train Epoch: 33 [640/8274 (8%)]\tLoss: 0.002767\n",
      "Train Epoch: 33 [1280/8274 (15%)]\tLoss: 0.010811\n",
      "Train Epoch: 33 [1920/8274 (23%)]\tLoss: 0.007541\n",
      "Train Epoch: 33 [2560/8274 (31%)]\tLoss: 0.017338\n",
      "Train Epoch: 33 [3200/8274 (38%)]\tLoss: 0.001549\n",
      "Train Epoch: 33 [3840/8274 (46%)]\tLoss: 0.005696\n",
      "Train Epoch: 33 [4480/8274 (54%)]\tLoss: 0.001752\n",
      "Train Epoch: 33 [5120/8274 (62%)]\tLoss: 0.012058\n",
      "Train Epoch: 33 [5760/8274 (69%)]\tLoss: 0.002786\n",
      "Train Epoch: 33 [6400/8274 (77%)]\tLoss: 0.010941\n",
      "Train Epoch: 33 [7040/8274 (85%)]\tLoss: 0.001580\n",
      "Train Epoch: 33 [7680/8274 (92%)]\tLoss: 0.003238\n",
      "\n",
      "Test set: Average loss: 3.1142, Accuracy: 590/1019 (58%)\n",
      "\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "class DNNArgs:\n",
    "  epochs = 33 # Tuning the epoch to 33 using validation set \n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "DNNargs = DNNArgs()\n",
    "\n",
    "torch.manual_seed(DNNargs.seed)\n",
    "\n",
    "dnn_model = DNN().to(device)\n",
    "\n",
    "for param_tensor in dnn_model.state_dict():\n",
    "        print(param_tensor, \"\\t\", dnn_model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "dnn_optimizer = optim.Adam(dnn_model.parameters(), lr=DNNargs.lr)\n",
    "\n",
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, DNNargs.epochs + 1):\n",
    "    train(DNNargs, dnn_model, device, train_loader, dnn_optimizer, epoch)\n",
    "    ACC_ = test(dnn_model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(dnn_model.state_dict(), \"Baseline_DNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 60.15701668302257\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "dnn_model.eval()\n",
    "dnn_correct_val = 0\n",
    "dnn_total_val = 0\n",
    "dnn_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = dnn_model(data)\n",
    "        pred = torch.argmax(output_test, 1)\n",
    "        \n",
    "        dnn_val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        dnn_correct_val += (pred == target).sum().item()\n",
    "        \n",
    "        dnn_total_val += target.size(0)\n",
    "    \n",
    "    test_accuracy = (dnn_correct_val / dnn_total_val) * 100\n",
    "            \n",
    "    dnn_val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(['target', 'casename'], axis=1)\n",
    "y = processed_df['target'] \n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "# Handle imbalanced classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Param Tuning (Grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.83\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Optionally, use the best estimator to make predictions\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(best_rf, 'model/rf_model.joblib')\n",
    "best_rf = load('model/rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>division</td>\n",
       "      <td>0.009125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>anything</td>\n",
       "      <td>0.004404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>july</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>criminal law</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>argued</td>\n",
       "      <td>0.002695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>adverse possession</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>international taxation</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>hdb flat</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>courts and jurisdiction</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>misrepresentation act</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature  Importance\n",
       "1788                 division    0.009125\n",
       "1542                 anything    0.004404\n",
       "1985                     july    0.003800\n",
       "487              criminal law    0.003110\n",
       "1567                   argued    0.002695\n",
       "...                       ...         ...\n",
       "226        adverse possession    0.000053\n",
       "825    international taxation    0.000052\n",
       "732                  hdb flat    0.000050\n",
       "473   courts and jurisdiction    0.000050\n",
       "940     misrepresentation act    0.000050\n",
       "\n",
       "[1166 rows x 2 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_train_resampled.columns\n",
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df = feature_importance_df[(feature_importance_df['Importance']) > 0.00005]\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>division</th>\n",
       "      <th>anything</th>\n",
       "      <th>july</th>\n",
       "      <th>criminal law</th>\n",
       "      <th>argued</th>\n",
       "      <th>majority</th>\n",
       "      <th>allegedly</th>\n",
       "      <th>failed</th>\n",
       "      <th>ordinary</th>\n",
       "      <th>iii</th>\n",
       "      <th>...</th>\n",
       "      <th>res judicata</th>\n",
       "      <th>family violence</th>\n",
       "      <th>admiralty and shipping</th>\n",
       "      <th>advice</th>\n",
       "      <th>advice</th>\n",
       "      <th>adverse possession</th>\n",
       "      <th>international taxation</th>\n",
       "      <th>hdb flat</th>\n",
       "      <th>courts and jurisdiction</th>\n",
       "      <th>misrepresentation act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029518</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.174046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8269</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.019155</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8273</th>\n",
       "      <td>0.007187</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8274 rows × 1257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      division  anything      july  criminal law    argued  majority  \\\n",
       "0     0.009229  0.000000  0.000000             0  0.005410  0.000000   \n",
       "1     0.001328  0.001974  0.000802             0  0.003113  0.002480   \n",
       "2     0.000000  0.017659  0.038272             0  0.000000  0.000000   \n",
       "3     0.000000  0.001575  0.174046             0  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000             0  0.008033  0.000000   \n",
       "...        ...       ...       ...           ...       ...       ...   \n",
       "8269  0.000000  0.001823  0.017783             0  0.030196  0.002291   \n",
       "8270  0.000000  0.000000  0.000000             0  0.000000  0.000000   \n",
       "8271  0.000000  0.000000  0.005170             0  0.020067  0.000000   \n",
       "8272  0.000000  0.000000  0.005254             0  0.010197  0.000000   \n",
       "8273  0.007187  0.005342  0.008684             0  0.000000  0.000000   \n",
       "\n",
       "      allegedly    failed  ordinary       iii  ...  res judicata  \\\n",
       "0      0.000000  0.005148  0.000000  0.029518  ...             0   \n",
       "1      0.000000  0.002962  0.012157  0.005308  ...             0   \n",
       "2      0.006375  0.022086  0.006592  0.000000  ...             0   \n",
       "3      0.017053  0.009453  0.001763  0.000000  ...             0   \n",
       "4      0.000000  0.007644  0.000000  0.010957  ...             0   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "8269   0.001975  0.019155  0.004084  0.005884  ...             0   \n",
       "8270   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8271   0.006889  0.038189  0.007124  0.000000  ...             0   \n",
       "8272   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8273   0.000000  0.016036  0.000000  0.000000  ...             0   \n",
       "\n",
       "      family violence  admiralty and shipping  advice  advice  \\\n",
       "0                   0                       0       0     0.0   \n",
       "1                   0                       0       0     0.0   \n",
       "2                   0                       0       0     0.0   \n",
       "3                   0                       0       0     0.0   \n",
       "4                   0                       0       0     0.0   \n",
       "...               ...                     ...     ...     ...   \n",
       "8269                0                       0       0     0.0   \n",
       "8270                0                       0       0     0.0   \n",
       "8271                0                       0       0     0.0   \n",
       "8272                0                       0       0     0.0   \n",
       "8273                0                       0       0     0.0   \n",
       "\n",
       "      adverse possession  international taxation  hdb flat  \\\n",
       "0                      0                       0         0   \n",
       "1                      0                       0         0   \n",
       "2                      0                       0         0   \n",
       "3                      0                       0         0   \n",
       "4                      0                       0         0   \n",
       "...                  ...                     ...       ...   \n",
       "8269                   0                       0         0   \n",
       "8270                   0                       0         0   \n",
       "8271                   0                       0         0   \n",
       "8272                   0                       0         0   \n",
       "8273                   0                       0         0   \n",
       "\n",
       "      courts and jurisdiction  misrepresentation act  \n",
       "0                           0                      0  \n",
       "1                           0                      0  \n",
       "2                           0                      0  \n",
       "3                           0                      0  \n",
       "4                           0                      0  \n",
       "...                       ...                    ...  \n",
       "8269                        0                      0  \n",
       "8270                        0                      0  \n",
       "8271                        0                      0  \n",
       "8272                        0                      0  \n",
       "8273                        0                      0  \n",
       "\n",
       "[8274 rows x 1257 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = feature_importance_df['Feature'].tolist()\n",
    "\n",
    "X_train_filtered = X_train_resampled[important_features]\n",
    "X_test_filtered = X_test[important_features]\n",
    "X_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFE(best_rf, n_features_to_select=1000, step=1)\n",
    "selector = selector.fit(X_train_filtered, y_train_resampled)\n",
    "X_train_reduced = selector.transform(X_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-14 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-14 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-14 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-14 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-14 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-14 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-14 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-14 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-14 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.fit(X_train_filtered, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5834151128557409\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Favourable       0.59      0.95      0.73      1183\n",
      "  No outcome       0.31      0.02      0.04       238\n",
      "Unfavourable       0.48      0.10      0.17       617\n",
      "\n",
      "    accuracy                           0.58      2038\n",
      "   macro avg       0.46      0.36      0.31      2038\n",
      "weighted avg       0.53      0.58      0.48      2038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_rf.predict(X_test_filtered)\n",
    "\n",
    "# Evaluating the Model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Multiclass Logistic Regression) in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# evaluate pipeline\u001b[39;00m\n\u001b[0;32m     20\u001b[0m cv \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m     23\u001b[0m \tresults\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti lbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m,key,mean(scores)])\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imblearn\\pipeline.py:326\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    325\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 326\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, yt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[0;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    454\u001b[0m ]\n\u001b[1;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    470\u001b[0m     solver,\n\u001b[0;32m    471\u001b[0m     opt_res,\n\u001b[0;32m    472\u001b[0m     max_iter,\n\u001b[0;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:692\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    690\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 692\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    693\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    695\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    696\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ng_ho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:276\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    273\u001b[0m n_dof \u001b[38;5;241m=\u001b[39m n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     weights, intercept, raw_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_intercept_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = processed_df.drop(columns=['target','casename','processed_facts', 'processed_issues'])\n",
    "y = processed_df['target']\n",
    "\n",
    "# Handle imbalanced classes\n",
    "smt = SMOTE(random_state=42)\n",
    "\n",
    "results = []\n",
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', smt), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "  \n",
    "\t\t#StratifiedKFold is the improved version of KFold\n",
    "\t\t#KFold is a cross-validator that divides the dataset into k folds. \n",
    "  \t\t#Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['multi lbfgs','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['multi lbfgs','l2',key,mean(scores)])\n",
    "\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p  \n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='lbfgs', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='lbfgs', penalty='l2', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', SMOTE()), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['ovr lbfgs','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['ovr lbfgs','l2',key,mean(scores)])\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='saga', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l1', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', smt), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['multi saga','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['multi saga','l1',key,mean(scores)])\n",
    "\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the results\n",
      "0 0.0000 0.489\n",
      "l2 0.0000 0.489\n",
      "l2 0.0001 0.275\n",
      "l2 0.0010 0.397\n",
      "l2 0.0100 0.462\n",
      "l2 0.1000 0.489\n",
      "l2 1.0000 0.490\n",
      "l1 0.0001 0.285\n",
      "l1 0.0010 0.277\n",
      "l1 0.0100 0.331\n",
      "l1 0.1000 0.479\n",
      "l1 1.0000 0.505\n"
     ]
    }
   ],
   "source": [
    "for p in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p  \n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='liblinear', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='liblinear', penalty='l1', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', SMOTE()), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['ovr liblinear','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['ovr liblinear','l1',key,mean(scores)])\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = max(result, key=lambda x: x[3])\n",
    "print(\"Maximum acc:\", max_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
