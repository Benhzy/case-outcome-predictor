{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips from prof\n",
    "\n",
    "- Narrow scope of work (e.g. court level)\n",
    "\n",
    "- Could try both binary/multi-class model outcomes and compare the performance \n",
    "\n",
    "- Change user from layperson to legal professional (and mention that this project is a stepping stone towards having layperson use the model)\n",
    "\n",
    "- Link features to predicted outcome (if time permits can try using XGBoost with LIME for model interpretability)\n",
    "\n",
    "- Can also try to see accuracy of models with different areas of law, lowest accuracy may be hardest area of law to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from spacy import displacy\n",
    "import json\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename                                        area_of_law  \\\n",
      "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
      "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
      "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
      "3  2000_SGCA_12.pdf  {'courts and jurisdiction': ['court of appeal'...   \n",
      "4  2000_SGCA_13.pdf                     {'criminal law': ['offences']}   \n",
      "\n",
      "  court_level                                             issues  \\\n",
      "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
      "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
      "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
      "3        SGCA  the appeals from the assistant registrar. In h...   \n",
      "4        SGCA  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
      "4  facts. Mere assertion would not suffice. In ex...          28.0   \n",
      "\n",
      "   facts_topic        target  \n",
      "0          7.0    Favourable  \n",
      "1          3.0    Favourable  \n",
      "2         12.0    No outcome  \n",
      "3         10.0  Unfavourable  \n",
      "4         13.0  Unfavourable  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV files into DataFrames\n",
    "areas_of_law_df = pd.read_csv(\"data/prediction_data/areas_of_law.csv\")\n",
    "coram_df = pd.read_csv(\"data/prediction_data/coram.csv\")\n",
    "sg_legal_cases_df = pd.read_csv(\"data/prediction_data/sg_legal_cases_dataset.csv\")\n",
    "target_rulings_df = pd.read_csv(\"data/prediction_data/target_rulings.csv\")\n",
    "issues_facts_df = pd.read_csv(\"data/prediction_data/issues_facts_topic.csv\")\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/issues.json') as f:\n",
    "    issues_data = [json.loads(line) for line in f]\n",
    "issues_df = pd.DataFrame(issues_data)\n",
    "\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/rawish_data/facts.json') as f:\n",
    "    facts_data = [json.loads(line) for line in f]\n",
    "raw_facts_df = pd.DataFrame(facts_data)\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(areas_of_law_df, sg_legal_cases_df, on='casename', how='outer')\n",
    "merged_df = pd.merge(merged_df, issues_df, on='casename', how='outer')\n",
    "merged_df = pd.merge(merged_df, raw_facts_df, on='casename', how='outer')\n",
    "merged_df = pd.merge(merged_df, issues_facts_df, on='casename', how='outer')\n",
    "merged_df = pd.merge(merged_df, target_rulings_df, on='casename', how='outer')\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate coram names and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coram_names(coram_list):\n",
    "    all_names = set()\n",
    "    for item in coram_list:\n",
    "        split_names = re.split(r';\\s(?![a-zA-Z]+\\s)', item)\n",
    "        for name in split_names:\n",
    "            if ';' in name and not re.search(r';\\s[a-zA-Z]+$', name):\n",
    "                sub_names = name.split(';')\n",
    "                all_names.update([n.strip() for n in sub_names if n.strip()])\n",
    "            else:\n",
    "                all_names.add(name.strip())\n",
    "    return list(all_names)\n",
    "\n",
    "def remove_coram_roles(coram_list):\n",
    "    roles = [' CJ', ' AG', ' J', ' DCJ', ' JA', ' AR', ' JC', 'SAR']\n",
    "    for role in roles:\n",
    "        coram_list = [re.sub(rf'{role}$', '', name) for name in coram_list]\n",
    "    return coram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "coram_df = coram_df.dropna()\n",
    "for i, coram_str in enumerate(coram_df['Coram']):\n",
    "    coram = ast.literal_eval(coram_str)\n",
    "    \n",
    "    coram_modified = clean_coram_names(coram)\n",
    "    coram_modified = remove_coram_roles(coram_modified)\n",
    "    coram_df.at[i, 'Coram'] = str(coram_modified)\n",
    "merged_df = pd.merge(merged_df, coram_df, on='casename', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casename         7\n",
      "area_of_law      7\n",
      "court_level      7\n",
      "issues          54\n",
      "facts           54\n",
      "issues_topic    54\n",
      "facts_topic     54\n",
      "target          54\n",
      "Unnamed: 0      14\n",
      "Coram            7\n",
      "dtype: int64\n",
      "               casename area_of_law court_level issues facts  issues_topic  \\\n",
      "241   2000_SGHC_257.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "274   2000_SGHC_290.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "412    2001_SGCA_66.pdf          []        SGCA    NaN   NaN           NaN   \n",
      "432   2001_SGHC_101.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "438   2001_SGHC_108.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "442   2001_SGHC_111.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "448   2001_SGHC_118.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "457   2001_SGHC_128.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "460   2001_SGHC_130.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "462   2001_SGHC_132.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "475   2001_SGHC_148.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "478   2001_SGHC_150.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "479   2001_SGHC_151.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "489   2001_SGHC_163.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "498   2001_SGHC_174.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "536   2001_SGHC_214.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "537   2001_SGHC_215.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "544   2001_SGHC_222.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "546   2001_SGHC_224.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "550   2001_SGHC_228.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "551   2001_SGHC_229.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "555   2001_SGHC_232.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "564   2001_SGHC_240.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "568   2001_SGHC_244.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "574   2001_SGHC_250.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "578   2001_SGHC_254.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "581   2001_SGHC_257.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "591   2001_SGHC_266.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "592   2001_SGHC_267.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "596   2001_SGHC_270.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "603   2001_SGHC_277.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "615   2001_SGHC_289.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "619   2001_SGHC_292.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "622   2001_SGHC_295.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "630   2001_SGHC_301.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "633   2001_SGHC_304.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "647   2001_SGHC_319.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "651   2001_SGHC_322.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "658   2001_SGHC_329.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "667   2001_SGHC_337.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "675   2001_SGHC_344.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "678   2001_SGHC_347.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "700   2001_SGHC_367.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "701   2001_SGHC_368.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "705   2001_SGHC_371.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "707   2001_SGHC_373.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "713   2001_SGHC_379.pdf          []        SGHC    NaN   NaN           NaN   \n",
      "8567                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8568                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8569                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8570                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8571                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8572                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8573                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "\n",
      "      facts_topic target  Unnamed: 0  \\\n",
      "241           NaN    NaN      4576.0   \n",
      "274           NaN    NaN      6101.0   \n",
      "412           NaN    NaN       546.0   \n",
      "432           NaN    NaN      3523.0   \n",
      "438           NaN    NaN      5634.0   \n",
      "442           NaN    NaN      2921.0   \n",
      "448           NaN    NaN      5000.0   \n",
      "457           NaN    NaN      3437.0   \n",
      "460           NaN    NaN      5026.0   \n",
      "462           NaN    NaN      4572.0   \n",
      "475           NaN    NaN      1856.0   \n",
      "478           NaN    NaN      6622.0   \n",
      "479           NaN    NaN      6720.0   \n",
      "489           NaN    NaN      1300.0   \n",
      "498           NaN    NaN      1840.0   \n",
      "536           NaN    NaN      1549.0   \n",
      "537           NaN    NaN      1166.0   \n",
      "544           NaN    NaN      8492.0   \n",
      "546           NaN    NaN      7382.0   \n",
      "550           NaN    NaN      1534.0   \n",
      "551           NaN    NaN      1180.0   \n",
      "555           NaN    NaN      6498.0   \n",
      "564           NaN    NaN      5942.0   \n",
      "568           NaN    NaN      4712.0   \n",
      "574           NaN    NaN      4698.0   \n",
      "578           NaN    NaN      5957.0   \n",
      "581           NaN    NaN      5405.0   \n",
      "591           NaN    NaN      3120.0   \n",
      "592           NaN    NaN      2787.0   \n",
      "596           NaN    NaN      2259.0   \n",
      "603           NaN    NaN      3650.0   \n",
      "615           NaN    NaN      3324.0   \n",
      "619           NaN    NaN      4384.0   \n",
      "622           NaN    NaN      5764.0   \n",
      "630           NaN    NaN      3871.0   \n",
      "633           NaN    NaN      2337.0   \n",
      "647           NaN    NaN      4403.0   \n",
      "651           NaN    NaN      5728.0   \n",
      "658           NaN    NaN      3879.0   \n",
      "667           NaN    NaN      5477.0   \n",
      "675           NaN    NaN      7311.0   \n",
      "678           NaN    NaN      6765.0   \n",
      "700           NaN    NaN       174.0   \n",
      "701           NaN    NaN      7642.0   \n",
      "705           NaN    NaN       909.0   \n",
      "707           NaN    NaN       162.0   \n",
      "713           NaN    NaN      7070.0   \n",
      "8567          NaN    NaN         NaN   \n",
      "8568          NaN    NaN         NaN   \n",
      "8569          NaN    NaN         NaN   \n",
      "8570          NaN    NaN         NaN   \n",
      "8571          NaN    NaN         NaN   \n",
      "8572          NaN    NaN         NaN   \n",
      "8573          NaN    NaN         NaN   \n",
      "\n",
      "                                                  Coram  \n",
      "241                                 ['Sundaresh Menon']  \n",
      "274   ['Tan Lee Meng', 'Chao Hick Tin', 'Yong Pung H...  \n",
      "412                                  ['Ang Cheng Hock']  \n",
      "432   ['Chao Hick Tin', 'Choo Han Teck', 'Yong Pung ...  \n",
      "438                                 ['Pang Khang Chau']  \n",
      "442                                     ['Edmund Leow']  \n",
      "448                                  ['Amarjeet Singh']  \n",
      "457                                     ['Quentin Loh']  \n",
      "460                      ['V K Rajah', 'Chao Hick Tin']  \n",
      "462                                   ['Chan Seng Onn']  \n",
      "475                             ['Vinodh Coomaraswamy']  \n",
      "478                                   ['Kannan Ramesh']  \n",
      "479   ['Judith Prakash', 'Steven Chong', 'Sundaresh ...  \n",
      "489                                      ['Woo Bih Li']  \n",
      "498                                    ['Lai Siu Chiu']  \n",
      "536                                     ['S Rajendran']  \n",
      "537                                       ['MPH Rubin']  \n",
      "544   ['Tay Yong Kwang', 'Judith Prakash', 'Sundares...  \n",
      "546                                  ['Tan Siong Thye']  \n",
      "550                                  ['Hoo Sheau Peng']  \n",
      "551                                    ['Lai Kew Chai']  \n",
      "555                                   ['Choo Han Teck']  \n",
      "564                                      ['George Wei']  \n",
      "568                                   ['Chan Seng Onn']  \n",
      "574                                  ['Tay Yong Kwang']  \n",
      "578                                   ['Kan Ting Chiu']  \n",
      "581   ['Tan Lee Meng', 'Chao Hick Tin', 'Yong Pung H...  \n",
      "591                                  ['Judith Prakash']  \n",
      "592                                    ['Lee Seiu Kin']  \n",
      "596                                    ['Lee Kim Shin']  \n",
      "603                                   ['Kan Ting Chiu']  \n",
      "615                                       ['MPH Rubin']  \n",
      "619                                  ['Kwek Mean Luck']  \n",
      "622                                      ['Woo Bih Li']  \n",
      "630                             ['Belinda Ang Saw Ean']  \n",
      "633                                   ['Choo Han Teck']  \n",
      "647                                   ['Valerie Thean']  \n",
      "651                                  ['Judith Prakash']  \n",
      "658                             ['Belinda Ang Saw Ean']  \n",
      "667                                      ['Woo Bih Li']  \n",
      "675                                   ['Yong Pung How']  \n",
      "678                             ['Belinda Ang Saw Ean']  \n",
      "700                                      ['Woo Bih Li']  \n",
      "701                                   ['Valerie Thean']  \n",
      "705                                    ['Lai Siu Chiu']  \n",
      "707                                   ['Yong Pung How']  \n",
      "713                                ['Dedar Singh Gill']  \n",
      "8567  ['Steven Chong', 'Andrew Phang Boon Leong', 'S...  \n",
      "8568                                    ['Lim Jian Yi']  \n",
      "8569  ['Tay Yong Kwang', 'Judith Prakash', 'Andrew P...  \n",
      "8570                     ['L P Thean', 'Chao Hick Tin']  \n",
      "8571                                 ['Judith Prakash']  \n",
      "8572     ['V K Rajah', 'Steven Chong', 'Kan Ting Chiu']  \n",
      "8573                                  ['Valerie Thean']  \n",
      "casename        0\n",
      "area_of_law     0\n",
      "court_level     0\n",
      "issues          0\n",
      "facts           0\n",
      "issues_topic    0\n",
      "facts_topic     0\n",
      "target          0\n",
      "Unnamed: 0      0\n",
      "Coram           0\n",
      "dtype: int64\n",
      "target\n",
      "Favourable      3941\n",
      "Unfavourable    2056\n",
      "No outcome       794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = merged_df.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "#nas are probably those reassigned cases, coram has 7, i just drop them for now\n",
    "na_target_rows = merged_df[merged_df['target'].isna()]\n",
    "print(na_target_rows)\n",
    "\n",
    "merged_df.dropna(axis=0, inplace=True)\n",
    "print(merged_df.isna().sum())\n",
    "\n",
    "#remove empty lists\n",
    "merged_df = merged_df.query(\"area_of_law != '[]'\")\n",
    "\n",
    "#target is unbalanced\n",
    "target_counts = merged_df['target'].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True) # prevent nan values from appearing after one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casename</th>\n",
       "      <th>area_of_law</th>\n",
       "      <th>court_level</th>\n",
       "      <th>issues</th>\n",
       "      <th>facts</th>\n",
       "      <th>issues_topic</th>\n",
       "      <th>facts_topic</th>\n",
       "      <th>target</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Coram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_SGCA_1.pdf</td>\n",
       "      <td>{'civil procedure': ['pleadings'], 'res judica...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The claim was dismissed with costs by the\\nHig...</td>\n",
       "      <td>The facts\\nThe appellant is the widow of one T...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>4258.0</td>\n",
       "      <td>[V K Rajah, Chan Sek Keong, Andrew Phang Boon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_SGCA_10.pdf</td>\n",
       "      <td>{'contract': ['formation'], 'equity': ['defenc...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the claim and\\nagainst that decision this appe...</td>\n",
       "      <td>facts and surrounding circumstances including ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>[V K Rajah, Chao Hick Tin, Andrew Phang Boon L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000_SGCA_11.pdf</td>\n",
       "      <td>{'contract': ['discharge'], 'damages': ['asses...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The appeal \\nThe questions which arise in this...</td>\n",
       "      <td>Background \\nThe first appellants, a French co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>7876.0</td>\n",
       "      <td>[Tan Lee Meng, Chan Sek Keong, Andrew Phang Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           casename                                        area_of_law  \\\n",
       "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
       "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
       "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
       "\n",
       "  court_level                                             issues  \\\n",
       "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
       "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
       "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
       "\n",
       "                                               facts  issues_topic  \\\n",
       "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
       "1  facts and surrounding circumstances including ...           8.0   \n",
       "2  Background \\nThe first appellants, a French co...           0.0   \n",
       "\n",
       "   facts_topic      target  Unnamed: 0  \\\n",
       "0          7.0  Favourable      4258.0   \n",
       "1          3.0  Favourable      7628.0   \n",
       "2         12.0  No outcome      7876.0   \n",
       "\n",
       "                                               Coram  \n",
       "0  [V K Rajah, Chan Sek Keong, Andrew Phang Boon ...  \n",
       "1  [V K Rajah, Chao Hick Tin, Andrew Phang Boon L...  \n",
       "2  [Tan Lee Meng, Chan Sek Keong, Andrew Phang Bo...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(ast.literal_eval)\n",
    "merged_df['Coram'] = merged_df['Coram'].apply(ast.literal_eval)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flatten areas_of_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Flattened AOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(law_list):\n",
    "    cleaned_list = [item.replace(']', '').strip() for item in law_list]  # Removes ']' and extra spaces\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_keys_only(d):\n",
    "    if isinstance(d, dict):\n",
    "        return list(d.keys())\n",
    "    return [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_areas(areas):\n",
    "    flat_areas = []\n",
    "    for main_area, sub_areas in areas.items():\n",
    "        flat_areas.append(main_area)\n",
    "        # Remove sub-area if it's longer than 33 characters\n",
    "        sub_areas = [sarea for sarea in sub_areas if len(sarea) <= 33]\n",
    "        flat_areas.extend(sub_areas)\n",
    "    return flat_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df['area_of_law'] = merged_df['area_of_law'].apply(flatten_keys_only)\n",
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(flatten_areas)\n",
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casename</th>\n",
       "      <th>area_of_law</th>\n",
       "      <th>court_level</th>\n",
       "      <th>issues</th>\n",
       "      <th>facts</th>\n",
       "      <th>issues_topic</th>\n",
       "      <th>facts_topic</th>\n",
       "      <th>target</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Coram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_SGCA_1.pdf</td>\n",
       "      <td>[civil procedure, res judicata, trusts]</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The claim was dismissed with costs by the\\nHig...</td>\n",
       "      <td>The facts\\nThe appellant is the widow of one T...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>4258.0</td>\n",
       "      <td>[V K Rajah, Chan Sek Keong, Andrew Phang Boon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_SGCA_10.pdf</td>\n",
       "      <td>[contract, equity]</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the claim and\\nagainst that decision this appe...</td>\n",
       "      <td>facts and surrounding circumstances including ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>[V K Rajah, Chao Hick Tin, Andrew Phang Boon L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000_SGCA_11.pdf</td>\n",
       "      <td>[contract, damages]</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The appeal \\nThe questions which arise in this...</td>\n",
       "      <td>Background \\nThe first appellants, a French co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>7876.0</td>\n",
       "      <td>[Tan Lee Meng, Chan Sek Keong, Andrew Phang Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000_SGCA_12.pdf</td>\n",
       "      <td>[courts and jurisdiction, words and phrases]</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the appeals from the assistant registrar. In h...</td>\n",
       "      <td>Background\\nMicrosoft, Adobe and Autodesk are ...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Unfavourable</td>\n",
       "      <td>8424.0</td>\n",
       "      <td>[Teo Guan Siew]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000_SGCA_13.pdf</td>\n",
       "      <td>[criminal law]</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the appeal on 24 January 2000 and dismissed it...</td>\n",
       "      <td>facts. Mere assertion would not suffice. In ex...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Unfavourable</td>\n",
       "      <td>8158.0</td>\n",
       "      <td>[Lionel Yee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>2023_SGHC_95.pdf</td>\n",
       "      <td>[criminal law]</td>\n",
       "      <td>SGHC</td>\n",
       "      <td>the issues (and sub-issue) that arise for my \\...</td>\n",
       "      <td>Facts \\n1 The first accused, Low Sze Song (“Lo...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>[Chan Seng Onn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>2023_SGHC_96.pdf</td>\n",
       "      <td>[tort]</td>\n",
       "      <td>SGHC</td>\n",
       "      <td>the claim for conspiracy to defraud). At most,...</td>\n",
       "      <td>the facts, and can only be giving his opinion,...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unfavourable</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>[Tay Yong Kwang, Judith Prakash, Sundaresh Men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>2023_SGHC_97.pdf</td>\n",
       "      <td>[civil procedure]</td>\n",
       "      <td>SGHC</td>\n",
       "      <td>the issues relating to the defences of \\njusti...</td>\n",
       "      <td>Facts\\nThe parties \\n3 The plaintiff, Mr Karan...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Unfavourable</td>\n",
       "      <td>5062.0</td>\n",
       "      <td>[Woo Bih Li]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>2023_SGHC_98.pdf</td>\n",
       "      <td>[insolvency law]</td>\n",
       "      <td>SGHC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>3995.0</td>\n",
       "      <td>[Judith Prakash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>2023_SGHC_99.pdf</td>\n",
       "      <td>[intellectual property]</td>\n",
       "      <td>SGHC</td>\n",
       "      <td>Background to the dispute\\n4 Towa commenced Su...</td>\n",
       "      <td>Facts\\nThe parties\\n2 The plaintiff (“Towa”) i...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>3988.0</td>\n",
       "      <td>[Yong Pung How]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6791 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              casename                                   area_of_law  \\\n",
       "0      2000_SGCA_1.pdf       [civil procedure, res judicata, trusts]   \n",
       "1     2000_SGCA_10.pdf                            [contract, equity]   \n",
       "2     2000_SGCA_11.pdf                           [contract, damages]   \n",
       "3     2000_SGCA_12.pdf  [courts and jurisdiction, words and phrases]   \n",
       "4     2000_SGCA_13.pdf                                [criminal law]   \n",
       "...                ...                                           ...   \n",
       "6786  2023_SGHC_95.pdf                                [criminal law]   \n",
       "6787  2023_SGHC_96.pdf                                        [tort]   \n",
       "6788  2023_SGHC_97.pdf                             [civil procedure]   \n",
       "6789  2023_SGHC_98.pdf                              [insolvency law]   \n",
       "6790  2023_SGHC_99.pdf                       [intellectual property]   \n",
       "\n",
       "     court_level                                             issues  \\\n",
       "0           SGCA  The claim was dismissed with costs by the\\nHig...   \n",
       "1           SGCA  the claim and\\nagainst that decision this appe...   \n",
       "2           SGCA  The appeal \\nThe questions which arise in this...   \n",
       "3           SGCA  the appeals from the assistant registrar. In h...   \n",
       "4           SGCA  the appeal on 24 January 2000 and dismissed it...   \n",
       "...          ...                                                ...   \n",
       "6786        SGHC  the issues (and sub-issue) that arise for my \\...   \n",
       "6787        SGHC  the claim for conspiracy to defraud). At most,...   \n",
       "6788        SGHC  the issues relating to the defences of \\njusti...   \n",
       "6789        SGHC                                                      \n",
       "6790        SGHC  Background to the dispute\\n4 Towa commenced Su...   \n",
       "\n",
       "                                                  facts  issues_topic  \\\n",
       "0     The facts\\nThe appellant is the widow of one T...          12.0   \n",
       "1     facts and surrounding circumstances including ...           8.0   \n",
       "2     Background \\nThe first appellants, a French co...           0.0   \n",
       "3     Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
       "4     facts. Mere assertion would not suffice. In ex...          28.0   \n",
       "...                                                 ...           ...   \n",
       "6786  Facts \\n1 The first accused, Low Sze Song (“Lo...          15.0   \n",
       "6787  the facts, and can only be giving his opinion,...          10.0   \n",
       "6788  Facts\\nThe parties \\n3 The plaintiff, Mr Karan...          18.0   \n",
       "6789                                                              0.0   \n",
       "6790  Facts\\nThe parties\\n2 The plaintiff (“Towa”) i...           5.0   \n",
       "\n",
       "      facts_topic        target  Unnamed: 0  \\\n",
       "0             7.0    Favourable      4258.0   \n",
       "1             3.0    Favourable      7628.0   \n",
       "2            12.0    No outcome      7876.0   \n",
       "3            10.0  Unfavourable      8424.0   \n",
       "4            13.0  Unfavourable      8158.0   \n",
       "...           ...           ...         ...   \n",
       "6786         11.0    No outcome      4512.0   \n",
       "6787          1.0  Unfavourable      5054.0   \n",
       "6788         10.0  Unfavourable      5062.0   \n",
       "6789          0.0    Favourable      3995.0   \n",
       "6790         19.0    Favourable      3988.0   \n",
       "\n",
       "                                                  Coram  \n",
       "0     [V K Rajah, Chan Sek Keong, Andrew Phang Boon ...  \n",
       "1     [V K Rajah, Chao Hick Tin, Andrew Phang Boon L...  \n",
       "2     [Tan Lee Meng, Chan Sek Keong, Andrew Phang Bo...  \n",
       "3                                       [Teo Guan Siew]  \n",
       "4                                          [Lionel Yee]  \n",
       "...                                                 ...  \n",
       "6786                                    [Chan Seng Onn]  \n",
       "6787  [Tay Yong Kwang, Judith Prakash, Sundaresh Men...  \n",
       "6788                                       [Woo Bih Li]  \n",
       "6789                                   [Judith Prakash]  \n",
       "6790                                    [Yong Pung How]  \n",
       "\n",
       "[6791 rows x 10 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using main area ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "\n",
      "   facts_topic      target  Unnamed: 0  \\\n",
      "0          7.0  Favourable      4258.0   \n",
      "1          3.0  Favourable      7628.0   \n",
      "2         12.0  No outcome      7876.0   \n",
      "\n",
      "                                               Coram  \\\n",
      "0  [V K Rajah, Chan Sek Keong, Andrew Phang Boon ...   \n",
      "1  [V K Rajah, Chao Hick Tin, Andrew Phang Boon L...   \n",
      "2  [Tan Lee Meng, Chan Sek Keong, Andrew Phang Bo...   \n",
      "\n",
      "   29 of the dissenting opinion) that  ...  statutory interpretation  \\\n",
      "0                                   0  ...                         0   \n",
      "1                                   0  ...                         0   \n",
      "2                                   0  ...                         0   \n",
      "\n",
      "   succession and  wills  succession and wills  tort  \\\n",
      "0                      0                     0     0   \n",
      "1                      0                     0     0   \n",
      "2                      0                     0     0   \n",
      "\n",
      "   trade marks  and trade names  trusts  \\\n",
      "0                             0       1   \n",
      "1                             0       0   \n",
      "2                             0       0   \n",
      "\n",
      "   unincorporated associations  and trade unions  \\\n",
      "0                                              0   \n",
      "1                                              0   \n",
      "2                                              0   \n",
      "\n",
      "   unincorporated associations and trade unions  words and phrases  \\\n",
      "0                                             0                  0   \n",
      "1                                             0                  0   \n",
      "2                                             0                  0   \n",
      "\n",
      "   young offenders  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "\n",
      "[3 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode aol\n",
    "processed_df = merged_df[merged_df['area_of_law'].apply(lambda x: isinstance(x, list))]\n",
    "processed_df = processed_df.reset_index(drop=True)\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(processed_df['area_of_law'])\n",
    "binary_aol_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_aol_df = binary_aol_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([processed_df.drop('area_of_law', axis=1), binary_aol_df], axis=1)\n",
    "\n",
    "print(processed_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "3  2000_SGCA_12.pdf        SGCA   \n",
      "4  2000_SGCA_13.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "3  the appeals from the assistant registrar. In h...   \n",
      "4  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
      "4  facts. Mere assertion would not suffice. In ex...          28.0   \n",
      "\n",
      "   facts_topic        target  Unnamed: 0  29 of the dissenting opinion) that  \\\n",
      "0          7.0    Favourable      4258.0                                   0   \n",
      "1          3.0    Favourable      7628.0                                   0   \n",
      "2         12.0    No outcome      7876.0                                   0   \n",
      "3         10.0  Unfavourable      8424.0                                   0   \n",
      "4         13.0  Unfavourable      8158.0                                   0   \n",
      "\n",
      "   abitration  ...  Vincent Hoong  Vincent Leow  Vinodh Coomaraswamy  \\\n",
      "0           0  ...              0             0                    0   \n",
      "1           0  ...              0             0                    0   \n",
      "2           0  ...              0             0                    0   \n",
      "3           0  ...              0             0                    0   \n",
      "4           0  ...              0             0                    0   \n",
      "\n",
      "   Wong Li Kok, Alex  Woo Bih Li  Yap Yew Choh Kenneth  Yeong Zee Kin  \\\n",
      "0                  0           0                     0              0   \n",
      "1                  0           0                     0              0   \n",
      "2                  0           0                     0              0   \n",
      "3                  0           0                     0              0   \n",
      "4                  0           0                     0              0   \n",
      "\n",
      "   Yong Pung How  Yong Pung How,  Zhuo Wenzhao  \n",
      "0              0               0             0  \n",
      "1              0               0             0  \n",
      "2              0               0             0  \n",
      "3              0               0             0  \n",
      "4              0               0             0  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode coram\n",
    "processed_df = processed_df[processed_df['Coram'].apply(lambda x: isinstance(x, list))]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(processed_df['Coram'])\n",
    "binary_coram_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_coram_df = binary_coram_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([processed_df.drop('Coram', axis=1), binary_coram_df], axis=1)\n",
    "\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['SGCA'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGCA' else 0)\n",
    "processed_df['SGHC'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGHC' else 0)\n",
    "processed_df = processed_df.drop('court_level', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(columns=['target'])\n",
    "y = processed_df['target']\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test_val = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test_val = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "#balanced dataset (target variable was imbalanced Favourable 5006 Unfavourable 2523 No outcome 984)\n",
    "#randomly found one online, can be changed -> need to check am i doing this right \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "#split further from X_test_val into X_val and X_test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target variable to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = X_train_resampled.drop(columns=['facts', 'issues'])\n",
    "X_test = X_test.drop(columns=['facts', 'issues'])\n",
    "X_val = X_val.drop(columns=['facts', 'issues'])\n",
    "\n",
    "mapping = {'Favourable': 1, 'Unfavourable': 0, 'No outcome':0.5}\n",
    "\n",
    "y_train_resampled, y_test, y_val = y_train_resampled.copy().map(mapping), y_test.copy().map(mapping), y_val.copy().map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform modelling\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  epochs = 20\n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: torch.Size([8274, 197])\n",
      "Shape of X_test: torch.Size([1019, 197])\n",
      "Shape of X_val: torch.Size([1019, 197])\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled = X_train_resampled.iloc[:, 1:1596].copy()\n",
    "X_train_resampled = torch.tensor(X_train_resampled.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_train_resampled: {X_train_resampled.shape}')\n",
    "\n",
    "X_test = X_test.iloc[:, 1:1596].copy()\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "\n",
    "X_val = X_val.iloc[:, 1:1596].copy()\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8274, 1, 197])\n"
     ]
    }
   ],
   "source": [
    "y_train_resampled, y_test, y_val = torch.tensor(y_train_resampled.values).to(device), torch.tensor(y_test.values).to(device), torch.tensor(y_val.values).to(device)\n",
    "\n",
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0],1,X_train_resampled.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])\n",
    "print(X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, 1,1, bias=True)\n",
    "        # Define the first 1D convolution layer. Takes 1 input channel, outputs 32 channels, kernel size is 3, stride is 1, padding is 1.\n",
    "        self.Bn1 = nn.BatchNorm1d(64)\n",
    "        # Apply Batch Normalization to the output of the first convolutional layer.\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # Apply 1D Average Pooling after the first Batch Normalization. The kernel size and stride are 2.\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3, 1,1, bias=True)\n",
    "        self.Bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(3136, 100, bias=True)\n",
    "        # Define the first fully connected layer. It takes 25472 inputs and outputs 100 nodes.\n",
    "\n",
    "        self.fc2 = nn.Linear(100, 30, bias=True)\n",
    "        # Define the second fully connected layer. It takes 100 inputs and outputs 50 nodes.\n",
    "\n",
    "        self.fc3 = nn.Linear(30, 3, bias=True)\n",
    "        # Define the third fully connected layer (output layer). It takes 50 inputs and outputs 3 nodes.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.Bn1(self.conv1(x)))\n",
    "        # Pass the input through the first convolutional layer, then Batch Normalization, and then apply ReLU activation.\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        # Apply Average Pooling to the output of the previous step.\n",
    "        x = F.relu(self.Bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Flatten the output from the previous step. This is necessary because fully connected layers expect a 1D input.\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output through the first fully connected layer.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Pass the output through the second fully connected layer with ReLU activation.\n",
    "        x = self.fc3(x)\n",
    "        # Pass the output through the third fully connected layer. This is the output of the network.\n",
    "        return x\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n",
    "        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous training step\n",
    "        output = model(data)  # Run forward pass (model predictions)\n",
    "\n",
    "        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n",
    "        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n",
    "        for data, target in test_loader:  # Loop over each batch from the testing set\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "            target = target.long()  # Convert target to long after adjusting value\n",
    "            output = model(data)  # Run forward pass (model predictions)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # Sum up the batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    return correct  # Return the number of correctly classified samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([64, 1, 3])\n",
      "conv1.bias \t torch.Size([64])\n",
      "Bn1.weight \t torch.Size([64])\n",
      "Bn1.bias \t torch.Size([64])\n",
      "Bn1.running_mean \t torch.Size([64])\n",
      "Bn1.running_var \t torch.Size([64])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 64, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "Bn2.weight \t torch.Size([64])\n",
      "Bn2.bias \t torch.Size([64])\n",
      "Bn2.running_mean \t torch.Size([64])\n",
      "Bn2.running_var \t torch.Size([64])\n",
      "Bn2.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([100, 3136])\n",
      "fc1.bias \t torch.Size([100])\n",
      "fc2.weight \t torch.Size([30, 100])\n",
      "fc2.bias \t torch.Size([30])\n",
      "fc3.weight \t torch.Size([3, 30])\n",
      "fc3.bias \t torch.Size([3])\n",
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.075337\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.645248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.701008\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.673608\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.664185\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.657935\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.696572\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.603941\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.613066\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.664815\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.602779\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.720407\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.674416\n",
      "\n",
      "Test set: Average loss: 0.8144, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.595437\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.643984\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.636484\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.714236\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.689526\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.631262\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.665805\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.635389\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.658085\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.640128\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.686294\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.683583\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.663664\n",
      "\n",
      "Test set: Average loss: 0.8453, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.728488\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.670600\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.590488\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.682524\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.646520\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.606689\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.777547\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.668293\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.733316\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.683632\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.608014\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.641012\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.556325\n",
      "\n",
      "Test set: Average loss: 0.7968, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.603097\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.653375\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.607107\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.669168\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.598166\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.605316\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.700281\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.633670\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.628085\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.662818\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.632403\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.669100\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.636212\n",
      "\n",
      "Test set: Average loss: 0.7852, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.617155\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.591556\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.686002\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.567076\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.611724\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.611296\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.655868\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.613729\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.633307\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.644682\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.647841\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.644433\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.651524\n",
      "\n",
      "Test set: Average loss: 0.7986, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.579310\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.701375\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.643404\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.656162\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.650817\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.606821\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.651896\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.651324\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.602857\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.570337\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.646377\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.605111\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.635350\n",
      "\n",
      "Test set: Average loss: 0.8108, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.629095\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.647606\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.667995\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.586654\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.570719\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.596227\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.681908\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.643404\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.609529\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.611347\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.610203\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.666146\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.663574\n",
      "\n",
      "Test set: Average loss: 0.8020, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.579551\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.718414\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.666644\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.654359\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.579595\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.589338\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.642320\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.643641\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.678573\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.610213\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.690347\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.671876\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.668183\n",
      "\n",
      "Test set: Average loss: 0.7956, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.529537\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.608030\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.620728\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.620891\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.665916\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.688641\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.688291\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.650030\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.609775\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.698794\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.697967\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.608939\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.631139\n",
      "\n",
      "Test set: Average loss: 0.8060, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.646639\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.667710\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.558799\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.676880\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.641344\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.614726\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.589804\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.695971\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.644993\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.638779\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.647788\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.598637\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.631801\n",
      "\n",
      "Test set: Average loss: 0.8158, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.550273\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.636930\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.703733\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.576323\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.575237\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.637550\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.576928\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.692638\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.613359\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.583955\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.569268\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.641934\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.580596\n",
      "\n",
      "Test set: Average loss: 0.8011, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.631945\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.610730\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.622886\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.657079\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.626008\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.683332\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.657995\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.694775\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.598385\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.741315\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.620136\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.542825\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.672621\n",
      "\n",
      "Test set: Average loss: 0.8051, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.608899\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.676856\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.710177\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.584675\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.688892\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.623287\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.710254\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.681404\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.569146\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.689454\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.658563\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.570957\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.689234\n",
      "\n",
      "Test set: Average loss: 0.8065, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.683444\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.668830\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.633166\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.594229\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.613697\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.667612\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.678721\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.665078\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.717227\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.608573\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.719601\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.566336\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.590202\n",
      "\n",
      "Test set: Average loss: 0.8025, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.611561\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.581948\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.662478\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.655563\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.653709\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.613872\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.629968\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.689901\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.632412\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.643991\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.624990\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.641446\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.653278\n",
      "\n",
      "Test set: Average loss: 0.8043, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.638764\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.621919\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.637867\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.589622\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.622582\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.618290\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.665236\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.609351\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.631865\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.567384\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.594157\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.666295\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.634707\n",
      "\n",
      "Test set: Average loss: 0.8055, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.644516\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.661338\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.700935\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.614938\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.662647\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.637510\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.656402\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.633853\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.620304\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.697604\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.622113\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.696017\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.677204\n",
      "\n",
      "Test set: Average loss: 0.8041, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.608347\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.642797\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.657436\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.645687\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.628536\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.598533\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.644707\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.622026\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.630557\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.652893\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.739149\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.580538\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.538283\n",
      "\n",
      "Test set: Average loss: 0.8045, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.680880\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.638621\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.598413\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.645466\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.633029\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.676213\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.655605\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.577664\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.596934\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.717586\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.710081\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.594311\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.666424\n",
      "\n",
      "Test set: Average loss: 0.8064, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.601969\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.620798\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.655737\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.656131\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.667198\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.675740\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.623438\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.660219\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.610024\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.586458\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.623618\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.667998\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.614394\n",
      "\n",
      "Test set: Average loss: 0.8055, Accuracy: 428/1019 (42%)\n",
      "\n",
      "428\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_resampled, y_train_resampled)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    ACC_ = test(model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(model.state_dict(), \"Baseline_CNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test set for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(['target', 'casename'], axis=1)\n",
    "y = processed_df['target'] \n",
    "\n",
    "# Handle imbalanced classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train['facts'].astype('U') + ' ' + X_train['issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train = X_train.drop(['facts', 'issues'], axis=1)\n",
    "X_train = pd.concat([X_train.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['facts'].astype('U') + ' ' + X_test['issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['facts', 'issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Param Tuning (Grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.83\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Optionally, use the best estimator to make predictions\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8522695235410206\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Favourable       0.74      0.86      0.80      1195\n",
      "  No outcome       0.98      0.98      0.98      1165\n",
      "Unfavourable       0.85      0.72      0.78      1187\n",
      "\n",
      "    accuracy                           0.85      3547\n",
      "   macro avg       0.86      0.85      0.85      3547\n",
      "weighted avg       0.86      0.85      0.85      3547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the training sets\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluating the Model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
