{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips from prof\n",
    "\n",
    "- Narrow scope of work (e.g. court level)\n",
    "\n",
    "- Could try both binary/multi-class model outcomes and compare the performance \n",
    "\n",
    "- Change user from layperson to legal professional (and mention that this project is a stepping stone towards having layperson use the model)\n",
    "\n",
    "- Link features to predicted outcome (if time permits can try using XGBoost with LIME for model interpretability)\n",
    "\n",
    "- Can also try to see accuracy of models with different areas of law, lowest accuracy may be hardest area of law to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "# Ignore the DeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename                                        area_of_law  \\\n",
      "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
      "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
      "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
      "3  2000_SGCA_12.pdf  {'courts and jurisdiction': ['court of appeal'...   \n",
      "4  2000_SGCA_13.pdf                     {'criminal law': ['offences']}   \n",
      "\n",
      "  court_level                                             issues  \\\n",
      "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
      "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
      "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
      "3        SGCA  the appeals from the assistant registrar. In h...   \n",
      "4        SGCA  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...            12   \n",
      "1  facts and surrounding circumstances including ...             8   \n",
      "2  Background \\nThe first appellants, a French co...             0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...            27   \n",
      "4  facts. Mere assertion would not suffice. In ex...            28   \n",
      "\n",
      "   facts_topic        target  \n",
      "0            7    Favourable  \n",
      "1            3    Favourable  \n",
      "2           12    No outcome  \n",
      "3           10  Unfavourable  \n",
      "4           13  Unfavourable  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV files into DataFrames\n",
    "areas_of_law_df = pd.read_csv(\"data/prediction_data/areas_of_law.csv\")\n",
    "coram_df = pd.read_csv(\"data/prediction_data/coram.csv\")\n",
    "sg_legal_cases_df = pd.read_csv(\"data/prediction_data/sg_legal_cases_dataset.csv\")\n",
    "target_rulings_df = pd.read_csv(\"data/prediction_data/target_rulings.csv\")\n",
    "issues_facts_df = pd.read_csv(\"data/prediction_data/issues_facts_topic.csv\")\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/issues.json') as f:\n",
    "    issues_data = [json.loads(line) for line in f]\n",
    "issues_df = pd.DataFrame(issues_data)\n",
    "\n",
    "# Load the JSON file into a dictionary\n",
    "with open('data/prediction_data/updated_facts.json') as f:\n",
    "    facts_data = [json.loads(line) for line in f]\n",
    "raw_facts_df = pd.DataFrame(facts_data)\n",
    "raw_facts_df[\"casename\"] = raw_facts_df[\"casename\"].apply(lambda case: case + \".pdf\" if case[-4:] != \".pdf\" else case)\n",
    "raw_facts_df[\"facts\"] = raw_facts_df[\"facts\"].fillna(\"\") + raw_facts_df[\"fact\"].fillna(\"\")\n",
    "raw_facts_df = raw_facts_df.drop(columns=[\"fact\"])\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(areas_of_law_df, sg_legal_cases_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, raw_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, issues_facts_df, on='casename', how='inner')\n",
    "merged_df = pd.merge(merged_df, target_rulings_df, on='casename', how='inner')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# Display the resulting DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "casename        0\n",
       "area_of_law     0\n",
       "court_level     0\n",
       "issues          0\n",
       "facts           0\n",
       "issues_topic    0\n",
       "facts_topic     0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate coram names and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coram_names(coram_list):\n",
    "    all_names = set()\n",
    "    for item in coram_list:\n",
    "        split_names = re.split(r';\\s(?![a-zA-Z]+\\s)', item)\n",
    "        for name in split_names:\n",
    "            if ';' in name and not re.search(r';\\s[a-zA-Z]+$', name):\n",
    "                sub_names = name.split(';')\n",
    "                all_names.update([n.strip() for n in sub_names if n.strip()])\n",
    "            else:\n",
    "                all_names.add(name.strip())\n",
    "    return list(all_names)\n",
    "\n",
    "def remove_coram_roles(coram_list):\n",
    "    roles = [' CJ', ' AG', ' J', ' DCJ', ' JA', ' AR', ' JC', 'SAR']\n",
    "    for role in roles:\n",
    "        coram_list = [re.sub(rf'{role}$', '', name) for name in coram_list]\n",
    "    return coram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coram_df = coram_df.dropna()\n",
    "for i, coram_str in enumerate(coram_df['Coram']):\n",
    "    coram = ast.literal_eval(coram_str)\n",
    "    \n",
    "    coram_modified = clean_coram_names(coram)\n",
    "    coram_modified = remove_coram_roles(coram_modified)\n",
    "    coram_df.at[i, 'Coram'] = str(coram_modified)\n",
    "merged_df = pd.merge(merged_df, coram_df, on='casename', how='outer')\n",
    "\n",
    "try:\n",
    "    merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casename         7\n",
      "area_of_law     54\n",
      "court_level     54\n",
      "issues          54\n",
      "facts           54\n",
      "issues_topic    54\n",
      "facts_topic     54\n",
      "target          54\n",
      "Coram            7\n",
      "dtype: int64"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               casename area_of_law court_level issues facts  issues_topic  \\\n",
      "241   2000_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "274   2000_SGHC_290.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "412    2001_SGCA_66.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "432   2001_SGHC_101.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "438   2001_SGHC_108.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "442   2001_SGHC_111.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "448   2001_SGHC_118.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "457   2001_SGHC_128.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "460   2001_SGHC_130.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "462   2001_SGHC_132.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "475   2001_SGHC_148.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "478   2001_SGHC_150.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "479   2001_SGHC_151.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "489   2001_SGHC_163.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "498   2001_SGHC_174.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "536   2001_SGHC_214.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "537   2001_SGHC_215.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "544   2001_SGHC_222.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "546   2001_SGHC_224.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "550   2001_SGHC_228.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "551   2001_SGHC_229.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "555   2001_SGHC_232.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "564   2001_SGHC_240.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "568   2001_SGHC_244.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "574   2001_SGHC_250.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "578   2001_SGHC_254.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "581   2001_SGHC_257.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "591   2001_SGHC_266.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "592   2001_SGHC_267.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "596   2001_SGHC_270.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "603   2001_SGHC_277.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "615   2001_SGHC_289.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "619   2001_SGHC_292.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "622   2001_SGHC_295.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "630   2001_SGHC_301.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "633   2001_SGHC_304.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "647   2001_SGHC_319.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "651   2001_SGHC_322.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "658   2001_SGHC_329.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "667   2001_SGHC_337.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "675   2001_SGHC_344.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "678   2001_SGHC_347.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "700   2001_SGHC_367.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "701   2001_SGHC_368.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "705   2001_SGHC_371.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "707   2001_SGHC_373.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "713   2001_SGHC_379.pdf         NaN         NaN    NaN   NaN           NaN   \n",
      "8567                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8568                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8569                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8570                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8571                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8572                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "8573                NaN         NaN         NaN    NaN   NaN           NaN   \n",
      "\n",
      "      facts_topic target                                              Coram  \n",
      "241           NaN    NaN                                ['Sundaresh Menon']  \n",
      "274           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Tan Lee Me...  \n",
      "412           NaN    NaN                                 ['Ang Cheng Hock']  \n",
      "432           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Choo Han T...  \n",
      "438           NaN    NaN                                ['Pang Khang Chau']  \n",
      "442           NaN    NaN                                    ['Edmund Leow']  \n",
      "448           NaN    NaN                                 ['Amarjeet Singh']  \n",
      "457           NaN    NaN                                    ['Quentin Loh']  \n",
      "460           NaN    NaN                     ['Chao Hick Tin', 'V K Rajah']  \n",
      "462           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "475           NaN    NaN                            ['Vinodh Coomaraswamy']  \n",
      "478           NaN    NaN                                  ['Kannan Ramesh']  \n",
      "479           NaN    NaN  ['Sundaresh Menon', 'Steven Chong', 'Judith Pr...  \n",
      "489           NaN    NaN                                     ['Woo Bih Li']  \n",
      "498           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "536           NaN    NaN                                    ['S Rajendran']  \n",
      "537           NaN    NaN                                      ['MPH Rubin']  \n",
      "544           NaN    NaN  ['Sundaresh Menon', 'Judith Prakash', 'Tay Yon...  \n",
      "546           NaN    NaN                                 ['Tan Siong Thye']  \n",
      "550           NaN    NaN                                 ['Hoo Sheau Peng']  \n",
      "551           NaN    NaN                                   ['Lai Kew Chai']  \n",
      "555           NaN    NaN                                  ['Choo Han Teck']  \n",
      "564           NaN    NaN                                     ['George Wei']  \n",
      "568           NaN    NaN                                  ['Chan Seng Onn']  \n",
      "574           NaN    NaN                                 ['Tay Yong Kwang']  \n",
      "578           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "581           NaN    NaN  ['Chao Hick Tin', 'Yong Pung How', 'Tan Lee Me...  \n",
      "591           NaN    NaN                                 ['Judith Prakash']  \n",
      "592           NaN    NaN                                   ['Lee Seiu Kin']  \n",
      "596           NaN    NaN                                   ['Lee Kim Shin']  \n",
      "603           NaN    NaN                                  ['Kan Ting Chiu']  \n",
      "615           NaN    NaN                                      ['MPH Rubin']  \n",
      "619           NaN    NaN                                 ['Kwek Mean Luck']  \n",
      "622           NaN    NaN                                     ['Woo Bih Li']  \n",
      "630           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "633           NaN    NaN                                  ['Choo Han Teck']  \n",
      "647           NaN    NaN                                  ['Valerie Thean']  \n",
      "651           NaN    NaN                                 ['Judith Prakash']  \n",
      "658           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "667           NaN    NaN                                     ['Woo Bih Li']  \n",
      "675           NaN    NaN                                  ['Yong Pung How']  \n",
      "678           NaN    NaN                            ['Belinda Ang Saw Ean']  \n",
      "700           NaN    NaN                                     ['Woo Bih Li']  \n",
      "701           NaN    NaN                                  ['Valerie Thean']  \n",
      "705           NaN    NaN                                   ['Lai Siu Chiu']  \n",
      "707           NaN    NaN                                  ['Yong Pung How']  \n",
      "713           NaN    NaN                               ['Dedar Singh Gill']  \n",
      "8567          NaN    NaN  ['Sundaresh Menon', 'Steven Chong', 'Andrew Ph...  \n",
      "8568          NaN    NaN                                    ['Lim Jian Yi']  \n",
      "8569          NaN    NaN  ['Judith Prakash', 'Andrew Phang Boon Leong', ...  \n",
      "8570          NaN    NaN                     ['Chao Hick Tin', 'L P Thean']  \n",
      "8571          NaN    NaN                                 ['Judith Prakash']  \n",
      "8572          NaN    NaN     ['Steven Chong', 'Kan Ting Chiu', 'V K Rajah']  \n",
      "8573          NaN    NaN                                  ['Valerie Thean']  \n",
      "casename        0\n",
      "area_of_law     0\n",
      "court_level     0\n",
      "issues          0\n",
      "facts           0\n",
      "issues_topic    0\n",
      "facts_topic     0\n",
      "target          0\n",
      "Coram           0\n",
      "dtype: int64\n",
      "target\n",
      "Favourable      3941\n",
      "Unfavourable    2056\n",
      "No outcome       794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = merged_df.isna().sum()\n",
    "print(nan_counts)\n",
    "\n",
    "#nas are probably those reassigned cases, coram has 7, i just drop them for now\n",
    "na_target_rows = merged_df[merged_df['target'].isna()]\n",
    "print(na_target_rows)\n",
    "\n",
    "merged_df.dropna(axis=0, inplace=True)\n",
    "print(merged_df.isna().sum())\n",
    "\n",
    "#remove empty lists\n",
    "merged_df = merged_df.query(\"area_of_law != '[]'\")\n",
    "\n",
    "#target is unbalanced\n",
    "target_counts = merged_df['target'].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True) # prevent nan values from appearing after one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casename</th>\n",
       "      <th>area_of_law</th>\n",
       "      <th>court_level</th>\n",
       "      <th>issues</th>\n",
       "      <th>facts</th>\n",
       "      <th>issues_topic</th>\n",
       "      <th>facts_topic</th>\n",
       "      <th>target</th>\n",
       "      <th>Coram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000_SGCA_1.pdf</td>\n",
       "      <td>{'civil procedure': ['pleadings'], 'res judica...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The claim was dismissed with costs by the\\nHig...</td>\n",
       "      <td>The facts\\nThe appellant is the widow of one T...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[Chan Sek Keong, Andrew Phang Boon Leong, V K ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_SGCA_10.pdf</td>\n",
       "      <td>{'contract': ['formation'], 'equity': ['defenc...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>the claim and\\nagainst that decision this appe...</td>\n",
       "      <td>facts and surrounding circumstances including ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Favourable</td>\n",
       "      <td>[Chao Hick Tin, Andrew Phang Boon Leong, V K R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000_SGCA_11.pdf</td>\n",
       "      <td>{'contract': ['discharge'], 'damages': ['asses...</td>\n",
       "      <td>SGCA</td>\n",
       "      <td>The appeal \\nThe questions which arise in this...</td>\n",
       "      <td>Background \\nThe first appellants, a French co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>No outcome</td>\n",
       "      <td>[Chan Sek Keong, Andrew Phang Boon Leong, Tan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           casename                                        area_of_law  \\\n",
       "0   2000_SGCA_1.pdf  {'civil procedure': ['pleadings'], 'res judica...   \n",
       "1  2000_SGCA_10.pdf  {'contract': ['formation'], 'equity': ['defenc...   \n",
       "2  2000_SGCA_11.pdf  {'contract': ['discharge'], 'damages': ['asses...   \n",
       "\n",
       "  court_level                                             issues  \\\n",
       "0        SGCA  The claim was dismissed with costs by the\\nHig...   \n",
       "1        SGCA  the claim and\\nagainst that decision this appe...   \n",
       "2        SGCA  The appeal \\nThe questions which arise in this...   \n",
       "\n",
       "                                               facts  issues_topic  \\\n",
       "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
       "1  facts and surrounding circumstances including ...           8.0   \n",
       "2  Background \\nThe first appellants, a French co...           0.0   \n",
       "\n",
       "   facts_topic      target                                              Coram  \n",
       "0          7.0  Favourable  [Chan Sek Keong, Andrew Phang Boon Leong, V K ...  \n",
       "1          3.0  Favourable  [Chao Hick Tin, Andrew Phang Boon Leong, V K R...  \n",
       "2         12.0  No outcome  [Chan Sek Keong, Andrew Phang Boon Leong, Tan ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['area_of_law'] = merged_df['area_of_law'].apply(ast.literal_eval)\n",
    "merged_df['Coram'] = merged_df['Coram'].apply(ast.literal_eval)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten areas_of_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_areas = []\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "\n",
    "    areas = row['area_of_law']\n",
    "    flat_areas = []\n",
    "    for main_area, sub_areas in areas.items():\n",
    "        flat_areas.append(main_area)\n",
    "        for sarea in sub_areas.copy():\n",
    "            if len(sarea) > 33:\n",
    "                sub_areas.remove(sarea)\n",
    "        flat_areas.extend(sub_areas)\n",
    "    all_areas.append(flat_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in all_areas:\n",
    "    if area == []:\n",
    "        print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "\n",
      "   facts_topic      target                                              Coram  \\\n",
      "0          7.0  Favourable  [Chan Sek Keong, Andrew Phang Boon Leong, V K ...   \n",
      "1          3.0  Favourable  [Chao Hick Tin, Andrew Phang Boon Leong, V K R...   \n",
      "2         12.0  No outcome  [Chan Sek Keong, Andrew Phang Boon Leong, Tan ...   \n",
      "\n",
      "   \"a larger sum being repaid\"  \"abet\"  ...  work injury compensation act  \\\n",
      "0                            0       0  ...                             0   \n",
      "1                            0       0  ...                             0   \n",
      "2                            0       0  ...                             0   \n",
      "\n",
      "   workmen’s compensation act  writ of  seizure and sale  writ of summons  \\\n",
      "0                           0                          0                0   \n",
      "1                           0                          0                0   \n",
      "2                           0                          0                0   \n",
      "\n",
      "   wrongful dismissal  young offenders]  “any claim  hereunder”  \\\n",
      "0                   0                 0                       0   \n",
      "1                   0                 0                       0   \n",
      "2                   0                 0                       0   \n",
      "\n",
      "   “any fire accidentally begin”  “charity proceedings”  \\\n",
      "0                              0                      0   \n",
      "1                              0                      0   \n",
      "2                              0                      0   \n",
      "\n",
      "   “rash” and “negligent”  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "\n",
      "[3 rows x 1377 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode aol\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(all_areas)\n",
    "\n",
    "binary_aol_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_aol_df = binary_aol_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([merged_df.drop('area_of_law', axis=1), binary_aol_df], axis=1)\n",
    "processed_df = processed_df[processed_df['Coram'].apply(lambda x: isinstance(x, list))]\n",
    "print(processed_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           casename court_level  \\\n",
      "0   2000_SGCA_1.pdf        SGCA   \n",
      "1  2000_SGCA_10.pdf        SGCA   \n",
      "2  2000_SGCA_11.pdf        SGCA   \n",
      "3  2000_SGCA_12.pdf        SGCA   \n",
      "4  2000_SGCA_13.pdf        SGCA   \n",
      "\n",
      "                                              issues  \\\n",
      "0  The claim was dismissed with costs by the\\nHig...   \n",
      "1  the claim and\\nagainst that decision this appe...   \n",
      "2  The appeal \\nThe questions which arise in this...   \n",
      "3  the appeals from the assistant registrar. In h...   \n",
      "4  the appeal on 24 January 2000 and dismissed it...   \n",
      "\n",
      "                                               facts  issues_topic  \\\n",
      "0  The facts\\nThe appellant is the widow of one T...          12.0   \n",
      "1  facts and surrounding circumstances including ...           8.0   \n",
      "2  Background \\nThe first appellants, a French co...           0.0   \n",
      "3  Background\\nMicrosoft, Adobe and Autodesk are ...          27.0   \n",
      "4  facts. Mere assertion would not suffice. In ex...          28.0   \n",
      "\n",
      "   facts_topic        target  \"a larger sum being repaid\"  \"abet\"  \\\n",
      "0          7.0    Favourable                            0       0   \n",
      "1          3.0    Favourable                            0       0   \n",
      "2         12.0    No outcome                            0       0   \n",
      "3         10.0  Unfavourable                            0       0   \n",
      "4         13.0  Unfavourable                            0       0   \n",
      "\n",
      "   \"an interest in any matter\"  ...  Vincent Hoong  Vincent Leow  \\\n",
      "0                            0  ...              0             0   \n",
      "1                            0  ...              0             0   \n",
      "2                            0  ...              0             0   \n",
      "3                            0  ...              0             0   \n",
      "4                            0  ...              0             0   \n",
      "\n",
      "   Vinodh Coomaraswamy  Wong Li Kok, Alex  Woo Bih Li  Yap Yew Choh Kenneth  \\\n",
      "0                    0                  0           0                     0   \n",
      "1                    0                  0           0                     0   \n",
      "2                    0                  0           0                     0   \n",
      "3                    0                  0           0                     0   \n",
      "4                    0                  0           0                     0   \n",
      "\n",
      "   Yeong Zee Kin  Yong Pung How  Yong Pung How,  Zhuo Wenzhao  \n",
      "0              0              0               0             0  \n",
      "1              0              0               0             0  \n",
      "2              0              0               0             0  \n",
      "3              0              0               0             0  \n",
      "4              0              0               0             0  \n",
      "\n",
      "[5 rows x 1487 columns]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode coram\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_features = mlb.fit_transform(processed_df['Coram'])\n",
    "\n",
    "binary_coram_df = pd.DataFrame(binary_features, columns=mlb.classes_)\n",
    "binary_coram_df = binary_coram_df.reset_index(drop=True)\n",
    "processed_df = pd.concat([processed_df.drop('Coram', axis=1), binary_coram_df], axis=1)\n",
    "\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['SGCA'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGCA' else 0)\n",
    "processed_df['SGHC'] = processed_df['court_level'].apply(lambda x: 1 if x == 'SGHC' else 0)\n",
    "processed_df = processed_df.drop('court_level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'\\W*\\b(?!no)\\w{1,2}\\b', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    legal_stopwords = ('appellant', 'respondent', 'plaintiff', 'defendant', 'mr', 'mrs', 'dr', 'mdm', 'court','version', 'hr', 'would', 'case', 'sghc', 'court', 'sgca', 'slr', 'sgdc', 'also', 'first', 'person', 'statement', 'line', 'para', 'fact', 'one', 'may', 'time', 'could', 'next', 'legal', 'issues', 'issue')\n",
    "    stop_words.update(legal_stopwords)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "processed_df['processed_facts'] = processed_df['facts'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['facts'], inplace=True)\n",
    "\n",
    "processed_df['processed_issues'] = processed_df['issues'].apply(preprocess_text)\n",
    "processed_df.drop(columns=['issues'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(columns=['target','casename'])\n",
    "y = processed_df['target']\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test_val = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test_val = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "#balanced dataset (target variable was imbalanced Favourable 5006 Unfavourable 2523 No outcome 984)\n",
    "#randomly found one online, can be changed -> need to check am i doing this right \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smt = SMOTE(random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "# X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "#split further from X_test_val into X_val and X_test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train_resampled\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_val\n",
    "X_val_text = tfidf_vectorizer.transform(X_val['processed_facts'].astype('U') + ' ' + X_val['processed_issues'].astype('U'))\n",
    "X_val_text = pd.DataFrame(X_val_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_val = X_val.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_val = pd.concat([X_val.reset_index(drop=True), X_val_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target variable to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_resampled = X_train_resampled.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_test = X_test.drop(columns=['processed_facts', 'processed_issues'])\n",
    "# X_val = X_val.drop(columns=['processed_facts', 'processed_issues'])\n",
    "\n",
    "mapping = {'Favourable': 1, 'Unfavourable': 0, 'No outcome':0.5}\n",
    "\n",
    "y_train_resampled, y_test, y_val = y_train_resampled.copy().map(mapping), y_test.copy().map(mapping), y_val.copy().map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  epochs = 20\n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: torch.Size([8274, 2484])\n",
      "Shape of X_test: torch.Size([1019, 2484])\n",
      "Shape of X_val: torch.Size([1019, 2484])\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled = X_train_resampled.iloc[:, :].copy()\n",
    "X_train_resampled = torch.tensor(X_train_resampled.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_train_resampled: {X_train_resampled.shape}')\n",
    "\n",
    "X_test = X_test.iloc[:, :].copy()\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "\n",
    "X_val = X_val.iloc[:,:].copy()\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "print(f'Shape of X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8274, 1, 2484])\n"
     ]
    }
   ],
   "source": [
    "y_train_resampled, y_test, y_val = torch.tensor(y_train_resampled.values).to(device), torch.tensor(y_test.values).to(device), torch.tensor(y_val.values).to(device)\n",
    "\n",
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0],1,X_train_resampled.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1])\n",
    "print(X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, 1,1, bias=True)\n",
    "        # Define the first 1D convolution layer. Takes 1 input channel, outputs 32 channels, kernel size is 3, stride is 1, padding is 1.\n",
    "        self.Bn1 = nn.BatchNorm1d(64)\n",
    "        # Apply Batch Normalization to the output of the first convolutional layer.\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        # Apply 1D Average Pooling after the first Batch Normalization. The kernel size and stride are 2.\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3, 1,1, bias=True)\n",
    "        self.Bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(39744, 100, bias=True)\n",
    "        # Define the first fully connected layer. It takes 25472 inputs and outputs 100 nodes.\n",
    "\n",
    "        self.fc2 = nn.Linear(100, 30, bias=True)\n",
    "        # Define the second fully connected layer. It takes 100 inputs and outputs 50 nodes.\n",
    "\n",
    "        self.fc3 = nn.Linear(30, 3, bias=True)\n",
    "        # Define the third fully connected layer (output layer). It takes 50 inputs and outputs 3 nodes.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.Bn1(self.conv1(x)))\n",
    "        # Pass the input through the first convolutional layer, then Batch Normalization, and then apply ReLU activation.\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        # Apply Average Pooling to the output of the previous step.\n",
    "        x = F.tanh(self.Bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Flatten the output from the previous step. This is necessary because fully connected layers expect a 1D input.\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output through the first fully connected layer.\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        # Pass the output through the second fully connected layer with tanh activation.\n",
    "        x = self.fc3(x)\n",
    "        # Pass the output through the third fully connected layer. This is the output of the network.\n",
    "        return x\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n",
    "        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous training step\n",
    "        output = model(data)  # Run forward pass (model predictions)\n",
    "        #print(output.shape)\n",
    "        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n",
    "        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n",
    "        for data, target in test_loader:  # Loop over each batch from the testing set\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n",
    "\n",
    "            target = target.long()  # Convert target to long after adjusting value\n",
    "            output = model(data)  # Run forward pass (model predictions)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # Sum up the batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    return correct  # Return the number of correctly classified samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([64, 1, 3])\n",
      "conv1.bias \t torch.Size([64])\n",
      "Bn1.weight \t torch.Size([64])\n",
      "Bn1.bias \t torch.Size([64])\n",
      "Bn1.running_mean \t torch.Size([64])\n",
      "Bn1.running_var \t torch.Size([64])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 64, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "Bn2.weight \t torch.Size([64])\n",
      "Bn2.bias \t torch.Size([64])\n",
      "Bn2.running_mean \t torch.Size([64])\n",
      "Bn2.running_var \t torch.Size([64])\n",
      "Bn2.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([100, 39744])\n",
      "fc1.bias \t torch.Size([100])\n",
      "fc2.weight \t torch.Size([30, 100])\n",
      "fc2.bias \t torch.Size([30])\n",
      "fc3.weight \t torch.Size([3, 30])\n",
      "fc3.bias \t torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_resampled, y_train_resampled)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.084656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.641188\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.591416\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.643810\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.648608\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.594248\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.588982\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.647122\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.600026\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.696396\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.611304\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.630699\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.623725\n",
      "\n",
      "Test set: Average loss: 0.9051, Accuracy: 428/1019 (42%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.712027\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.725785\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.652489\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.610812\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.617150\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.624064\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.572611\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.659281\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.583223\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.570359\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.675328\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.570555\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.567189\n",
      "\n",
      "Test set: Average loss: 0.7460, Accuracy: 544/1019 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.564508\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.642657\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.568498\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.733271\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.600488\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.638341\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.572715\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.641913\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.544596\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.508316\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.725656\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.579252\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.518097\n",
      "\n",
      "Test set: Average loss: 1.0278, Accuracy: 477/1019 (47%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.727490\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.572247\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.575319\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.573192\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.548318\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.573743\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.619060\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.578631\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.497941\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.582303\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.494592\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.498945\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.575932\n",
      "\n",
      "Test set: Average loss: 0.8147, Accuracy: 520/1019 (51%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.467109\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.579721\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.537045\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.563033\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.620962\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.588345\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.590521\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.578464\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.659664\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.607786\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.451941\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.565625\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.437152\n",
      "\n",
      "Test set: Average loss: 0.7640, Accuracy: 535/1019 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.500596\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.594632\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.483512\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.420465\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.450056\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.446720\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.544801\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.423273\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.548081\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.474022\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.418676\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.429385\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.480849\n",
      "\n",
      "Test set: Average loss: 0.7964, Accuracy: 517/1019 (51%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.463011\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.446459\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.449130\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.489865\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.549162\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.412774\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.507109\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.526525\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.396593\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.424595\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.374047\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.448012\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.457938\n",
      "\n",
      "Test set: Average loss: 0.7679, Accuracy: 537/1019 (53%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.446200\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.488831\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.567409\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.448386\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.450644\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.611421\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.378430\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.525146\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.510848\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.347348\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.483519\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.408986\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.530543\n",
      "\n",
      "Test set: Average loss: 0.7852, Accuracy: 538/1019 (53%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.429874\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.457721\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.486054\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.372105\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.348862\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.451356\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.494174\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.438657\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.356432\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.446819\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.504082\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.470341\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.369150\n",
      "\n",
      "Test set: Average loss: 0.7739, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.388834\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.512159\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.401285\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.417928\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.350085\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.392900\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.306367\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.552016\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.379140\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.514685\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.494533\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.347514\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.434875\n",
      "\n",
      "Test set: Average loss: 0.7716, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.398062\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.383847\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.423709\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.408496\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.412067\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.426009\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.399185\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.485526\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.395900\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.374776\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.455206\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.382344\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.379085\n",
      "\n",
      "Test set: Average loss: 0.7666, Accuracy: 549/1019 (54%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.364516\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.391571\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.495981\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.353837\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.401216\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.358359\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.410041\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.451074\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.435499\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.427203\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.527816\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.422099\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.390363\n",
      "\n",
      "Test set: Average loss: 0.7798, Accuracy: 539/1019 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.494977\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.400466\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.407650\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.389475\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.412201\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.387770\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.443030\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.347732\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.391940\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.398997\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.376920\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.442437\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.448887\n",
      "\n",
      "Test set: Average loss: 0.7726, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.337010\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.442036\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.336297\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.378741\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.425555\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.446117\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.404070\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.341995\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.366796\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.382240\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.382358\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.346896\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.408008\n",
      "\n",
      "Test set: Average loss: 0.7790, Accuracy: 540/1019 (53%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.406333\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.441763\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.427842\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.385472\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.343867\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.421583\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.384552\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.458831\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.448526\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.343609\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.450086\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.470879\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.318286\n",
      "\n",
      "Test set: Average loss: 0.7760, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.354991\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.375865\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.367690\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.503979\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.406123\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.459070\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.356672\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.448921\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.457701\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.424857\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.432226\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.384481\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.339982\n",
      "\n",
      "Test set: Average loss: 0.7776, Accuracy: 540/1019 (53%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.401302\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.418803\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.358131\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.430994\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.446384\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.350859\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.418544\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.376029\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.409591\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.316157\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.395564\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.398205\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.381823\n",
      "\n",
      "Test set: Average loss: 0.7768, Accuracy: 542/1019 (53%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.395010\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.368889\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.446173\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.504237\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.379787\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.447071\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.416352\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.380893\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.382506\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.378615\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.347028\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.412636\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.445053\n",
      "\n",
      "Test set: Average loss: 0.7786, Accuracy: 540/1019 (53%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.337236\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.458104\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.380872\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.434158\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.418253\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.401963\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.413454\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.359626\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.392402\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.305424\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.487942\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.428637\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.496565\n",
      "\n",
      "Test set: Average loss: 0.7759, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.309100\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.406965\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.320436\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.341121\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.390192\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.452787\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.348346\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.496083\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.395432\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.493120\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.408794\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.468456\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.409161\n",
      "\n",
      "Test set: Average loss: 0.7771, Accuracy: 539/1019 (53%)\n",
      "\n",
      "549\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "CNN_acc = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    ACC_ = test(model, device, test_loader)\n",
    "    if ACC_>CNN_acc or ACC_ == CNN_acc:\n",
    "        CNN_acc = ACC_\n",
    "        torch.save(model.state_dict(), \"Baseline_CNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(CNN_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 54.85770363101079\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "val_loss = 0\n",
    "CNN_test_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = model(data)\n",
    "        #pred = torch.argmax(output_test, 1)\n",
    "        pred = output_test.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n",
    "\n",
    "        \n",
    "        val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        #correct_val += (pred == target).sum().item()\n",
    "        correct_val += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "\n",
    "        \n",
    "        total_val += target.size(0)\n",
    "    \n",
    "    CNN_test_accuracy = (correct_val / total_val) * 100\n",
    "            \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {CNN_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(2484, 128, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(128)\n",
    "        # self.fc2 = nn.Linear(128, 128, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(128)\n",
    "        # self.fc3 = nn.Linear(128, 5, bias=True)\n",
    "\n",
    "        # self.fc1 = nn.Linear(2484, 1024, bias=True)\n",
    "        # self.Bn1 = nn.BatchNorm1d(1024)\n",
    "        # self.fc2 = nn.Linear(1024, 512, bias=True)\n",
    "        # self.Bn2 = nn.BatchNorm1d(512)\n",
    "        # self.fc3 = nn.Linear(512, 256, bias=True)\n",
    "        # self.Bn3 = nn.BatchNorm1d(256)\n",
    "        # self.fc4 = nn.Linear(256, 128, bias=True)\n",
    "        # self.Bn4 = nn.BatchNorm1d(128)\n",
    "        # self.fc5 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(2484, 512, bias=True)\n",
    "        self.Bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(512, 128, bias=True)\n",
    "        self.fc3 = nn.Linear(128, 3, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.Bn1(self.fc1(x)))\n",
    "        # x = F.tanh(self.Bn2(self.fc2(x)))\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = F.leaky_relu(self.fc1(x))\n",
    "        # x = F.leaky_relu(self.fc2(x))\n",
    "        # x = F.leaky_relu(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc4(x))\n",
    "        # x = self.fc5(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.tanh(self.fc1(x)) # [leaky_relu, tanh, relu,]\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight \t torch.Size([512, 2484])\n",
      "fc1.bias \t torch.Size([512])\n",
      "Bn1.weight \t torch.Size([256])\n",
      "Bn1.bias \t torch.Size([256])\n",
      "Bn1.running_mean \t torch.Size([256])\n",
      "Bn1.running_var \t torch.Size([256])\n",
      "Bn1.num_batches_tracked \t torch.Size([])\n",
      "fc2.weight \t torch.Size([128, 512])\n",
      "fc2.bias \t torch.Size([128])\n",
      "fc3.weight \t torch.Size([3, 128])\n",
      "fc3.bias \t torch.Size([3])\n",
      "Train Epoch: 1 [0/8274 (0%)]\tLoss: 1.144096\n",
      "Train Epoch: 1 [640/8274 (8%)]\tLoss: 0.591128\n",
      "Train Epoch: 1 [1280/8274 (15%)]\tLoss: 0.659717\n",
      "Train Epoch: 1 [1920/8274 (23%)]\tLoss: 0.637089\n",
      "Train Epoch: 1 [2560/8274 (31%)]\tLoss: 0.598890\n",
      "Train Epoch: 1 [3200/8274 (38%)]\tLoss: 0.621578\n",
      "Train Epoch: 1 [3840/8274 (46%)]\tLoss: 0.602656\n",
      "Train Epoch: 1 [4480/8274 (54%)]\tLoss: 0.615508\n",
      "Train Epoch: 1 [5120/8274 (62%)]\tLoss: 0.686932\n",
      "Train Epoch: 1 [5760/8274 (69%)]\tLoss: 0.652801\n",
      "Train Epoch: 1 [6400/8274 (77%)]\tLoss: 0.601091\n",
      "Train Epoch: 1 [7040/8274 (85%)]\tLoss: 0.513449\n",
      "Train Epoch: 1 [7680/8274 (92%)]\tLoss: 0.574737\n",
      "\n",
      "Test set: Average loss: 0.9043, Accuracy: 466/1019 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/8274 (0%)]\tLoss: 0.625196\n",
      "Train Epoch: 2 [640/8274 (8%)]\tLoss: 0.540608\n",
      "Train Epoch: 2 [1280/8274 (15%)]\tLoss: 0.514788\n",
      "Train Epoch: 2 [1920/8274 (23%)]\tLoss: 0.464694\n",
      "Train Epoch: 2 [2560/8274 (31%)]\tLoss: 0.527348\n",
      "Train Epoch: 2 [3200/8274 (38%)]\tLoss: 0.571075\n",
      "Train Epoch: 2 [3840/8274 (46%)]\tLoss: 0.608517\n",
      "Train Epoch: 2 [4480/8274 (54%)]\tLoss: 0.416023\n",
      "Train Epoch: 2 [5120/8274 (62%)]\tLoss: 0.598106\n",
      "Train Epoch: 2 [5760/8274 (69%)]\tLoss: 0.533073\n",
      "Train Epoch: 2 [6400/8274 (77%)]\tLoss: 0.480460\n",
      "Train Epoch: 2 [7040/8274 (85%)]\tLoss: 0.562345\n",
      "Train Epoch: 2 [7680/8274 (92%)]\tLoss: 0.515361\n",
      "\n",
      "Test set: Average loss: 0.7409, Accuracy: 537/1019 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/8274 (0%)]\tLoss: 0.510362\n",
      "Train Epoch: 3 [640/8274 (8%)]\tLoss: 0.429560\n",
      "Train Epoch: 3 [1280/8274 (15%)]\tLoss: 0.413808\n",
      "Train Epoch: 3 [1920/8274 (23%)]\tLoss: 0.464106\n",
      "Train Epoch: 3 [2560/8274 (31%)]\tLoss: 0.467952\n",
      "Train Epoch: 3 [3200/8274 (38%)]\tLoss: 0.524088\n",
      "Train Epoch: 3 [3840/8274 (46%)]\tLoss: 0.457678\n",
      "Train Epoch: 3 [4480/8274 (54%)]\tLoss: 0.678763\n",
      "Train Epoch: 3 [5120/8274 (62%)]\tLoss: 0.428776\n",
      "Train Epoch: 3 [5760/8274 (69%)]\tLoss: 0.568854\n",
      "Train Epoch: 3 [6400/8274 (77%)]\tLoss: 0.586704\n",
      "Train Epoch: 3 [7040/8274 (85%)]\tLoss: 0.454760\n",
      "Train Epoch: 3 [7680/8274 (92%)]\tLoss: 0.559691\n",
      "\n",
      "Test set: Average loss: 0.8130, Accuracy: 525/1019 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/8274 (0%)]\tLoss: 0.446254\n",
      "Train Epoch: 4 [640/8274 (8%)]\tLoss: 0.341359\n",
      "Train Epoch: 4 [1280/8274 (15%)]\tLoss: 0.535521\n",
      "Train Epoch: 4 [1920/8274 (23%)]\tLoss: 0.413134\n",
      "Train Epoch: 4 [2560/8274 (31%)]\tLoss: 0.563724\n",
      "Train Epoch: 4 [3200/8274 (38%)]\tLoss: 0.351350\n",
      "Train Epoch: 4 [3840/8274 (46%)]\tLoss: 0.332520\n",
      "Train Epoch: 4 [4480/8274 (54%)]\tLoss: 0.490716\n",
      "Train Epoch: 4 [5120/8274 (62%)]\tLoss: 0.442139\n",
      "Train Epoch: 4 [5760/8274 (69%)]\tLoss: 0.368996\n",
      "Train Epoch: 4 [6400/8274 (77%)]\tLoss: 0.354835\n",
      "Train Epoch: 4 [7040/8274 (85%)]\tLoss: 0.497785\n",
      "Train Epoch: 4 [7680/8274 (92%)]\tLoss: 0.478774\n",
      "\n",
      "Test set: Average loss: 0.9427, Accuracy: 536/1019 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/8274 (0%)]\tLoss: 0.300998\n",
      "Train Epoch: 5 [640/8274 (8%)]\tLoss: 0.391380\n",
      "Train Epoch: 5 [1280/8274 (15%)]\tLoss: 0.389028\n",
      "Train Epoch: 5 [1920/8274 (23%)]\tLoss: 0.573730\n",
      "Train Epoch: 5 [2560/8274 (31%)]\tLoss: 0.325227\n",
      "Train Epoch: 5 [3200/8274 (38%)]\tLoss: 0.414783\n",
      "Train Epoch: 5 [3840/8274 (46%)]\tLoss: 0.413460\n",
      "Train Epoch: 5 [4480/8274 (54%)]\tLoss: 0.498807\n",
      "Train Epoch: 5 [5120/8274 (62%)]\tLoss: 0.369913\n",
      "Train Epoch: 5 [5760/8274 (69%)]\tLoss: 0.272909\n",
      "Train Epoch: 5 [6400/8274 (77%)]\tLoss: 0.421780\n",
      "Train Epoch: 5 [7040/8274 (85%)]\tLoss: 0.553532\n",
      "Train Epoch: 5 [7680/8274 (92%)]\tLoss: 0.407320\n",
      "\n",
      "Test set: Average loss: 0.9253, Accuracy: 545/1019 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/8274 (0%)]\tLoss: 0.337476\n",
      "Train Epoch: 6 [640/8274 (8%)]\tLoss: 0.386359\n",
      "Train Epoch: 6 [1280/8274 (15%)]\tLoss: 0.434817\n",
      "Train Epoch: 6 [1920/8274 (23%)]\tLoss: 0.340883\n",
      "Train Epoch: 6 [2560/8274 (31%)]\tLoss: 0.286864\n",
      "Train Epoch: 6 [3200/8274 (38%)]\tLoss: 0.352078\n",
      "Train Epoch: 6 [3840/8274 (46%)]\tLoss: 0.324878\n",
      "Train Epoch: 6 [4480/8274 (54%)]\tLoss: 0.413646\n",
      "Train Epoch: 6 [5120/8274 (62%)]\tLoss: 0.322184\n",
      "Train Epoch: 6 [5760/8274 (69%)]\tLoss: 0.344062\n",
      "Train Epoch: 6 [6400/8274 (77%)]\tLoss: 0.403440\n",
      "Train Epoch: 6 [7040/8274 (85%)]\tLoss: 0.348992\n",
      "Train Epoch: 6 [7680/8274 (92%)]\tLoss: 0.360235\n",
      "\n",
      "Test set: Average loss: 1.0666, Accuracy: 554/1019 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/8274 (0%)]\tLoss: 0.269191\n",
      "Train Epoch: 7 [640/8274 (8%)]\tLoss: 0.318814\n",
      "Train Epoch: 7 [1280/8274 (15%)]\tLoss: 0.246343\n",
      "Train Epoch: 7 [1920/8274 (23%)]\tLoss: 0.193961\n",
      "Train Epoch: 7 [2560/8274 (31%)]\tLoss: 0.253149\n",
      "Train Epoch: 7 [3200/8274 (38%)]\tLoss: 0.272330\n",
      "Train Epoch: 7 [3840/8274 (46%)]\tLoss: 0.302685\n",
      "Train Epoch: 7 [4480/8274 (54%)]\tLoss: 0.408747\n",
      "Train Epoch: 7 [5120/8274 (62%)]\tLoss: 0.300993\n",
      "Train Epoch: 7 [5760/8274 (69%)]\tLoss: 0.355405\n",
      "Train Epoch: 7 [6400/8274 (77%)]\tLoss: 0.172651\n",
      "Train Epoch: 7 [7040/8274 (85%)]\tLoss: 0.264682\n",
      "Train Epoch: 7 [7680/8274 (92%)]\tLoss: 0.349531\n",
      "\n",
      "Test set: Average loss: 1.1016, Accuracy: 550/1019 (54%)\n",
      "\n",
      "Train Epoch: 8 [0/8274 (0%)]\tLoss: 0.257087\n",
      "Train Epoch: 8 [640/8274 (8%)]\tLoss: 0.270247\n",
      "Train Epoch: 8 [1280/8274 (15%)]\tLoss: 0.278058\n",
      "Train Epoch: 8 [1920/8274 (23%)]\tLoss: 0.208732\n",
      "Train Epoch: 8 [2560/8274 (31%)]\tLoss: 0.230168\n",
      "Train Epoch: 8 [3200/8274 (38%)]\tLoss: 0.189265\n",
      "Train Epoch: 8 [3840/8274 (46%)]\tLoss: 0.291188\n",
      "Train Epoch: 8 [4480/8274 (54%)]\tLoss: 0.327778\n",
      "Train Epoch: 8 [5120/8274 (62%)]\tLoss: 0.465877\n",
      "Train Epoch: 8 [5760/8274 (69%)]\tLoss: 0.482833\n",
      "Train Epoch: 8 [6400/8274 (77%)]\tLoss: 0.252025\n",
      "Train Epoch: 8 [7040/8274 (85%)]\tLoss: 0.309847\n",
      "Train Epoch: 8 [7680/8274 (92%)]\tLoss: 0.337604\n",
      "\n",
      "Test set: Average loss: 1.2469, Accuracy: 573/1019 (56%)\n",
      "\n",
      "Train Epoch: 9 [0/8274 (0%)]\tLoss: 0.278554\n",
      "Train Epoch: 9 [640/8274 (8%)]\tLoss: 0.201720\n",
      "Train Epoch: 9 [1280/8274 (15%)]\tLoss: 0.290454\n",
      "Train Epoch: 9 [1920/8274 (23%)]\tLoss: 0.152320\n",
      "Train Epoch: 9 [2560/8274 (31%)]\tLoss: 0.298668\n",
      "Train Epoch: 9 [3200/8274 (38%)]\tLoss: 0.252314\n",
      "Train Epoch: 9 [3840/8274 (46%)]\tLoss: 0.181425\n",
      "Train Epoch: 9 [4480/8274 (54%)]\tLoss: 0.332476\n",
      "Train Epoch: 9 [5120/8274 (62%)]\tLoss: 0.430491\n",
      "Train Epoch: 9 [5760/8274 (69%)]\tLoss: 0.262352\n",
      "Train Epoch: 9 [6400/8274 (77%)]\tLoss: 0.272417\n",
      "Train Epoch: 9 [7040/8274 (85%)]\tLoss: 0.284263\n",
      "Train Epoch: 9 [7680/8274 (92%)]\tLoss: 0.220035\n",
      "\n",
      "Test set: Average loss: 1.4497, Accuracy: 555/1019 (54%)\n",
      "\n",
      "Train Epoch: 10 [0/8274 (0%)]\tLoss: 0.166448\n",
      "Train Epoch: 10 [640/8274 (8%)]\tLoss: 0.132067\n",
      "Train Epoch: 10 [1280/8274 (15%)]\tLoss: 0.213159\n",
      "Train Epoch: 10 [1920/8274 (23%)]\tLoss: 0.222318\n",
      "Train Epoch: 10 [2560/8274 (31%)]\tLoss: 0.317187\n",
      "Train Epoch: 10 [3200/8274 (38%)]\tLoss: 0.264981\n",
      "Train Epoch: 10 [3840/8274 (46%)]\tLoss: 0.264913\n",
      "Train Epoch: 10 [4480/8274 (54%)]\tLoss: 0.190150\n",
      "Train Epoch: 10 [5120/8274 (62%)]\tLoss: 0.289548\n",
      "Train Epoch: 10 [5760/8274 (69%)]\tLoss: 0.224466\n",
      "Train Epoch: 10 [6400/8274 (77%)]\tLoss: 0.282691\n",
      "Train Epoch: 10 [7040/8274 (85%)]\tLoss: 0.189489\n",
      "Train Epoch: 10 [7680/8274 (92%)]\tLoss: 0.329829\n",
      "\n",
      "Test set: Average loss: 1.3761, Accuracy: 563/1019 (55%)\n",
      "\n",
      "Train Epoch: 11 [0/8274 (0%)]\tLoss: 0.216788\n",
      "Train Epoch: 11 [640/8274 (8%)]\tLoss: 0.220935\n",
      "Train Epoch: 11 [1280/8274 (15%)]\tLoss: 0.199135\n",
      "Train Epoch: 11 [1920/8274 (23%)]\tLoss: 0.079475\n",
      "Train Epoch: 11 [2560/8274 (31%)]\tLoss: 0.118267\n",
      "Train Epoch: 11 [3200/8274 (38%)]\tLoss: 0.134767\n",
      "Train Epoch: 11 [3840/8274 (46%)]\tLoss: 0.209565\n",
      "Train Epoch: 11 [4480/8274 (54%)]\tLoss: 0.175697\n",
      "Train Epoch: 11 [5120/8274 (62%)]\tLoss: 0.149764\n",
      "Train Epoch: 11 [5760/8274 (69%)]\tLoss: 0.351301\n",
      "Train Epoch: 11 [6400/8274 (77%)]\tLoss: 0.328691\n",
      "Train Epoch: 11 [7040/8274 (85%)]\tLoss: 0.388703\n",
      "Train Epoch: 11 [7680/8274 (92%)]\tLoss: 0.148398\n",
      "\n",
      "Test set: Average loss: 1.6501, Accuracy: 558/1019 (55%)\n",
      "\n",
      "Train Epoch: 12 [0/8274 (0%)]\tLoss: 0.088030\n",
      "Train Epoch: 12 [640/8274 (8%)]\tLoss: 0.168931\n",
      "Train Epoch: 12 [1280/8274 (15%)]\tLoss: 0.170107\n",
      "Train Epoch: 12 [1920/8274 (23%)]\tLoss: 0.159161\n",
      "Train Epoch: 12 [2560/8274 (31%)]\tLoss: 0.213373\n",
      "Train Epoch: 12 [3200/8274 (38%)]\tLoss: 0.151190\n",
      "Train Epoch: 12 [3840/8274 (46%)]\tLoss: 0.227589\n",
      "Train Epoch: 12 [4480/8274 (54%)]\tLoss: 0.197462\n",
      "Train Epoch: 12 [5120/8274 (62%)]\tLoss: 0.317766\n",
      "Train Epoch: 12 [5760/8274 (69%)]\tLoss: 0.228947\n",
      "Train Epoch: 12 [6400/8274 (77%)]\tLoss: 0.095872\n",
      "Train Epoch: 12 [7040/8274 (85%)]\tLoss: 0.155563\n",
      "Train Epoch: 12 [7680/8274 (92%)]\tLoss: 0.245609\n",
      "\n",
      "Test set: Average loss: 1.6142, Accuracy: 543/1019 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/8274 (0%)]\tLoss: 0.164204\n",
      "Train Epoch: 13 [640/8274 (8%)]\tLoss: 0.122563\n",
      "Train Epoch: 13 [1280/8274 (15%)]\tLoss: 0.112346\n",
      "Train Epoch: 13 [1920/8274 (23%)]\tLoss: 0.152037\n",
      "Train Epoch: 13 [2560/8274 (31%)]\tLoss: 0.077023\n",
      "Train Epoch: 13 [3200/8274 (38%)]\tLoss: 0.146233\n",
      "Train Epoch: 13 [3840/8274 (46%)]\tLoss: 0.135326\n",
      "Train Epoch: 13 [4480/8274 (54%)]\tLoss: 0.098914\n",
      "Train Epoch: 13 [5120/8274 (62%)]\tLoss: 0.282919\n",
      "Train Epoch: 13 [5760/8274 (69%)]\tLoss: 0.147741\n",
      "Train Epoch: 13 [6400/8274 (77%)]\tLoss: 0.141697\n",
      "Train Epoch: 13 [7040/8274 (85%)]\tLoss: 0.081373\n",
      "Train Epoch: 13 [7680/8274 (92%)]\tLoss: 0.128392\n",
      "\n",
      "Test set: Average loss: 1.7065, Accuracy: 574/1019 (56%)\n",
      "\n",
      "Train Epoch: 14 [0/8274 (0%)]\tLoss: 0.061978\n",
      "Train Epoch: 14 [640/8274 (8%)]\tLoss: 0.093049\n",
      "Train Epoch: 14 [1280/8274 (15%)]\tLoss: 0.073246\n",
      "Train Epoch: 14 [1920/8274 (23%)]\tLoss: 0.211850\n",
      "Train Epoch: 14 [2560/8274 (31%)]\tLoss: 0.120688\n",
      "Train Epoch: 14 [3200/8274 (38%)]\tLoss: 0.217451\n",
      "Train Epoch: 14 [3840/8274 (46%)]\tLoss: 0.103348\n",
      "Train Epoch: 14 [4480/8274 (54%)]\tLoss: 0.075388\n",
      "Train Epoch: 14 [5120/8274 (62%)]\tLoss: 0.114320\n",
      "Train Epoch: 14 [5760/8274 (69%)]\tLoss: 0.077610\n",
      "Train Epoch: 14 [6400/8274 (77%)]\tLoss: 0.091969\n",
      "Train Epoch: 14 [7040/8274 (85%)]\tLoss: 0.205663\n",
      "Train Epoch: 14 [7680/8274 (92%)]\tLoss: 0.180729\n",
      "\n",
      "Test set: Average loss: 1.8795, Accuracy: 569/1019 (56%)\n",
      "\n",
      "Train Epoch: 15 [0/8274 (0%)]\tLoss: 0.165696\n",
      "Train Epoch: 15 [640/8274 (8%)]\tLoss: 0.113460\n",
      "Train Epoch: 15 [1280/8274 (15%)]\tLoss: 0.103383\n",
      "Train Epoch: 15 [1920/8274 (23%)]\tLoss: 0.181091\n",
      "Train Epoch: 15 [2560/8274 (31%)]\tLoss: 0.052874\n",
      "Train Epoch: 15 [3200/8274 (38%)]\tLoss: 0.043493\n",
      "Train Epoch: 15 [3840/8274 (46%)]\tLoss: 0.104581\n",
      "Train Epoch: 15 [4480/8274 (54%)]\tLoss: 0.053355\n",
      "Train Epoch: 15 [5120/8274 (62%)]\tLoss: 0.102458\n",
      "Train Epoch: 15 [5760/8274 (69%)]\tLoss: 0.189194\n",
      "Train Epoch: 15 [6400/8274 (77%)]\tLoss: 0.153335\n",
      "Train Epoch: 15 [7040/8274 (85%)]\tLoss: 0.157744\n",
      "Train Epoch: 15 [7680/8274 (92%)]\tLoss: 0.065375\n",
      "\n",
      "Test set: Average loss: 1.7935, Accuracy: 567/1019 (56%)\n",
      "\n",
      "Train Epoch: 16 [0/8274 (0%)]\tLoss: 0.165694\n",
      "Train Epoch: 16 [640/8274 (8%)]\tLoss: 0.157382\n",
      "Train Epoch: 16 [1280/8274 (15%)]\tLoss: 0.205833\n",
      "Train Epoch: 16 [1920/8274 (23%)]\tLoss: 0.034632\n",
      "Train Epoch: 16 [2560/8274 (31%)]\tLoss: 0.111201\n",
      "Train Epoch: 16 [3200/8274 (38%)]\tLoss: 0.078134\n",
      "Train Epoch: 16 [3840/8274 (46%)]\tLoss: 0.090278\n",
      "Train Epoch: 16 [4480/8274 (54%)]\tLoss: 0.159437\n",
      "Train Epoch: 16 [5120/8274 (62%)]\tLoss: 0.070372\n",
      "Train Epoch: 16 [5760/8274 (69%)]\tLoss: 0.072910\n",
      "Train Epoch: 16 [6400/8274 (77%)]\tLoss: 0.108162\n",
      "Train Epoch: 16 [7040/8274 (85%)]\tLoss: 0.105170\n",
      "Train Epoch: 16 [7680/8274 (92%)]\tLoss: 0.103307\n",
      "\n",
      "Test set: Average loss: 1.9715, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 17 [0/8274 (0%)]\tLoss: 0.073738\n",
      "Train Epoch: 17 [640/8274 (8%)]\tLoss: 0.088899\n",
      "Train Epoch: 17 [1280/8274 (15%)]\tLoss: 0.074882\n",
      "Train Epoch: 17 [1920/8274 (23%)]\tLoss: 0.114307\n",
      "Train Epoch: 17 [2560/8274 (31%)]\tLoss: 0.149008\n",
      "Train Epoch: 17 [3200/8274 (38%)]\tLoss: 0.066079\n",
      "Train Epoch: 17 [3840/8274 (46%)]\tLoss: 0.053714\n",
      "Train Epoch: 17 [4480/8274 (54%)]\tLoss: 0.197973\n",
      "Train Epoch: 17 [5120/8274 (62%)]\tLoss: 0.045736\n",
      "Train Epoch: 17 [5760/8274 (69%)]\tLoss: 0.082869\n",
      "Train Epoch: 17 [6400/8274 (77%)]\tLoss: 0.111240\n",
      "Train Epoch: 17 [7040/8274 (85%)]\tLoss: 0.178065\n",
      "Train Epoch: 17 [7680/8274 (92%)]\tLoss: 0.091870\n",
      "\n",
      "Test set: Average loss: 2.0132, Accuracy: 580/1019 (57%)\n",
      "\n",
      "Train Epoch: 18 [0/8274 (0%)]\tLoss: 0.080004\n",
      "Train Epoch: 18 [640/8274 (8%)]\tLoss: 0.077099\n",
      "Train Epoch: 18 [1280/8274 (15%)]\tLoss: 0.022260\n",
      "Train Epoch: 18 [1920/8274 (23%)]\tLoss: 0.058934\n",
      "Train Epoch: 18 [2560/8274 (31%)]\tLoss: 0.031628\n",
      "Train Epoch: 18 [3200/8274 (38%)]\tLoss: 0.021345\n",
      "Train Epoch: 18 [3840/8274 (46%)]\tLoss: 0.111898\n",
      "Train Epoch: 18 [4480/8274 (54%)]\tLoss: 0.027325\n",
      "Train Epoch: 18 [5120/8274 (62%)]\tLoss: 0.134367\n",
      "Train Epoch: 18 [5760/8274 (69%)]\tLoss: 0.052373\n",
      "Train Epoch: 18 [6400/8274 (77%)]\tLoss: 0.090574\n",
      "Train Epoch: 18 [7040/8274 (85%)]\tLoss: 0.053378\n",
      "Train Epoch: 18 [7680/8274 (92%)]\tLoss: 0.224938\n",
      "\n",
      "Test set: Average loss: 2.2072, Accuracy: 571/1019 (56%)\n",
      "\n",
      "Train Epoch: 19 [0/8274 (0%)]\tLoss: 0.034368\n",
      "Train Epoch: 19 [640/8274 (8%)]\tLoss: 0.297415\n",
      "Train Epoch: 19 [1280/8274 (15%)]\tLoss: 0.118108\n",
      "Train Epoch: 19 [1920/8274 (23%)]\tLoss: 0.164532\n",
      "Train Epoch: 19 [2560/8274 (31%)]\tLoss: 0.043562\n",
      "Train Epoch: 19 [3200/8274 (38%)]\tLoss: 0.067502\n",
      "Train Epoch: 19 [3840/8274 (46%)]\tLoss: 0.071764\n",
      "Train Epoch: 19 [4480/8274 (54%)]\tLoss: 0.075906\n",
      "Train Epoch: 19 [5120/8274 (62%)]\tLoss: 0.087370\n",
      "Train Epoch: 19 [5760/8274 (69%)]\tLoss: 0.068723\n",
      "Train Epoch: 19 [6400/8274 (77%)]\tLoss: 0.179665\n",
      "Train Epoch: 19 [7040/8274 (85%)]\tLoss: 0.056440\n",
      "Train Epoch: 19 [7680/8274 (92%)]\tLoss: 0.046467\n",
      "\n",
      "Test set: Average loss: 2.2023, Accuracy: 589/1019 (58%)\n",
      "\n",
      "Train Epoch: 20 [0/8274 (0%)]\tLoss: 0.060617\n",
      "Train Epoch: 20 [640/8274 (8%)]\tLoss: 0.042289\n",
      "Train Epoch: 20 [1280/8274 (15%)]\tLoss: 0.031318\n",
      "Train Epoch: 20 [1920/8274 (23%)]\tLoss: 0.036602\n",
      "Train Epoch: 20 [2560/8274 (31%)]\tLoss: 0.043818\n",
      "Train Epoch: 20 [3200/8274 (38%)]\tLoss: 0.018168\n",
      "Train Epoch: 20 [3840/8274 (46%)]\tLoss: 0.102271\n",
      "Train Epoch: 20 [4480/8274 (54%)]\tLoss: 0.060927\n",
      "Train Epoch: 20 [5120/8274 (62%)]\tLoss: 0.034336\n",
      "Train Epoch: 20 [5760/8274 (69%)]\tLoss: 0.012616\n",
      "Train Epoch: 20 [6400/8274 (77%)]\tLoss: 0.063111\n",
      "Train Epoch: 20 [7040/8274 (85%)]\tLoss: 0.101758\n",
      "Train Epoch: 20 [7680/8274 (92%)]\tLoss: 0.029474\n",
      "\n",
      "Test set: Average loss: 2.2883, Accuracy: 605/1019 (59%)\n",
      "\n",
      "Train Epoch: 21 [0/8274 (0%)]\tLoss: 0.081451\n",
      "Train Epoch: 21 [640/8274 (8%)]\tLoss: 0.038359\n",
      "Train Epoch: 21 [1280/8274 (15%)]\tLoss: 0.101008\n",
      "Train Epoch: 21 [1920/8274 (23%)]\tLoss: 0.020144\n",
      "Train Epoch: 21 [2560/8274 (31%)]\tLoss: 0.059641\n",
      "Train Epoch: 21 [3200/8274 (38%)]\tLoss: 0.053672\n",
      "Train Epoch: 21 [3840/8274 (46%)]\tLoss: 0.055626\n",
      "Train Epoch: 21 [4480/8274 (54%)]\tLoss: 0.042002\n",
      "Train Epoch: 21 [5120/8274 (62%)]\tLoss: 0.039061\n",
      "Train Epoch: 21 [5760/8274 (69%)]\tLoss: 0.043904\n",
      "Train Epoch: 21 [6400/8274 (77%)]\tLoss: 0.054775\n",
      "Train Epoch: 21 [7040/8274 (85%)]\tLoss: 0.023871\n",
      "Train Epoch: 21 [7680/8274 (92%)]\tLoss: 0.065675\n",
      "\n",
      "Test set: Average loss: 2.4668, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 22 [0/8274 (0%)]\tLoss: 0.022674\n",
      "Train Epoch: 22 [640/8274 (8%)]\tLoss: 0.067598\n",
      "Train Epoch: 22 [1280/8274 (15%)]\tLoss: 0.031368\n",
      "Train Epoch: 22 [1920/8274 (23%)]\tLoss: 0.032587\n",
      "Train Epoch: 22 [2560/8274 (31%)]\tLoss: 0.020036\n",
      "Train Epoch: 22 [3200/8274 (38%)]\tLoss: 0.033594\n",
      "Train Epoch: 22 [3840/8274 (46%)]\tLoss: 0.031401\n",
      "Train Epoch: 22 [4480/8274 (54%)]\tLoss: 0.044426\n",
      "Train Epoch: 22 [5120/8274 (62%)]\tLoss: 0.012346\n",
      "Train Epoch: 22 [5760/8274 (69%)]\tLoss: 0.034235\n",
      "Train Epoch: 22 [6400/8274 (77%)]\tLoss: 0.017133\n",
      "Train Epoch: 22 [7040/8274 (85%)]\tLoss: 0.059678\n",
      "Train Epoch: 22 [7680/8274 (92%)]\tLoss: 0.058039\n",
      "\n",
      "Test set: Average loss: 2.3352, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 23 [0/8274 (0%)]\tLoss: 0.034534\n",
      "Train Epoch: 23 [640/8274 (8%)]\tLoss: 0.056807\n",
      "Train Epoch: 23 [1280/8274 (15%)]\tLoss: 0.073159\n",
      "Train Epoch: 23 [1920/8274 (23%)]\tLoss: 0.020769\n",
      "Train Epoch: 23 [2560/8274 (31%)]\tLoss: 0.102107\n",
      "Train Epoch: 23 [3200/8274 (38%)]\tLoss: 0.093320\n",
      "Train Epoch: 23 [3840/8274 (46%)]\tLoss: 0.075593\n",
      "Train Epoch: 23 [4480/8274 (54%)]\tLoss: 0.052037\n",
      "Train Epoch: 23 [5120/8274 (62%)]\tLoss: 0.032472\n",
      "Train Epoch: 23 [5760/8274 (69%)]\tLoss: 0.045307\n",
      "Train Epoch: 23 [6400/8274 (77%)]\tLoss: 0.015498\n",
      "Train Epoch: 23 [7040/8274 (85%)]\tLoss: 0.039425\n",
      "Train Epoch: 23 [7680/8274 (92%)]\tLoss: 0.039563\n",
      "\n",
      "Test set: Average loss: 2.4957, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 24 [0/8274 (0%)]\tLoss: 0.013583\n",
      "Train Epoch: 24 [640/8274 (8%)]\tLoss: 0.015109\n",
      "Train Epoch: 24 [1280/8274 (15%)]\tLoss: 0.022396\n",
      "Train Epoch: 24 [1920/8274 (23%)]\tLoss: 0.023216\n",
      "Train Epoch: 24 [2560/8274 (31%)]\tLoss: 0.009459\n",
      "Train Epoch: 24 [3200/8274 (38%)]\tLoss: 0.020897\n",
      "Train Epoch: 24 [3840/8274 (46%)]\tLoss: 0.019918\n",
      "Train Epoch: 24 [4480/8274 (54%)]\tLoss: 0.126928\n",
      "Train Epoch: 24 [5120/8274 (62%)]\tLoss: 0.012062\n",
      "Train Epoch: 24 [5760/8274 (69%)]\tLoss: 0.177633\n",
      "Train Epoch: 24 [6400/8274 (77%)]\tLoss: 0.008754\n",
      "Train Epoch: 24 [7040/8274 (85%)]\tLoss: 0.067970\n",
      "Train Epoch: 24 [7680/8274 (92%)]\tLoss: 0.011033\n",
      "\n",
      "Test set: Average loss: 2.6644, Accuracy: 590/1019 (58%)\n",
      "\n",
      "Train Epoch: 25 [0/8274 (0%)]\tLoss: 0.007161\n",
      "Train Epoch: 25 [640/8274 (8%)]\tLoss: 0.012297\n",
      "Train Epoch: 25 [1280/8274 (15%)]\tLoss: 0.007811\n",
      "Train Epoch: 25 [1920/8274 (23%)]\tLoss: 0.020816\n",
      "Train Epoch: 25 [2560/8274 (31%)]\tLoss: 0.010438\n",
      "Train Epoch: 25 [3200/8274 (38%)]\tLoss: 0.008567\n",
      "Train Epoch: 25 [3840/8274 (46%)]\tLoss: 0.030842\n",
      "Train Epoch: 25 [4480/8274 (54%)]\tLoss: 0.052421\n",
      "Train Epoch: 25 [5120/8274 (62%)]\tLoss: 0.026481\n",
      "Train Epoch: 25 [5760/8274 (69%)]\tLoss: 0.006044\n",
      "Train Epoch: 25 [6400/8274 (77%)]\tLoss: 0.041571\n",
      "Train Epoch: 25 [7040/8274 (85%)]\tLoss: 0.012302\n",
      "Train Epoch: 25 [7680/8274 (92%)]\tLoss: 0.024672\n",
      "\n",
      "Test set: Average loss: 2.7590, Accuracy: 591/1019 (58%)\n",
      "\n",
      "Train Epoch: 26 [0/8274 (0%)]\tLoss: 0.007808\n",
      "Train Epoch: 26 [640/8274 (8%)]\tLoss: 0.008159\n",
      "Train Epoch: 26 [1280/8274 (15%)]\tLoss: 0.005539\n",
      "Train Epoch: 26 [1920/8274 (23%)]\tLoss: 0.027577\n",
      "Train Epoch: 26 [2560/8274 (31%)]\tLoss: 0.022328\n",
      "Train Epoch: 26 [3200/8274 (38%)]\tLoss: 0.007900\n",
      "Train Epoch: 26 [3840/8274 (46%)]\tLoss: 0.004705\n",
      "Train Epoch: 26 [4480/8274 (54%)]\tLoss: 0.007780\n",
      "Train Epoch: 26 [5120/8274 (62%)]\tLoss: 0.026498\n",
      "Train Epoch: 26 [5760/8274 (69%)]\tLoss: 0.039090\n",
      "Train Epoch: 26 [6400/8274 (77%)]\tLoss: 0.031113\n",
      "Train Epoch: 26 [7040/8274 (85%)]\tLoss: 0.023815\n",
      "Train Epoch: 26 [7680/8274 (92%)]\tLoss: 0.011278\n",
      "\n",
      "Test set: Average loss: 2.8549, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 27 [0/8274 (0%)]\tLoss: 0.014295\n",
      "Train Epoch: 27 [640/8274 (8%)]\tLoss: 0.013152\n",
      "Train Epoch: 27 [1280/8274 (15%)]\tLoss: 0.028498\n",
      "Train Epoch: 27 [1920/8274 (23%)]\tLoss: 0.022730\n",
      "Train Epoch: 27 [2560/8274 (31%)]\tLoss: 0.008494\n",
      "Train Epoch: 27 [3200/8274 (38%)]\tLoss: 0.019090\n",
      "Train Epoch: 27 [3840/8274 (46%)]\tLoss: 0.009679\n",
      "Train Epoch: 27 [4480/8274 (54%)]\tLoss: 0.008291\n",
      "Train Epoch: 27 [5120/8274 (62%)]\tLoss: 0.078989\n",
      "Train Epoch: 27 [5760/8274 (69%)]\tLoss: 0.135462\n",
      "Train Epoch: 27 [6400/8274 (77%)]\tLoss: 0.006230\n",
      "Train Epoch: 27 [7040/8274 (85%)]\tLoss: 0.139640\n",
      "Train Epoch: 27 [7680/8274 (92%)]\tLoss: 0.067146\n",
      "\n",
      "Test set: Average loss: 2.9278, Accuracy: 587/1019 (58%)\n",
      "\n",
      "Train Epoch: 28 [0/8274 (0%)]\tLoss: 0.087195\n",
      "Train Epoch: 28 [640/8274 (8%)]\tLoss: 0.035368\n",
      "Train Epoch: 28 [1280/8274 (15%)]\tLoss: 0.022279\n",
      "Train Epoch: 28 [1920/8274 (23%)]\tLoss: 0.074493\n",
      "Train Epoch: 28 [2560/8274 (31%)]\tLoss: 0.047947\n",
      "Train Epoch: 28 [3200/8274 (38%)]\tLoss: 0.118312\n",
      "Train Epoch: 28 [3840/8274 (46%)]\tLoss: 0.018994\n",
      "Train Epoch: 28 [4480/8274 (54%)]\tLoss: 0.032705\n",
      "Train Epoch: 28 [5120/8274 (62%)]\tLoss: 0.066585\n",
      "Train Epoch: 28 [5760/8274 (69%)]\tLoss: 0.142450\n",
      "Train Epoch: 28 [6400/8274 (77%)]\tLoss: 0.026270\n",
      "Train Epoch: 28 [7040/8274 (85%)]\tLoss: 0.106372\n",
      "Train Epoch: 28 [7680/8274 (92%)]\tLoss: 0.073717\n",
      "\n",
      "Test set: Average loss: 2.8160, Accuracy: 586/1019 (58%)\n",
      "\n",
      "Train Epoch: 29 [0/8274 (0%)]\tLoss: 0.054149\n",
      "Train Epoch: 29 [640/8274 (8%)]\tLoss: 0.027874\n",
      "Train Epoch: 29 [1280/8274 (15%)]\tLoss: 0.080834\n",
      "Train Epoch: 29 [1920/8274 (23%)]\tLoss: 0.017308\n",
      "Train Epoch: 29 [2560/8274 (31%)]\tLoss: 0.094987\n",
      "Train Epoch: 29 [3200/8274 (38%)]\tLoss: 0.016403\n",
      "Train Epoch: 29 [3840/8274 (46%)]\tLoss: 0.021982\n",
      "Train Epoch: 29 [4480/8274 (54%)]\tLoss: 0.021192\n",
      "Train Epoch: 29 [5120/8274 (62%)]\tLoss: 0.004673\n",
      "Train Epoch: 29 [5760/8274 (69%)]\tLoss: 0.032932\n",
      "Train Epoch: 29 [6400/8274 (77%)]\tLoss: 0.140668\n",
      "Train Epoch: 29 [7040/8274 (85%)]\tLoss: 0.034239\n",
      "Train Epoch: 29 [7680/8274 (92%)]\tLoss: 0.036395\n",
      "\n",
      "Test set: Average loss: 2.7405, Accuracy: 590/1019 (58%)\n",
      "\n",
      "Train Epoch: 30 [0/8274 (0%)]\tLoss: 0.012351\n",
      "Train Epoch: 30 [640/8274 (8%)]\tLoss: 0.040658\n",
      "Train Epoch: 30 [1280/8274 (15%)]\tLoss: 0.039749\n",
      "Train Epoch: 30 [1920/8274 (23%)]\tLoss: 0.033518\n",
      "Train Epoch: 30 [2560/8274 (31%)]\tLoss: 0.014102\n",
      "Train Epoch: 30 [3200/8274 (38%)]\tLoss: 0.032188\n",
      "Train Epoch: 30 [3840/8274 (46%)]\tLoss: 0.011797\n",
      "Train Epoch: 30 [4480/8274 (54%)]\tLoss: 0.061425\n",
      "Train Epoch: 30 [5120/8274 (62%)]\tLoss: 0.024519\n",
      "Train Epoch: 30 [5760/8274 (69%)]\tLoss: 0.035009\n",
      "Train Epoch: 30 [6400/8274 (77%)]\tLoss: 0.026070\n",
      "Train Epoch: 30 [7040/8274 (85%)]\tLoss: 0.014676\n",
      "Train Epoch: 30 [7680/8274 (92%)]\tLoss: 0.039102\n",
      "\n",
      "Test set: Average loss: 2.7373, Accuracy: 600/1019 (59%)\n",
      "\n",
      "Train Epoch: 31 [0/8274 (0%)]\tLoss: 0.012246\n",
      "Train Epoch: 31 [640/8274 (8%)]\tLoss: 0.050450\n",
      "Train Epoch: 31 [1280/8274 (15%)]\tLoss: 0.004842\n",
      "Train Epoch: 31 [1920/8274 (23%)]\tLoss: 0.013393\n",
      "Train Epoch: 31 [2560/8274 (31%)]\tLoss: 0.056836\n",
      "Train Epoch: 31 [3200/8274 (38%)]\tLoss: 0.046134\n",
      "Train Epoch: 31 [3840/8274 (46%)]\tLoss: 0.027790\n",
      "Train Epoch: 31 [4480/8274 (54%)]\tLoss: 0.020022\n",
      "Train Epoch: 31 [5120/8274 (62%)]\tLoss: 0.005069\n",
      "Train Epoch: 31 [5760/8274 (69%)]\tLoss: 0.010823\n",
      "Train Epoch: 31 [6400/8274 (77%)]\tLoss: 0.003087\n",
      "Train Epoch: 31 [7040/8274 (85%)]\tLoss: 0.013752\n",
      "Train Epoch: 31 [7680/8274 (92%)]\tLoss: 0.005686\n",
      "\n",
      "Test set: Average loss: 2.9137, Accuracy: 596/1019 (58%)\n",
      "\n",
      "Train Epoch: 32 [0/8274 (0%)]\tLoss: 0.005129\n",
      "Train Epoch: 32 [640/8274 (8%)]\tLoss: 0.052425\n",
      "Train Epoch: 32 [1280/8274 (15%)]\tLoss: 0.005652\n",
      "Train Epoch: 32 [1920/8274 (23%)]\tLoss: 0.015080\n",
      "Train Epoch: 32 [2560/8274 (31%)]\tLoss: 0.013221\n",
      "Train Epoch: 32 [3200/8274 (38%)]\tLoss: 0.004670\n",
      "Train Epoch: 32 [3840/8274 (46%)]\tLoss: 0.073755\n",
      "Train Epoch: 32 [4480/8274 (54%)]\tLoss: 0.012212\n",
      "Train Epoch: 32 [5120/8274 (62%)]\tLoss: 0.002763\n",
      "Train Epoch: 32 [5760/8274 (69%)]\tLoss: 0.008462\n",
      "Train Epoch: 32 [6400/8274 (77%)]\tLoss: 0.006330\n",
      "Train Epoch: 32 [7040/8274 (85%)]\tLoss: 0.003992\n",
      "Train Epoch: 32 [7680/8274 (92%)]\tLoss: 0.003326\n",
      "\n",
      "Test set: Average loss: 3.0110, Accuracy: 593/1019 (58%)\n",
      "\n",
      "Train Epoch: 33 [0/8274 (0%)]\tLoss: 0.001408\n",
      "Train Epoch: 33 [640/8274 (8%)]\tLoss: 0.007096\n",
      "Train Epoch: 33 [1280/8274 (15%)]\tLoss: 0.004063\n",
      "Train Epoch: 33 [1920/8274 (23%)]\tLoss: 0.005392\n",
      "Train Epoch: 33 [2560/8274 (31%)]\tLoss: 0.010651\n",
      "Train Epoch: 33 [3200/8274 (38%)]\tLoss: 0.001236\n",
      "Train Epoch: 33 [3840/8274 (46%)]\tLoss: 0.005080\n",
      "Train Epoch: 33 [4480/8274 (54%)]\tLoss: 0.001863\n",
      "Train Epoch: 33 [5120/8274 (62%)]\tLoss: 0.007823\n",
      "Train Epoch: 33 [5760/8274 (69%)]\tLoss: 0.003535\n",
      "Train Epoch: 33 [6400/8274 (77%)]\tLoss: 0.010930\n",
      "Train Epoch: 33 [7040/8274 (85%)]\tLoss: 0.002165\n",
      "Train Epoch: 33 [7680/8274 (92%)]\tLoss: 0.002994\n",
      "\n",
      "Test set: Average loss: 3.0840, Accuracy: 587/1019 (58%)\n",
      "\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "class DNNArgs:\n",
    "  epochs = 33 # Tuning the epoch to 33 using validation set \n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "DNNargs = DNNArgs()\n",
    "\n",
    "torch.manual_seed(DNNargs.seed)\n",
    "\n",
    "dnn_model = DNN().to(device)\n",
    "\n",
    "for param_tensor in dnn_model.state_dict():\n",
    "        print(param_tensor, \"\\t\", dnn_model.state_dict()[param_tensor].size())\n",
    "\n",
    "#Form training and testing dataset\n",
    "dnn_optimizer = optim.Adam(dnn_model.parameters(), lr=DNNargs.lr)\n",
    "\n",
    "#Model training\n",
    "ACC = 0\n",
    "for epoch in range(1, DNNargs.epochs + 1):\n",
    "    train(DNNargs, dnn_model, device, train_loader, dnn_optimizer, epoch)\n",
    "    ACC_ = test(dnn_model, device, test_loader)\n",
    "    if ACC_>ACC or ACC_ == ACC:\n",
    "        ACC = ACC_\n",
    "        torch.save(dnn_model.state_dict(), \"Baseline_DNN.pt\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(ACC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 59.37193326790972\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "dnn_model.eval()\n",
    "dnn_correct_val = 0\n",
    "dnn_total_val = 0\n",
    "dnn_val_loss = 0\n",
    "DNN_test_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device).long()\n",
    "        \n",
    "        output_test = dnn_model(data)\n",
    "        pred = torch.argmax(output_test, 1)\n",
    "        \n",
    "        dnn_val_loss += F.cross_entropy(output_test, target) \n",
    "            \n",
    "        dnn_correct_val += (pred == target).sum().item()\n",
    "        \n",
    "        dnn_total_val += target.size(0)\n",
    "    \n",
    "    DNN_test_accuracy = (dnn_correct_val / dnn_total_val) * 100\n",
    "            \n",
    "    dnn_val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Testing Accuracy = {DNN_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test set for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(['target', 'casename'], axis=1)\n",
    "y = processed_df['target'] \n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "for train_index, remaining_index in stratified_split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[remaining_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[remaining_index]\n",
    "\n",
    "# Handle imbalanced classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing textual features using TF-IDF for X_train\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_text = tfidf_vectorizer.fit_transform(X_train_resampled['processed_facts'].astype('U') + ' ' + X_train_resampled['processed_issues'].astype('U'))\n",
    "X_train_text = pd.DataFrame(X_train_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_train_resampled = X_train_resampled.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_train_resampled = pd.concat([X_train_resampled.reset_index(drop=True), X_train_text], axis=1)\n",
    "\n",
    "# Vectorizing textual features using TF-IDF for X_test\n",
    "X_test_text = tfidf_vectorizer.transform(X_test['processed_facts'].astype('U') + ' ' + X_test['processed_issues'].astype('U'))\n",
    "X_test_text = pd.DataFrame(X_test_text.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text columns and concatenate TF-IDF features\n",
    "X_test = X_test.drop(['processed_facts', 'processed_issues'], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_text], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Param Tuning (Grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_classifier = RandomForestClassifier(random_state=42)\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Setup the grid search\n",
    "# grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# # Fit grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and best score\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dump(best_rf, 'model/rf_model.joblib')\n",
    "best_rf = load('model/rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_resampled.columns\n",
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df = feature_importance_df[(feature_importance_df['Importance']) > 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>division</th>\n",
       "      <th>anything</th>\n",
       "      <th>july</th>\n",
       "      <th>criminal law</th>\n",
       "      <th>argued</th>\n",
       "      <th>majority</th>\n",
       "      <th>allegedly</th>\n",
       "      <th>failed</th>\n",
       "      <th>ordinary</th>\n",
       "      <th>iii</th>\n",
       "      <th>...</th>\n",
       "      <th>res judicata</th>\n",
       "      <th>family violence</th>\n",
       "      <th>admiralty and shipping</th>\n",
       "      <th>advice</th>\n",
       "      <th>advice</th>\n",
       "      <th>adverse possession</th>\n",
       "      <th>international taxation</th>\n",
       "      <th>hdb flat</th>\n",
       "      <th>courts and jurisdiction</th>\n",
       "      <th>misrepresentation act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029518</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.174046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8269</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.019155</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8273</th>\n",
       "      <td>0.007187</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8274 rows × 1257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      division  anything      july  criminal law    argued  majority  \\\n",
       "0     0.009229  0.000000  0.000000             0  0.005410  0.000000   \n",
       "1     0.001328  0.001974  0.000802             0  0.003113  0.002480   \n",
       "2     0.000000  0.017659  0.038272             0  0.000000  0.000000   \n",
       "3     0.000000  0.001575  0.174046             0  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000             0  0.008033  0.000000   \n",
       "...        ...       ...       ...           ...       ...       ...   \n",
       "8269  0.000000  0.001823  0.017783             0  0.030196  0.002291   \n",
       "8270  0.000000  0.000000  0.000000             0  0.000000  0.000000   \n",
       "8271  0.000000  0.000000  0.005170             0  0.020067  0.000000   \n",
       "8272  0.000000  0.000000  0.005254             0  0.010197  0.000000   \n",
       "8273  0.007187  0.005342  0.008684             0  0.000000  0.000000   \n",
       "\n",
       "      allegedly    failed  ordinary       iii  ...  res judicata  \\\n",
       "0      0.000000  0.005148  0.000000  0.029518  ...             0   \n",
       "1      0.000000  0.002962  0.012157  0.005308  ...             0   \n",
       "2      0.006375  0.022086  0.006592  0.000000  ...             0   \n",
       "3      0.017053  0.009453  0.001763  0.000000  ...             0   \n",
       "4      0.000000  0.007644  0.000000  0.010957  ...             0   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "8269   0.001975  0.019155  0.004084  0.005884  ...             0   \n",
       "8270   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8271   0.006889  0.038189  0.007124  0.000000  ...             0   \n",
       "8272   0.000000  0.000000  0.000000  0.000000  ...             0   \n",
       "8273   0.000000  0.016036  0.000000  0.000000  ...             0   \n",
       "\n",
       "      family violence  admiralty and shipping  advice  advice  \\\n",
       "0                   0                       0       0     0.0   \n",
       "1                   0                       0       0     0.0   \n",
       "2                   0                       0       0     0.0   \n",
       "3                   0                       0       0     0.0   \n",
       "4                   0                       0       0     0.0   \n",
       "...               ...                     ...     ...     ...   \n",
       "8269                0                       0       0     0.0   \n",
       "8270                0                       0       0     0.0   \n",
       "8271                0                       0       0     0.0   \n",
       "8272                0                       0       0     0.0   \n",
       "8273                0                       0       0     0.0   \n",
       "\n",
       "      adverse possession  international taxation  hdb flat  \\\n",
       "0                      0                       0         0   \n",
       "1                      0                       0         0   \n",
       "2                      0                       0         0   \n",
       "3                      0                       0         0   \n",
       "4                      0                       0         0   \n",
       "...                  ...                     ...       ...   \n",
       "8269                   0                       0         0   \n",
       "8270                   0                       0         0   \n",
       "8271                   0                       0         0   \n",
       "8272                   0                       0         0   \n",
       "8273                   0                       0         0   \n",
       "\n",
       "      courts and jurisdiction  misrepresentation act  \n",
       "0                           0                      0  \n",
       "1                           0                      0  \n",
       "2                           0                      0  \n",
       "3                           0                      0  \n",
       "4                           0                      0  \n",
       "...                       ...                    ...  \n",
       "8269                        0                      0  \n",
       "8270                        0                      0  \n",
       "8271                        0                      0  \n",
       "8272                        0                      0  \n",
       "8273                        0                      0  \n",
       "\n",
       "[8274 rows x 1257 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = feature_importance_df['Feature'].tolist()\n",
    "\n",
    "X_train_filtered = X_train_resampled[important_features]\n",
    "X_test_filtered = X_test[important_features]\n",
    "X_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RFE from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# selector = RFE(best_rf, n_features_to_select=1000, step=1)\n",
    "# selector = selector.fit(X_train_filtered, y_train_resampled)\n",
    "# dump(selector, 'model/rfe_selector.joblib')\n",
    "selector = load('model/rfe_selector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = selector.transform(X_train_filtered)\n",
    "X_test_reduced = selector.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.fit(X_train_reduced, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5912659470068695\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Favourable       0.60      0.95      0.73      1183\n",
      "  No outcome       0.40      0.03      0.06       238\n",
      "Unfavourable       0.54      0.11      0.19       617\n",
      "\n",
      "    accuracy                           0.59      2038\n",
      "   macro avg       0.51      0.37      0.33      2038\n",
      "weighted avg       0.56      0.59      0.49      2038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_rf.predict(X_test_reduced)\n",
    "RF_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Evaluating the Model\n",
    "print(\"Accuracy:\", RF_accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling (Multiclass Logistic Regression) in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0001\n",
      "Processing for 0.001\n",
      "Processing for 0.01\n",
      "Processing for 0.1\n",
      "Processing for 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the results\n",
      "multi lbfgs 0 0.0000 0.485\n",
      "multi lbfgs l2 0.0001 0.275\n",
      "multi lbfgs l2 0.0010 0.396\n",
      "multi lbfgs l2 0.0100 0.462\n",
      "multi lbfgs l2 0.1000 0.493\n",
      "multi lbfgs l2 1.0000 0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = processed_df.drop(columns=['target','casename','processed_facts', 'processed_issues'])\n",
    "y = processed_df['target']\n",
    "\n",
    "# Handle imbalanced classes\n",
    "smt = SMOTE(random_state=42)\n",
    "\n",
    "results = []\n",
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', smt), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "  \n",
    "\t\t#StratifiedKFold is the improved version of KFold\n",
    "\t\t#KFold is a cross-validator that divides the dataset into k folds. \n",
    "  \t\t#Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['multi lbfgs','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['multi lbfgs','l2',key,mean(scores)])\n",
    "\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0001\n",
      "Processing for 0.001\n",
      "Processing for 0.01\n",
      "Processing for 0.1\n",
      "Processing for 1.0\n",
      "\n",
      "Here are the results\n",
      "multi lbfgs 0 0.0000 0.485\n",
      "multi lbfgs l2 0.0001 0.275\n",
      "multi lbfgs l2 0.0010 0.396\n",
      "multi lbfgs l2 0.0100 0.462\n",
      "multi lbfgs l2 0.1000 0.493\n",
      "multi lbfgs l2 1.0000 0.495\n",
      "ovr lbfgs l2 0.0001 0.259\n",
      "ovr lbfgs l2 0.0010 0.383\n",
      "ovr lbfgs l2 0.0100 0.452\n",
      "ovr lbfgs l2 0.1000 0.499\n",
      "ovr lbfgs l2 1.0000 0.505\n"
     ]
    }
   ],
   "source": [
    "for p in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p  \n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='lbfgs', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='lbfgs', penalty='l2', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', SMOTE()), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['ovr lbfgs','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['ovr lbfgs','l2',key,mean(scores)])\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 0.0001\n",
      "Processing for 0.001\n",
      "Processing for 0.01\n",
      "Processing for 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39704\\1033703413.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'over'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;31m# evaluate pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRepeatedStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi saga'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                     )\n\u001b[0;32m    212\u001b[0m                 ):\n\u001b[0;32m    213\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# the function to delegate validation to the estimator, but we replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# the name of the estimator by the name of the function in the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \"\"\"\n\u001b[0;32m    716\u001b[0m     \u001b[1;31m# To ensure multimetric format is not supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    720\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                     )\n\u001b[0;32m    212\u001b[0m                 ):\n\u001b[0;32m    213\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# the function to delegate validation to the estimator, but we replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# the name of the estimator by the name of the function in the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;31m# We clone the estimator to make sure that all the folds are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    431\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    432\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m         iterable_with_config = (\n\u001b[0;32m     64\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         )\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1914\u001b[0m             \u001b[1;31m# If n_jobs==1, run the computation sequentially and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m             \u001b[1;31m# immediatly to avoid overheads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[1;31m# call is interrupted early and that the same instance is immediately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             )\n\u001b[0;32m    127\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m         \u001b[1;31m# Note fit time as time until error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                 )\n\u001b[0;32m   1473\u001b[0m             ):\n\u001b[1;32m-> 1474\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m    321\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                 \u001b[0mlast_step_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    254\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 )\n\u001b[0;32m    257\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_resample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 X, y, fitted_transformer = fit_resample_one_cached(\n\u001b[0m\u001b[0;32m    259\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(sampler, X, y, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_fit_resample_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fit_resample\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0my_resampled\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    114\u001b[0m         y_ = (\n\u001b[0;32m    115\u001b[0m             \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfrom_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_props\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfrom_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_props\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         if self.x_props[\"type\"].lower() == \"dataframe\" and self.y_props[\n\u001b[0;32m     43\u001b[0m             \u001b[1;34m\"type\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, array, props)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dtypes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# We special case the following error:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[1;31m# https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[1;31m# There is no easy way to have a generic workaround. Here, we detect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6617\u001b[0m                     \u001b[0mres_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6618\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6619\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6620\u001b[0m                         \u001b[0mres_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6621\u001b[1;33m                     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6622\u001b[0m                         ex.args = (\n\u001b[0;32m   6623\u001b[0m                             \u001b[1;34mf\"{ex}: Error while type casting for column '{col_name}'\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6624\u001b[0m                         )\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6641\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6642\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6643\u001b[0m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6644\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6645\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6647\u001b[0m         \u001b[1;31m# GH 33113: handle empty frame or series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_constructor_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# caller is responsible for setting real name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# This would also work `if self._constructor is Series`, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\case-outcome-predictor\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6317\u001b[0m         \u001b[1;31m# if this fails, go on to more involved attribute setting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6318\u001b[0m         \u001b[1;31m# (note that this matches __getattr__, above).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_names_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6320\u001b[1;33m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6321\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6322\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6323\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='saga', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l1', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', smt), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['multi saga','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['multi saga','l1',key,mean(scores)])\n",
    "\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the results\n",
      "0 0.0000 0.489\n",
      "l2 0.0000 0.489\n",
      "l2 0.0001 0.275\n",
      "l2 0.0010 0.397\n",
      "l2 0.0100 0.462\n",
      "l2 0.1000 0.489\n",
      "l2 1.0000 0.490\n",
      "l1 0.0001 0.285\n",
      "l1 0.0010 0.277\n",
      "l1 0.0100 0.331\n",
      "l1 0.1000 0.479\n",
      "l1 1.0000 0.505\n"
     ]
    }
   ],
   "source": [
    "for p in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\tprint(f'Processing for {p}')\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f'  p == 0.0:\n",
    "\t\t\tlm = LogisticRegression(multi_class='ovr', solver='liblinear', penalty=None, max_iter=1000)\n",
    "\t\telse:\n",
    "\t\t\tlm = Logistic% p  \n",
    "\t\tifRegression(multi_class='ovr', solver='liblinear', penalty='l1', C=p, max_iter=1000)\n",
    "\n",
    "\t\tsteps = [('over', SMOTE()), ('model', lm)]\t\n",
    "\t\tpipeline = Pipeline(steps=steps)\n",
    "\t\t# evaluate pipeline\n",
    "\t\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n",
    "\t\tif p == 0.0:\n",
    "\t\t\tresults.append(['ovr liblinear','0',key,mean(scores)])\n",
    "\t\telse:\n",
    "\t\t\tresults.append(['ovr liblinear','l1',key,mean(scores)])\n",
    "\n",
    "print(\"\\nHere are the results\")\n",
    "for result in results:\n",
    "\tprint('%s %s %s %.3f' % (result[0], result[1], result[2], result[3]))\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_accuracy = max(result, key=lambda x: x[3])\n",
    "print(\"Maximum acc:\", best_lr_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    'CNN': CNN_test_accuracy,\n",
    "    'DNN': DNN_test_accuracy,\n",
    "    'Random Forest': RF_accuracy,\n",
    "    'Best Logistic Regression': best_lr_accuracy\n",
    "}\n",
    "\n",
    "# Print each model's accuracy for comparison\n",
    "for model, accuracy in accuracies.items():\n",
    "    print(f\"Accuracy of {model}: {accuracy:.2f}%\")\n",
    "\n",
    "# Identify the best model\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "best_accuracy = accuracies[best_model]\n",
    "print(f\"\\nThe best model is {best_model} with an accuracy of {best_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
